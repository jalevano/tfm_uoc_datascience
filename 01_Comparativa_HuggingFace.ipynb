{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlQPtsbNlkAmsVBnC0hMIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/01_Comparativa_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n_9cjv7sAb2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e2a68d25-a2f4-42f4-b476-3108dea40c70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEVALUACIÓN COMPARATIVA. Mask2former vs OneFormer para Segmentación de personas.\\n\\nSe proporcionar análisis detallado de las ventajas de la segmentación panóptica\\ncon OneFormer frente segmentación de instancias tradicional.\\n\\nConceptos clave:\\n- Segmentación panóptica: Segmentación semántica e instancias\\n- Comparación arquitectónica: Transformer-based models estado del arte\\n- Evaluación académica: Métricas comprehensivas.\\n\\nAutor: Jesús L.\\nTrabajo: Evaluación comparativa de técnicas de segmentación.\\nFecha: Agosto 2025.\\n\\nReferencias Técnicas:\\n- Cheng et al. \"Masked-attention Mask Transformer for Universal Image Segmentation\" (Mask2Former)\\n- Jain et al. \"OneFormer: One Transformer to Rule Universal Image Segmentation\" (OneFormer)\\n- Kirillov et al. \"Panoptic Segmentation\" (Conceptos fundamentales)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Evaluador de Segmentación de Personas - Arquitectura SOLID\n",
        "==========================================================\n",
        "\n",
        "Sistema de evaluación siguiendo principios SOLID:\n",
        "- Single Responsibility: Cada clase tiene una responsabilidad específica\n",
        "- Open/Closed: Extensible para nuevos modelos/métricas\n",
        "- Liskov Substitution: Interfaces bien definidas\n",
        "- Interface Segregation: Interfaces específicas\n",
        "- Dependency Inversion: Dependencias abstraídas\n",
        "\n",
        "Referencias:\n",
        "- Kirillov et al. \"Panoptic Segmentation\" CVPR 2019\n",
        "- Cheng et al. \"Per-Pixel Classification is Not All You Need\" NeurIPS 2021\n",
        "- Lin et al. \"Microsoft COCO: Common Objects in Context\" ECCV 2014\n",
        "\n",
        "Autor: Jesús L.\n",
        "Fecha: Agosto 2025.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZpmxzbqbfaH",
        "outputId": "50c4d80f-bd2f-40af-8ac8-9c1e2acf0892"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Protocol, Union, Tuple\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Transformers imports\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForUniversalSegmentation,\n",
        "    Mask2FormerImageProcessor,\n",
        "    Mask2FormerForUniversalSegmentation,\n",
        "    OneFormerImageProcessor,\n",
        "    OneFormerForUniversalSegmentation\n",
        ")\n",
        "\n",
        "# Metrics imports\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7gf3kDfrarKW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics imports\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "mKVmEPAScHhE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PROTOCOLS Y INTERFACES\n",
        "# ============================================================================\n",
        "\n",
        "class ModelInterface(Protocol):\n",
        "    \"\"\"Protocolo que define la interfaz para modelos de segmentación.\"\"\"\n",
        "\n",
        "    def predict(self, image: Image.Image) -> Dict:\n",
        "        \"\"\"Realizar predicción en imagen.\"\"\"\n",
        "        ...\n",
        "\n",
        "    def get_model_info(self) -> Dict:\n",
        "        \"\"\"Obtener información del modelo.\"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "class MetricsCalculatorInterface(Protocol):\n",
        "    \"\"\"Protocolo para calculadoras de métricas.\"\"\"\n",
        "\n",
        "    def calculate_metrics(self, prediction: np.ndarray, ground_truth: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcular métricas de evaluación.\"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "class DataLoaderInterface(Protocol):\n",
        "    \"\"\"Protocolo para cargadores de datos.\"\"\"\n",
        "\n",
        "    def load_images(self) -> List[str]:\n",
        "        \"\"\"Cargar lista de rutas de imágenes.\"\"\"\n",
        "        ...\n",
        "\n",
        "    def load_ground_truth(self, image_path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Cargar ground truth para imagen específica.\"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "class ResultsStorageInterface(Protocol):\n",
        "    \"\"\"Protocolo para almacenamiento de resultados.\"\"\"\n",
        "\n",
        "    def save_results(self, results: Dict, filepath: str) -> str:\n",
        "        \"\"\"Guardar resultados en formato especificado.\"\"\"\n",
        "        ...\n"
      ],
      "metadata": {
        "id": "B71wjiZcqhDX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATACLASSES Y CONFIGURACIONES\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class EvaluationConfig:\n",
        "    \"\"\"Configuración inmutable para evaluación de segmentación.\"\"\"\n",
        "    # Configuración del modelo\n",
        "    model_id: str\n",
        "    model_name: str\n",
        "    confidence_threshold: float = 0.7\n",
        "\n",
        "    # Configuración de datos\n",
        "    images_directory: str = \"\"\n",
        "    ground_truth_directory: str = \"\"\n",
        "    output_directory: str = \"evaluation_results\"\n",
        "\n",
        "    # Configuración de procesamiento\n",
        "    target_size: Tuple[int, int] = (512, 512)\n",
        "    max_images: Optional[int] = None\n",
        "    device: str = \"auto\"\n",
        "\n",
        "    # Configuración de evaluación\n",
        "    random_seed: int = 42\n",
        "    evaluation_protocol: str = \"COCO\"\n",
        "    save_predictions: bool = True\n",
        "    calculate_panoptic_metrics: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validación post-inicialización.\"\"\"\n",
        "        if not (0.0 <= self.confidence_threshold <= 1.0):\n",
        "            raise ValueError(f\"confidence_threshold debe estar en [0.0, 1.0], recibido: {self.confidence_threshold}\")\n",
        "\n",
        "        if self.max_images is not None and self.max_images <= 0:\n",
        "            raise ValueError(f\"max_images debe ser positivo, recibido: {self.max_images}\")\n",
        "\n",
        "        if not os.path.exists(self.images_directory):\n",
        "            raise FileNotFoundError(f\"Directorio de imágenes no encontrado: {self.images_directory}\")"
      ],
      "metadata": {
        "id": "7mTN_NnyrdJ2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ImageResult:\n",
        "    \"\"\"Resultado de evaluación para una imagen individual.\"\"\"\n",
        "    image_name: str\n",
        "    image_path: str\n",
        "    success: bool\n",
        "    processing_time_seconds: float\n",
        "    original_size: Tuple[int, int]\n",
        "    processed_size: Tuple[int, int]\n",
        "    has_ground_truth: bool\n",
        "    has_person_prediction: bool\n",
        "    metrics: Dict[str, float]\n",
        "    prediction_metadata: Dict\n",
        "    error_message: str = \"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelMetadata:\n",
        "    \"\"\"Metadatos del modelo evaluado.\"\"\"\n",
        "    model_id: str\n",
        "    model_name: str\n",
        "    architecture_type: str\n",
        "    supports_panoptic: bool\n",
        "    parameter_count: int\n",
        "    device_used: str\n",
        "    pytorch_version: str\n",
        "    transformers_version: str\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvaluationMetadata:\n",
        "    \"\"\"Metadatos completos de la evaluación.\"\"\"\n",
        "    model_metadata: ModelMetadata\n",
        "    evaluation_config: EvaluationConfig\n",
        "    execution_info: Dict\n",
        "    dataset_statistics: Dict"
      ],
      "metadata": {
        "id": "Ba7wLKVwrmbY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CALCULADORA DE MÉTRICAS COMPREHENSIVAS\n",
        "# ============================================================================\n",
        "\n",
        "class ComprehensiveMetricsCalculator:\n",
        "    \"\"\"\n",
        "    Calculadora de métricas comprehensivas para segmentación de personas.\n",
        "\n",
        "    Implementa métricas estándar:\n",
        "    - Métricas básicas: IoU, Dice, Precision, Recall, F1\n",
        "    - Métricas de contorno: Boundary IoU, distancias\n",
        "    - Métricas de área y conectividad\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, person_class_id: int = 1):\n",
        "        \"\"\"\n",
        "        Inicializar calculadora de métricas.\n",
        "\n",
        "        Args:\n",
        "            person_class_id: ID de clase para 'person' según dataset (COCO=1)\n",
        "        \"\"\"\n",
        "        self.person_class_id = person_class_id\n",
        "        self.epsilon = 1e-7  # Para estabilidad numérica\n",
        "\n",
        "    def calculate_comprehensive_metrics(self, prediction: np.ndarray,\n",
        "                                      ground_truth: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calcular conjunto comprehensivo de métricas.\n",
        "\n",
        "        Args:\n",
        "            prediction: Máscara predicha binaria (0, 1)\n",
        "            ground_truth: Máscara de ground truth binaria (0, 1)\n",
        "\n",
        "        Returns:\n",
        "            Dict con métricas completas\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Asegurar compatibilidad de dimensiones\n",
        "            prediction, ground_truth = self._ensure_compatible_shapes(prediction, ground_truth)\n",
        "\n",
        "            # Calcular métricas por categoría\n",
        "            basic_metrics = self._calculate_basic_metrics(prediction, ground_truth)\n",
        "            boundary_metrics = self._calculate_boundary_metrics(prediction, ground_truth)\n",
        "            area_metrics = self._calculate_area_metrics(prediction, ground_truth)\n",
        "            connectivity_metrics = self._calculate_connectivity_metrics(prediction, ground_truth)\n",
        "\n",
        "            # Combinar todas las métricas\n",
        "            comprehensive_metrics = {\n",
        "                **basic_metrics,\n",
        "                **boundary_metrics,\n",
        "                **area_metrics,\n",
        "                **connectivity_metrics\n",
        "            }\n",
        "\n",
        "            return comprehensive_metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error calculando métricas: {str(e)}\")\n",
        "            return self._get_empty_metrics()\n",
        "\n",
        "    def _ensure_compatible_shapes(self, pred: np.ndarray, gt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Asegurar que predicción y ground truth tengan formas compatibles.\"\"\"\n",
        "        if pred.shape != gt.shape:\n",
        "            pred = cv2.resize(pred.astype(np.uint8), (gt.shape[1], gt.shape[0]),\n",
        "                            interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Asegurar valores binarios\n",
        "        pred = (pred > 0.5).astype(np.uint8)\n",
        "        gt = (gt > 0.5).astype(np.uint8)\n",
        "\n",
        "        return pred, gt\n",
        "\n",
        "    def _calculate_basic_metrics(self, pred: np.ndarray, gt: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcular métricas básicas de segmentación.\"\"\"\n",
        "        pred_flat = pred.flatten()\n",
        "        gt_flat = gt.flatten()\n",
        "\n",
        "        # Componentes de matriz de confusión\n",
        "        tp = np.sum((pred_flat == 1) & (gt_flat == 1))\n",
        "        fp = np.sum((pred_flat == 1) & (gt_flat == 0))\n",
        "        tn = np.sum((pred_flat == 0) & (gt_flat == 0))\n",
        "        fn = np.sum((pred_flat == 0) & (gt_flat == 1))\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        # Intersection over Union (Jaccard Index)\n",
        "        intersection = tp\n",
        "        union = tp + fp + fn\n",
        "        metrics['iou'] = float(intersection / (union + self.epsilon))\n",
        "\n",
        "        # Dice Coefficient\n",
        "        metrics['dice'] = float(2 * tp / (2 * tp + fp + fn + self.epsilon))\n",
        "\n",
        "        # Precision (Positive Predictive Value)\n",
        "        metrics['precision'] = float(tp / (tp + fp + self.epsilon))\n",
        "\n",
        "        # Recall (Sensitivity)\n",
        "        metrics['recall'] = float(tp / (tp + fn + self.epsilon))\n",
        "\n",
        "        # Specificity (True Negative Rate)\n",
        "        metrics['specificity'] = float(tn / (tn + fp + self.epsilon))\n",
        "\n",
        "        # F1-Score\n",
        "        metrics['f1_score'] = float(2 * metrics['precision'] * metrics['recall'] /\n",
        "                                   (metrics['precision'] + metrics['recall'] + self.epsilon))\n",
        "\n",
        "        # Accuracy\n",
        "        metrics['pixel_accuracy'] = float((tp + tn) / (tp + tn + fp + fn + self.epsilon))\n",
        "\n",
        "        # Balanced Accuracy\n",
        "        sensitivity = metrics['recall']\n",
        "        specificity = metrics['specificity']\n",
        "        metrics['balanced_accuracy'] = float((sensitivity + specificity) / 2)\n",
        "\n",
        "        # Matthews Correlation Coefficient\n",
        "        numerator = (tp * tn) - (fp * fn)\n",
        "        denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "        metrics['mcc'] = float(numerator / (denominator + self.epsilon))\n",
        "\n",
        "        # Fowlkes-Mallows Index\n",
        "        metrics['fowlkes_mallows'] = float(np.sqrt(metrics['precision'] * metrics['recall']))\n",
        "\n",
        "        # Componentes de matriz de confusión\n",
        "        metrics.update({\n",
        "            'true_positives': int(tp),\n",
        "            'false_positives': int(fp),\n",
        "            'true_negatives': int(tn),\n",
        "            'false_negatives': int(fn)\n",
        "        })\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_boundary_metrics(self, pred: np.ndarray, gt: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcular métricas de precisión de contorno.\"\"\"\n",
        "        try:\n",
        "            pred_contours = self._extract_contours(pred)\n",
        "            gt_contours = self._extract_contours(gt)\n",
        "\n",
        "            metrics = {}\n",
        "\n",
        "            if len(pred_contours) > 0 and len(gt_contours) > 0:\n",
        "                # Boundary IoU aproximado\n",
        "                pred_boundary = self._dilate_contours(pred, iterations=2)\n",
        "                gt_boundary = self._dilate_contours(gt, iterations=2)\n",
        "\n",
        "                boundary_intersection = np.sum((pred_boundary == 1) & (gt_boundary == 1))\n",
        "                boundary_union = np.sum((pred_boundary == 1) | (gt_boundary == 1))\n",
        "\n",
        "                metrics['boundary_iou'] = float(boundary_intersection / (boundary_union + self.epsilon))\n",
        "\n",
        "                # Distancia promedio de contorno\n",
        "                metrics['average_boundary_distance'] = self._calculate_average_boundary_distance(\n",
        "                    pred_contours, gt_contours\n",
        "                )\n",
        "            else:\n",
        "                metrics['boundary_iou'] = 0.0\n",
        "                metrics['average_boundary_distance'] = float('inf')\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception:\n",
        "            return {'boundary_iou': 0.0, 'average_boundary_distance': float('inf')}\n",
        "\n",
        "    def _calculate_area_metrics(self, pred: np.ndarray, gt: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcular métricas relacionadas con área.\"\"\"\n",
        "        pred_area = np.sum(pred)\n",
        "        gt_area = np.sum(gt)\n",
        "        total_pixels = pred.size\n",
        "\n",
        "        metrics = {\n",
        "            'predicted_area_ratio': float(pred_area / total_pixels),\n",
        "            'ground_truth_area_ratio': float(gt_area / total_pixels),\n",
        "            'area_ratio_error': float(abs(pred_area - gt_area) / (gt_area + self.epsilon)),\n",
        "            'relative_area_error': float((pred_area - gt_area) / (gt_area + self.epsilon)),\n",
        "            'area_correlation': float(np.corrcoef(pred.flatten(), gt.flatten())[0, 1]\n",
        "                                   if gt_area > 0 else 0.0)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_connectivity_metrics(self, pred: np.ndarray, gt: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcular métricas de conectividad y topología.\"\"\"\n",
        "        try:\n",
        "            # Número de componentes conectados\n",
        "            pred_components = cv2.connectedComponents(pred.astype(np.uint8))[0] - 1\n",
        "            gt_components = cv2.connectedComponents(gt.astype(np.uint8))[0] - 1\n",
        "\n",
        "            metrics = {\n",
        "                'predicted_components': int(pred_components),\n",
        "                'ground_truth_components': int(gt_components),\n",
        "                'component_count_error': int(abs(pred_components - gt_components))\n",
        "            }\n",
        "\n",
        "            # Métrica de fragmentación\n",
        "            if gt_components > 0:\n",
        "                metrics['fragmentation_ratio'] = float(pred_components / gt_components)\n",
        "            else:\n",
        "                metrics['fragmentation_ratio'] = float('inf') if pred_components > 0 else 1.0\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception:\n",
        "            return {\n",
        "                'predicted_components': 0,\n",
        "                'ground_truth_components': 0,\n",
        "                'component_count_error': 0,\n",
        "                'fragmentation_ratio': 1.0\n",
        "            }\n",
        "\n",
        "    def _extract_contours(self, mask: np.ndarray) -> List:\n",
        "        \"\"\"Extraer contornos de máscara binaria.\"\"\"\n",
        "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        return contours\n",
        "\n",
        "    def _dilate_contours(self, mask: np.ndarray, iterations: int = 1) -> np.ndarray:\n",
        "        \"\"\"Dilatar contornos para cálculo de boundary IoU.\"\"\"\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        return cv2.dilate(mask.astype(np.uint8), kernel, iterations=iterations)\n",
        "\n",
        "    def _calculate_average_boundary_distance(self, pred_contours: List, gt_contours: List) -> float:\n",
        "        \"\"\"Calcular distancia promedio entre contornos.\"\"\"\n",
        "        try:\n",
        "            if not pred_contours or not gt_contours:\n",
        "                return float('inf')\n",
        "\n",
        "            pred_points = pred_contours[0].reshape(-1, 2) if len(pred_contours) > 0 else np.array([[0, 0]])\n",
        "            gt_points = gt_contours[0].reshape(-1, 2) if len(gt_contours) > 0 else np.array([[0, 0]])\n",
        "\n",
        "            # Calcular distancia Hausdorff aproximada\n",
        "            distances = []\n",
        "            for pred_point in pred_points[::10]:  # Subsample para eficiencia\n",
        "                min_dist = np.min(np.linalg.norm(gt_points - pred_point, axis=1))\n",
        "                distances.append(min_dist)\n",
        "\n",
        "            return float(np.mean(distances)) if distances else float('inf')\n",
        "\n",
        "        except Exception:\n",
        "            return float('inf')\n",
        "\n",
        "    def _get_empty_metrics(self) -> Dict[str, float]:\n",
        "        \"\"\"Retornar métricas vacías en caso de error.\"\"\"\n",
        "        return {\n",
        "            'iou': 0.0, 'dice': 0.0, 'precision': 0.0, 'recall': 0.0,\n",
        "            'specificity': 0.0, 'f1_score': 0.0, 'pixel_accuracy': 0.0,\n",
        "            'balanced_accuracy': 0.0, 'mcc': 0.0, 'fowlkes_mallows': 0.0,\n",
        "            'boundary_iou': 0.0, 'average_boundary_distance': float('inf'),\n",
        "            'predicted_area_ratio': 0.0, 'ground_truth_area_ratio': 0.0,\n",
        "            'area_ratio_error': float('inf'), 'relative_area_error': float('inf'),\n",
        "            'area_correlation': 0.0, 'predicted_components': 0,\n",
        "            'ground_truth_components': 0, 'component_count_error': 0,\n",
        "            'fragmentation_ratio': 1.0, 'true_positives': 0,\n",
        "            'false_positives': 0, 'true_negatives': 0, 'false_negatives': 0\n",
        "        }"
      ],
      "metadata": {
        "id": "M0bBfmMbsU9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CARGADORES DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "class RobustDataLoader:\n",
        "    \"\"\"Cargador de datos con validación robusta.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EvaluationConfig):\n",
        "        \"\"\"\n",
        "        Inicializar cargador de datos.\n",
        "\n",
        "        Args:\n",
        "            config: Configuración de evaluación validada\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def load_images(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Cargar lista validada de rutas de imágenes.\n",
        "\n",
        "        Returns:\n",
        "            Lista ordenada de rutas de imágenes válidas\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Cargando imágenes desde: {self.config.images_directory}\")\n",
        "\n",
        "        image_paths = []\n",
        "\n",
        "        for filename in os.listdir(self.config.images_directory):\n",
        "            if self._is_valid_image_file(filename):\n",
        "                full_path = os.path.join(self.config.images_directory, filename)\n",
        "\n",
        "                if self._validate_image_file(full_path):\n",
        "                    image_paths.append(full_path)\n",
        "                else:\n",
        "                    self.logger.warning(f\"Archivo de imagen inválido omitido: {filename}\")\n",
        "\n",
        "        if not image_paths:\n",
        "            raise ValueError(f\"No se encontraron imágenes válidas en {self.config.images_directory}\")\n",
        "\n",
        "        # Ordenar para reproducibilidad\n",
        "        image_paths.sort()\n",
        "\n",
        "        # Aplicar límite si especificado\n",
        "        if self.config.max_images:\n",
        "            image_paths = image_paths[:self.config.max_images]\n",
        "            self.logger.info(f\"Limitando evaluación a {self.config.max_images} imágenes\")\n",
        "\n",
        "        self.logger.info(f\"Cargadas {len(image_paths)} imágenes válidas\")\n",
        "        return image_paths\n",
        "\n",
        "    def load_ground_truth(self, image_path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Cargar ground truth correspondiente a una imagen.\n",
        "\n",
        "        Args:\n",
        "            image_path: Ruta de la imagen original\n",
        "\n",
        "        Returns:\n",
        "            Array numpy con máscara binaria o None si no existe\n",
        "        \"\"\"\n",
        "        if not self.config.ground_truth_directory:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            image_name = Path(image_path).stem\n",
        "            gt_extensions = ['.png', '.jpg', '.jpeg', '.bmp']\n",
        "\n",
        "            gt_path = None\n",
        "            for ext in gt_extensions:\n",
        "                candidate_path = os.path.join(self.config.ground_truth_directory, f\"{image_name}{ext}\")\n",
        "                if os.path.exists(candidate_path):\n",
        "                    gt_path = candidate_path\n",
        "                    break\n",
        "\n",
        "            if not gt_path:\n",
        "                return None\n",
        "\n",
        "            # Cargar y validar ground truth\n",
        "            gt_image = Image.open(gt_path).convert('L')\n",
        "\n",
        "            # Redimensionar si necesario\n",
        "            if gt_image.size != self.config.target_size[::-1]:\n",
        "                gt_image = gt_image.resize(self.config.target_size[::-1], Image.NEAREST)\n",
        "\n",
        "            # Convertir a array binario\n",
        "            gt_array = np.array(gt_image)\n",
        "            binary_mask = (gt_array > 127).astype(np.uint8)\n",
        "\n",
        "            if np.sum(binary_mask) == 0:\n",
        "                self.logger.warning(f\"Ground truth vacío para: {image_name}\")\n",
        "\n",
        "            return binary_mask\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error cargando ground truth para {image_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _is_valid_image_file(self, filename: str) -> bool:\n",
        "        \"\"\"Verificar si archivo tiene extensión de imagen válida.\"\"\"\n",
        "        return any(filename.lower().endswith(ext) for ext in self.valid_extensions)\n",
        "\n",
        "    def _validate_image_file(self, filepath: str) -> bool:\n",
        "        \"\"\"Validar que archivo de imagen sea accesible y válido.\"\"\"\n",
        "        try:\n",
        "            if not os.path.isfile(filepath):\n",
        "                return False\n",
        "\n",
        "            # Intentar abrir imagen para validar formato\n",
        "            with Image.open(filepath) as img:\n",
        "                img.verify()\n",
        "\n",
        "            # Verificar tamaño mínimo\n",
        "            with Image.open(filepath) as img:\n",
        "                if img.size[0] < 32 or img.size[1] < 32:\n",
        "                    self.logger.warning(f\"Imagen demasiado pequeña: {filepath}\")\n",
        "                    return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception:\n",
        "            return False\n"
      ],
      "metadata": {
        "id": "HLIM3NMMsaUO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# WRAPPERS DE MODELOS\n",
        "# ============================================================================\n",
        "\n",
        "class AbstractModelWrapper(ABC):\n",
        "    \"\"\"Clase abstracta para wrappers de modelos de segmentación.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, image: Image.Image) -> Dict:\n",
        "        \"\"\"Realizar predicción en imagen.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_model_info(self) -> ModelMetadata:\n",
        "        \"\"\"Obtener metadatos del modelo.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def extract_person_mask(self, prediction_outputs: Dict) -> Optional[np.ndarray]:\n",
        "        \"\"\"Extraer máscara de persona desde outputs del modelo.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class Mask2FormerWrapper(AbstractModelWrapper):\n",
        "    \"\"\"Wrapper para Mask2Former.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str, device: str):\n",
        "        \"\"\"Inicializar wrapper de Mask2Former.\"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.device = device\n",
        "        self.person_class_id = 1  # COCO person class\n",
        "\n",
        "        # Cargar modelo y procesador\n",
        "        self.processor = Mask2FormerImageProcessor.from_pretrained(model_id)\n",
        "        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(model_id)\n",
        "        self.model.eval()\n",
        "        self.model = self.model.to(device)\n",
        "\n",
        "        # Metadatos\n",
        "        self.parameter_count = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def predict(self, image: Image.Image) -> Dict:\n",
        "        \"\"\"Realizar predicción con Mask2Former.\"\"\"\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Procesar salidas\n",
        "            processed_outputs = {}\n",
        "\n",
        "            # Segmentación semántica\n",
        "            semantic_map = self.processor.post_process_semantic_segmentation(\n",
        "                outputs, target_sizes=[image.size[::-1]]\n",
        "            )[0]\n",
        "            processed_outputs['semantic_segmentation'] = semantic_map.cpu().numpy()\n",
        "\n",
        "            # Intentar segmentación panóptica\n",
        "            try:\n",
        "                panoptic_map = self.processor.post_process_panoptic_segmentation(\n",
        "                    outputs, target_sizes=[image.size[::-1]]\n",
        "                )[0]\n",
        "                processed_outputs['panoptic_segmentation'] = {\n",
        "                    'segmentation': panoptic_map['segmentation'].cpu().numpy(),\n",
        "                    'segments_info': panoptic_map['segments_info']\n",
        "                }\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            return processed_outputs\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error en predicción Mask2Former: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_person_mask(self, prediction_outputs: Dict) -> Optional[np.ndarray]:\n",
        "        \"\"\"Extraer máscara de persona desde outputs de Mask2Former.\"\"\"\n",
        "        try:\n",
        "            # Priorizar segmentación panóptica\n",
        "            if 'panoptic_segmentation' in prediction_outputs:\n",
        "                panoptic_data = prediction_outputs['panoptic_segmentation']\n",
        "                segmentation_map = panoptic_data['segmentation']\n",
        "                segments_info = panoptic_data['segments_info']\n",
        "\n",
        "                person_mask = np.zeros_like(segmentation_map, dtype=np.uint8)\n",
        "                for segment in segments_info:\n",
        "                    if segment.get('category_id') == self.person_class_id:\n",
        "                        person_mask[segmentation_map == segment['id']] = 1\n",
        "\n",
        "                return person_mask\n",
        "\n",
        "            # Fallback a segmentación semántica\n",
        "            elif 'semantic_segmentation' in prediction_outputs:\n",
        "                semantic_map = prediction_outputs['semantic_segmentation']\n",
        "                person_mask = (semantic_map == self.person_class_id).astype(np.uint8)\n",
        "                return person_mask\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error extrayendo máscara de persona: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_model_info(self) -> ModelMetadata:\n",
        "        \"\"\"Obtener metadatos de Mask2Former.\"\"\"\n",
        "        import transformers\n",
        "\n",
        "        return ModelMetadata(\n",
        "            model_id=self.model_id,\n",
        "            model_name=\"Mask2Former\",\n",
        "            architecture_type=\"mask2former\",\n",
        "            supports_panoptic=True,\n",
        "            parameter_count=self.parameter_count,\n",
        "            device_used=self.device,\n",
        "            pytorch_version=torch.__version__,\n",
        "            transformers_version=transformers.__version__\n",
        "        )\n",
        "\n",
        "\n",
        "class OneFormerWrapper(AbstractModelWrapper):\n",
        "    \"\"\"Wrapper para OneFormer.\"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str, device: str):\n",
        "        \"\"\"Inicializar wrapper de OneFormer.\"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.device = device\n",
        "        self.person_class_id = 1  # COCO person class\n",
        "\n",
        "        # Cargar modelo y procesador\n",
        "        self.processor = OneFormerImageProcessor.from_pretrained(model_id)\n",
        "        self.model = OneFormerForUniversalSegmentation.from_pretrained(model_id)\n",
        "        self.model.eval()\n",
        "        self.model = self.model.to(device)\n",
        "\n",
        "        # Metadatos\n",
        "        self.parameter_count = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def predict(self, image: Image.Image) -> Dict:\n",
        "        \"\"\"Realizar predicción con OneFormer.\"\"\"\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            # Procesar salidas específicas de OneFormer\n",
        "            processed_outputs = {}\n",
        "\n",
        "            # Segmentación semántica\n",
        "            try:\n",
        "                semantic_map = self.processor.post_process_semantic_segmentation(\n",
        "                    outputs, target_sizes=[image.size[::-1]]\n",
        "                )[0]\n",
        "                processed_outputs['semantic_segmentation'] = semantic_map.cpu().numpy()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Segmentación panóptica (especialidad de OneFormer)\n",
        "            try:\n",
        "                panoptic_map = self.processor.post_process_panoptic_segmentation(\n",
        "                    outputs, target_sizes=[image.size[::-1]]\n",
        "                )[0]\n",
        "                processed_outputs['panoptic_segmentation'] = {\n",
        "                    'segmentation': panoptic_map['segmentation'].cpu().numpy(),\n",
        "                    'segments_info': panoptic_map['segments_info']\n",
        "                }\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            return processed_outputs\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error en predicción OneFormer: {str(e)}\")\n",
        "            return {}"
      ],
      "metadata": {
        "id": "0DY-FuC9sgKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BygTMK-jsoWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}