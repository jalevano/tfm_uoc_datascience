{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPliBpbtBSqiewUSdD5wVBr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/01_Comparativa_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n_9cjv7sAb2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "85d865e0-d2fd-4469-db1e-2f99d721b7be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEVALUACIÓN COMPARATIVA. Mask2former vs OneFormer para Segmentación de personas.\\n\\nSe proporcionar análisis detallado de las ventajas de la segmentación panóptica\\ncon OneFormer frente segmentación de instancias tradicional.\\n\\nConceptos clave:\\n- Segmentación panóptica: Segmentación semántica e instancias\\n- Comparación arquitectónica: Transformer-based models estado del arte\\n- Evaluación académica: Métricas comprehensivas.\\n\\nAutor: Jesús L.\\nTrabajo: Evaluación comparativa de técnicas de segmentación.\\nFecha: Agosto 2025.\\n\\nReferencias Técnicas:\\n- Cheng et al. \"Masked-attention Mask Transformer for Universal Image Segmentation\" (Mask2Former)\\n- Jain et al. \"OneFormer: One Transformer to Rule Universal Image Segmentation\" (OneFormer)\\n- Kirillov et al. \"Panoptic Segmentation\" (Conceptos fundamentales)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "EVALUACIÓN COMPARATIVA. Mask2former vs OneFormer para Segmentación de personas.\n",
        "\n",
        "Se proporcionar análisis detallado de las ventajas de la segmentación panóptica\n",
        "con OneFormer frente segmentación de instancias tradicional.\n",
        "\n",
        "Conceptos clave:\n",
        "- Segmentación panóptica: Segmentación semántica e instancias\n",
        "- Comparación arquitectónica: Transformer-based models estado del arte\n",
        "- Evaluación académica: Métricas comprehensivas.\n",
        "\n",
        "Autor: Jesús L.\n",
        "Trabajo: Evaluación comparativa de técnicas de segmentación.\n",
        "Fecha: Agosto 2025.\n",
        "\n",
        "Referencias Técnicas:\n",
        "- Cheng et al. \"Masked-attention Mask Transformer for Universal Image Segmentation\" (Mask2Former)\n",
        "- Jain et al. \"OneFormer: One Transformer to Rule Universal Image Segmentation\" (OneFormer)\n",
        "- Kirillov et al. \"Panoptic Segmentation\" (Conceptos fundamentales)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZpmxzbqbfaH",
        "outputId": "81fa10a6-d8a5-4549-eff8-a2633a1eb594"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Optional, Tuple, Union, Literal\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import json\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "7gf3kDfrarKW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importaciones específicas de Hugging Face para ambos modelos\n",
        "from transformers import (\n",
        "    # Mask2Former components\n",
        "    Mask2FormerImageProcessor,\n",
        "    Mask2FormerForUniversalSegmentation,\n",
        "    # OneFormer components\n",
        "    OneFormerImageProcessor,\n",
        "    OneFormerForUniversalSegmentation,\n",
        "    # Generic auto-loading\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForUniversalSegmentation\n",
        ")"
      ],
      "metadata": {
        "id": "n7kcXJnNa2yF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Métricas y utilidades para evaluación\n",
        "from sklearn.metrics import precision_recall_fscore_support, jaccard_score\n",
        "import torchmetrics\n",
        "from torchmetrics.segmentation import MeanIoU, DiceScore\n",
        "from torchmetrics.classification import BinaryJaccardIndex"
      ],
      "metadata": {
        "id": "GQCmYcrVbHVB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilidades para procesamiento y visualización\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)"
      ],
      "metadata": {
        "id": "mKVmEPAScHhE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ComparativeEvaluationConfig:\n",
        "  \"\"\"\n",
        "    Evaluación comparativa de modelos de segmentación.\n",
        "\n",
        "    + Atributos de configuración de modelos:\n",
        "    ++models_to_evaluate(List[str]): Lista de modelos que vamos a comparar.\n",
        "      Opciones posibles:\n",
        "       - \"facebook/mask2former-swin-base-coco-panoptic\": Mask2Former baseline\n",
        "       - \"shi-labs/oneformer_coco_swin_large\": OneFormer optimizado para COCO\n",
        "       - \"shi-labs/oneformer_ade20k_swin_large\": OneFormer para escenas complejas\n",
        "    ++confidence_threshold(float): Umbral mínimo de confianza para aceptar detecciones.\n",
        "      Rango [0.0,1.0] para personas, valores óptimos entre 0.6 y 0.8 balancean\n",
        "      precisión y recall. Valor único aplicado para ambos modelos de comparación justa.\n",
        "    ++segmentation_task(str): Tipo de segmentación a evaluar específicamente.\n",
        "      - \"panoptic\": Segmentación panóptica. Recomendado para OneFormer.\n",
        "      - \"instance\": Segmentación de instancias. Comparación directa.\n",
        "      - \"semantic\": Segmentación semántica. Recomendado para Mask2Former. Análisis\n",
        "      de precisión.\n",
        "\n",
        "    + Atributos de configuración de datos:\n",
        "    ++images_directory(str): Directorio raíz de con imágenes de evaluación en formatos\n",
        "    estándar (JPG,PNG,BMP).\n",
        "    ++ground_truth_masks_dir(str): Directorio con máscaras de verdad fundamental\n",
        "    en formato PNG binario (0=fondo, 255=persona). Esto es opcional si solo se\n",
        "    requiere inferencia sin evaluación cuantitativa.\n",
        "    ++panoptic_annotations_file(str): Fichero COCO-format JSON con anotaciones\n",
        "    panópticas para evaluación completa. Incluye información semántica e instancias.\n",
        "    ++output_directory(str): Directorio de salida para todos los resultados.\n",
        "    Se crea automáticamente con subdirectorios organizados por modelo.\n",
        "\n",
        "    + Atributos de configuración de procesamiento:\n",
        "    ++batch_size(int): Número de imágenes procesadas simultáneamente por el modelo.\n",
        "    Este valor lo debemos ajustar dependiendo del entorno.\n",
        "    ++max_images(int): Límite de imágenes para evaluación rápida.\n",
        "    None para evaluar todas las imágenes.\n",
        "    ++resize_images(bool): Si debemos redimensionar imágenes para eficiencia de\n",
        "    cómputo. Aquí será \"True\" si tenemos dataset con imágenes de alta resolución.\n",
        "    ++target_size(Tuple[int,int]): Dimensiones objetivo (altura, ancho) para\n",
        "    redimensionar. Debe ser múltiplo de 32 para que sea eficiente y óptimo\n",
        "    de la arquitectura transformer.\n",
        "    ++enable_mixed_precision(bool): Si denbemos utilizar precisión mixta para\n",
        "    acelerar inferencia en GPUs compatibles. Reduce uso de memoria manteniendo\n",
        "    precisión.\n",
        "\n",
        "    + Atributos de configuración de análisis:\n",
        "    ++compare_architectures(bool): Para realizar análisis comparativo detallado\n",
        "    entre arquitecturas. Incluyo análisis de complejidad y eficiencia.\n",
        "    ++analyze_panoptic_advantages(bool): Si evaluamos específicamente ventajas\n",
        "    de segmentación panóptica sobre instancias.\n",
        "    ++save_detailed_predictions(bool): Para crear visualizaciones lado a lado\n",
        "    comparando resultados de los modelos.\n",
        "    ++visualization_samples(int): Número de muestras visuales a generar.\n",
        "    Balance entre utilidad analítica y uso de almacenamiento.\n",
        "    ++detailed_logging(bool): Para activar logging exhaustivo para la documentación\n",
        "    y reproducibilidad de los experimentos.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #Configuración de modelos para comparación.\n",
        "  models_to_evaluate: List[str] = None\n",
        "  confidence_threshold: float = 0.7\n",
        "  segmentation_task: Literal[\"panoptic\", \"instance\", \"semantic\"] = \"panoptic\"\n",
        "  device: str = \"auto\"\n",
        "\n",
        "  #Configuración de datos de entrada.\n",
        "  images_directory: str = \"\"\n",
        "  ground_truth_masks_dir: str = \"\"\n",
        "  panoptic_annotations_file: str = \"\"\n",
        "  output_directory: str = \"comparative_segmentation_results\"\n",
        "\n",
        "  #Configuración de procesamiento optimizado.\n",
        "  batch_size: int = 1 ## Revisar. Es muy conservador.\n",
        "  max_images: Optional[int] = None\n",
        "  resize_images: bool = True\n",
        "  target_size: Tuple[int,int] = (512,512) ## Eficiente para transformers.\n",
        "  enable_mixed_precision: bool = True\n",
        "\n",
        "  #Configuración de análisis comparativo.\n",
        "  compare_architectures: bool = True\n",
        "  analyze_panoptic_advantages: bool = True\n",
        "  save_detailed_predictions: bool = True\n",
        "  visualization_samples: int = 15\n",
        "  detailed_logging: bool = True\n",
        "\n",
        "  def __post_init__(self):\n",
        "    \"\"\"\n",
        "      Configuración automática y validación de parámetros.\n",
        "    \"\"\"\n",
        "    #Configuración automática si no se define.\n",
        "    if self.models_to_evaluate is None:\n",
        "      self.models_to_evaluate = [\"facebook/mask2former-swin-base-coco-panoptic\",\n",
        "                                  \"shi-labs/oneformer_coco_swin_large\",\n",
        "                                  \"shi-labs/oneformer_ade20k_swin_large\"]\n",
        "\n",
        "    #Detección automática de dispositivo.\n",
        "    if self.device == \"auto\":\n",
        "      if torch.cuda.is_available():\n",
        "        self.device = \"cuda\"\n",
        "      else: \"cpu\"\n",
        "\n",
        "    #Validación de directorio de imágenes.\n",
        "    if not self.images_directory:\n",
        "        raise ValueError(\"Debe proporcionar un directorio de imágenes.\")\n",
        "\n",
        "    #Crear directorio de salida.\n",
        "    os.makedirs(self.output_directory, exist_ok=True)\n",
        "\n",
        "    #Ajustar configuración según dispositivo disponible.\n",
        "    if self.device == \"cpu\":\n",
        "        self.enable_mixed_precision == False ##Solo en GPU.\n",
        "        self.batch_size = 1 ##Muy conservador en CPU."
      ],
      "metadata": {
        "id": "m45RFwz1cYSN"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}