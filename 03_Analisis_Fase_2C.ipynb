{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3JvGTisu9/ulF0Ukn/4os",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/03_Analisis_Fase_2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "================================================================================\n",
        "FASE 2C - PCA, CLUSTERING Y ANALISIS DE REDUNDANCIA DE METRICAS\n",
        "================================================================================\n",
        "Trabajo Fin de Master: Evaluacion Comparativa de Tecnicas de Segmentacion\n",
        "en Fotografia de Retrato\n",
        "\n",
        "Autor: Jesus L.\n",
        "Universidad: Universitat Oberta de Catalunya (UOC)\n",
        "Fecha: Diciembre 2025\n",
        "\n",
        "DESCRIPCION:\n",
        "Fase 2C del analisis comparativo. Realiza analisis exploratorio multivariante\n",
        "sobre las metricas de segmentacion calculadas en Fase 2A, incluyendo:\n",
        "- Reduccion de dimensionalidad mediante PCA\n",
        "- Clustering de fotografias por nivel de dificultad\n",
        "- Identificacion de metricas redundantes\n",
        "- Visualizaciones para el Capitulo 4 del TFM\n",
        "\n",
        "DEPENDENCIAS DE DATOS:\n",
        "- Requiere Fase 2B completada (metricas_fusionadas.csv)\n",
        "- Utiliza metricas geometricas Shapely y metricas clasicas\n",
        "\n",
        "ANALISIS INCLUIDOS:\n",
        "1. PCA de metricas de segmentacion (geometricas + clasicas)\n",
        "2. Clustering K-means de fotografias por dificultad (k=3)\n",
        "3. Clustering jerarquico con dendrograma\n",
        "4. Matriz de correlaciones entre metricas\n",
        "5. Identificacion de metricas redundantes (|r| > 0.9)\n",
        "6. Visualizaciones: biplot, heatmap, scatter PCA\n",
        "\n",
        "ESTRUCTURA DE SALIDA:\n",
        "/TFM/3_Analisis/fase2c_pca_clustering/\n",
        "├── pca_componentes.csv           # Scores PC1, PC2, PC3 por evaluacion\n",
        "├── pca_varianza_explicada.csv    # Varianza explicada por componente\n",
        "├── pca_loadings.csv              # Pesos de cada metrica en cada PC\n",
        "├── clusters_fotografias.csv      # Asignacion de cluster por fotografia\n",
        "├── clusters_centroides.csv       # Centroides de cada cluster\n",
        "├── correlaciones_metricas.csv    # Matriz de correlaciones\n",
        "├── metricas_redundantes.csv      # Pares con |r| > 0.9\n",
        "├── visualizaciones/\n",
        "│   ├── pca_varianza_explicada.png\n",
        "│   ├── pca_biplot.png\n",
        "│   ├── pca_scatter_clusters.png\n",
        "│   ├── dendrograma_fotografias.png\n",
        "│   ├── heatmap_correlaciones.png\n",
        "│   └── clusters_caracterizacion.png\n",
        "\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qT2PJcG2R1tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "import warnings\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "# Visualizacion\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "\n",
        "# Suprimir warnings de convergencia\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)"
      ],
      "metadata": {
        "id": "mXAuQMvoR2HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONFIGURACION DE VISUALIZACION\n",
        "# =============================================================================\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': (12, 8),\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 300,\n",
        "    'font.size': 10,\n",
        "    'axes.titlesize': 12,\n",
        "    'axes.labelsize': 10,\n",
        "    'xtick.labelsize': 9,\n",
        "    'ytick.labelsize': 9,\n",
        "    'legend.fontsize': 9,\n",
        "    'figure.titlesize': 14,\n",
        "    'axes.grid': True,\n",
        "    'grid.alpha': 0.3,\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False\n",
        "})\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Paleta de colores para modelos\n",
        "COLORES_MODELOS = {\n",
        "    'yolov8': '#2ecc71',      # Verde\n",
        "    'oneformer': '#3498db',   # Azul\n",
        "    'sam2': '#e74c3c',        # Rojo\n",
        "    'sam2_prompts': '#9b59b6', # Purpura\n",
        "    'mask2former': '#f39c12', # Naranja\n",
        "    'bodypix': '#1abc9c'      # Turquesa\n",
        "}\n",
        "\n",
        "# Paleta para clusters\n",
        "COLORES_CLUSTERS = {\n",
        "    0: '#27ae60',  # Verde - Facil\n",
        "    1: '#f1c40f',  # Amarillo - Medio\n",
        "    2: '#e74c3c'   # Rojo - Dificil\n",
        "}\n",
        "\n",
        "NOMBRES_CLUSTERS = {\n",
        "    0: 'Facil',\n",
        "    1: 'Medio',\n",
        "    2: 'Dificil'\n",
        "}"
      ],
      "metadata": {
        "id": "f7TXruWeR5vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONFIGURACION\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ConfiguracionFase2C:\n",
        "    \"\"\"\n",
        "    Configuracion central para el pipeline de Fase 2C.\n",
        "\n",
        "    Attributes:\n",
        "        ruta_base_tfm: Ruta raiz del proyecto TFM\n",
        "        ruta_datos_fase2b: Ruta al CSV fusionado de Fase 2B\n",
        "        ruta_salida: Directorio para resultados de Fase 2C\n",
        "        n_componentes_pca: Numero de componentes PCA a retener\n",
        "        n_clusters: Numero de clusters para K-means\n",
        "        umbral_redundancia: Umbral de correlacion para considerar redundancia\n",
        "        random_state: Semilla para reproducibilidad\n",
        "    \"\"\"\n",
        "    ruta_base_tfm: Path\n",
        "    ruta_datos_fase2b: Path = None\n",
        "    ruta_salida: Path = None\n",
        "    n_componentes_pca: int = 5\n",
        "    n_clusters: int = 3\n",
        "    umbral_redundancia: float = 0.90\n",
        "    random_state: int = 42\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Inicializa rutas derivadas si no se especifican.\"\"\"\n",
        "        if self.ruta_datos_fase2b is None:\n",
        "            self.ruta_datos_fase2b = (\n",
        "                self.ruta_base_tfm / \"3_Analisis\" / \"fase2b_correlaciones\" /\n",
        "                \"metricas_fusionadas.csv\"\n",
        "            )\n",
        "        if self.ruta_salida is None:\n",
        "            self.ruta_salida = self.ruta_base_tfm / \"3_Analisis\" / \"fase2c_pca_clustering\""
      ],
      "metadata": {
        "id": "_FSVJf8BST-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4qAtbFlOqUu"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONSTANTES: METRICAS PARA ANALISIS\n",
        "# =============================================================================\n",
        "\n",
        "# Metricas de segmentacion para PCA\n",
        "METRICAS_SEGMENTACION = [\n",
        "    # Metricas clasicas\n",
        "    'iou',\n",
        "    'dice',\n",
        "    'precision',\n",
        "    'recall',\n",
        "    'boundary_iou',\n",
        "\n",
        "    # Metricas geometricas Shapely - Basicas\n",
        "    'pred_area',\n",
        "    'pred_perimeter',\n",
        "    'pred_compactness',\n",
        "    'pred_convexity',\n",
        "    'pred_solidity',\n",
        "\n",
        "    # Metricas geometricas - Bounding box\n",
        "    'pred_bbox_aspect_ratio',\n",
        "    'pred_extent',\n",
        "\n",
        "    # Metricas geometricas - Centroide\n",
        "    'pred_centroid_x_norm',\n",
        "    'pred_centroid_y_norm',\n",
        "\n",
        "    # Metricas de comparacion con GT\n",
        "    'area_ratio',\n",
        "    'perimeter_ratio',\n",
        "    'centroid_distance',\n",
        "    'hausdorff_distance',\n",
        "]\n",
        "\n",
        "# Metricas alternativas si las anteriores no existen\n",
        "METRICAS_ALTERNATIVAS = {\n",
        "    'pred_area': ['area_pred', 'mask_area'],\n",
        "    'pred_perimeter': ['perimeter_pred', 'mask_perimeter'],\n",
        "    'pred_compactness': ['compactness', 'compacidad'],\n",
        "    'pred_convexity': ['convexity', 'convexidad'],\n",
        "    'pred_solidity': ['solidity', 'solidez'],\n",
        "    'boundary_iou': ['biou', 'boundary_score'],\n",
        "    'hausdorff_distance': ['hausdorff', 'hd'],\n",
        "}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASE: CARGADOR DE DATOS\n",
        "# =============================================================================\n",
        "\n",
        "class CargadorDatosFase2C:\n",
        "    \"\"\"\n",
        "    Gestiona la carga y preparacion de datos para Fase 2C.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionFase2C, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        Inicializa el cargador.\n",
        "\n",
        "        Args:\n",
        "            config: Configuracion de Fase 2C\n",
        "            logger: Logger para registro\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def cargar_datos_fusionados(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Carga el dataset fusionado de Fase 2B.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con metricas fusionadas\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Cargando datos de: {self.config.ruta_datos_fase2b}\")\n",
        "\n",
        "        if not self.config.ruta_datos_fase2b.exists():\n",
        "            raise FileNotFoundError(\n",
        "                f\"No se encuentra el archivo: {self.config.ruta_datos_fase2b}\\n\"\n",
        "                \"Ejecute Fase 2B primero.\"\n",
        "            )\n",
        "\n",
        "        df = pd.read_csv(self.config.ruta_datos_fase2b)\n",
        "\n",
        "        self.logger.info(f\"  Filas: {len(df)}\")\n",
        "        self.logger.info(f\"  Columnas: {len(df.columns)}\")\n",
        "        self.logger.info(f\"  Modelos: {df['modelo'].nunique()}\")\n",
        "        self.logger.info(f\"  Fotografias: {df['codigo_foto'].nunique()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def identificar_metricas_disponibles(self, df: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"\n",
        "        Identifica que metricas de segmentacion estan disponibles.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame con datos\n",
        "\n",
        "        Returns:\n",
        "            Lista de nombres de metricas disponibles\n",
        "        \"\"\"\n",
        "        metricas_disponibles = []\n",
        "\n",
        "        for metrica in METRICAS_SEGMENTACION:\n",
        "            if metrica in df.columns:\n",
        "                # Verificar que tiene valores no nulos\n",
        "                if df[metrica].notna().sum() > 0:\n",
        "                    metricas_disponibles.append(metrica)\n",
        "            else:\n",
        "                # Buscar alternativas\n",
        "                if metrica in METRICAS_ALTERNATIVAS:\n",
        "                    for alt in METRICAS_ALTERNATIVAS[metrica]:\n",
        "                        if alt in df.columns and df[alt].notna().sum() > 0:\n",
        "                            metricas_disponibles.append(alt)\n",
        "                            self.logger.info(f\"  Usando '{alt}' en lugar de '{metrica}'\")\n",
        "                            break\n",
        "\n",
        "        self.logger.info(f\"Metricas disponibles para PCA: {len(metricas_disponibles)}\")\n",
        "\n",
        "        return metricas_disponibles\n",
        "\n",
        "    def preparar_matriz_metricas(self, df: pd.DataFrame,\n",
        "                                  metricas: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Prepara la matriz de metricas para PCA, manejando valores faltantes.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame original\n",
        "            metricas: Lista de metricas a incluir\n",
        "\n",
        "        Returns:\n",
        "            Tupla (DataFrame con metricas, DataFrame con metadata)\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Preparando matriz de metricas para PCA...\")\n",
        "\n",
        "        # Extraer columnas de metricas\n",
        "        df_metricas = df[metricas].copy()\n",
        "\n",
        "        # Extraer metadata (identificadores)\n",
        "        cols_meta = ['modelo', 'config_codigo', 'codigo_foto']\n",
        "        cols_meta_disponibles = [c for c in cols_meta if c in df.columns]\n",
        "        df_meta = df[cols_meta_disponibles].copy()\n",
        "\n",
        "        # Reportar valores faltantes\n",
        "        nulos_por_metrica = df_metricas.isnull().sum()\n",
        "        if nulos_por_metrica.sum() > 0:\n",
        "            self.logger.warning(\"Valores faltantes detectados:\")\n",
        "            for col, nulos in nulos_por_metrica[nulos_por_metrica > 0].items():\n",
        "                self.logger.warning(f\"  {col}: {nulos} ({100*nulos/len(df):.1f}%)\")\n",
        "\n",
        "        # Eliminar filas con demasiados nulos (>50% de metricas)\n",
        "        umbral_nulos = len(metricas) * 0.5\n",
        "        filas_validas = df_metricas.isnull().sum(axis=1) < umbral_nulos\n",
        "\n",
        "        n_eliminadas = (~filas_validas).sum()\n",
        "        if n_eliminadas > 0:\n",
        "            self.logger.warning(f\"Eliminando {n_eliminadas} filas con >50% valores faltantes\")\n",
        "\n",
        "        df_metricas = df_metricas[filas_validas].copy()\n",
        "        df_meta = df_meta[filas_validas].copy()\n",
        "\n",
        "        # Imputar valores faltantes restantes con la mediana\n",
        "        for col in df_metricas.columns:\n",
        "            if df_metricas[col].isnull().any():\n",
        "                mediana = df_metricas[col].median()\n",
        "                df_metricas[col].fillna(mediana, inplace=True)\n",
        "\n",
        "        self.logger.info(f\"Matriz final: {df_metricas.shape[0]} filas x {df_metricas.shape[1]} metricas\")\n",
        "\n",
        "        return df_metricas, df_meta\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASE: ANALIZADOR PCA\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorPCA:\n",
        "    \"\"\"\n",
        "    Ejecuta analisis de componentes principales sobre metricas de segmentacion.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionFase2C, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        Inicializa el analizador PCA.\n",
        "\n",
        "        Args:\n",
        "            config: Configuracion de Fase 2C\n",
        "            logger: Logger para registro\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "        self.pca = None\n",
        "        self.X_scaled = None\n",
        "        self.X_pca = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def ejecutar_pca(self, df_metricas: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ejecuta PCA sobre la matriz de metricas.\n",
        "\n",
        "        Args:\n",
        "            df_metricas: DataFrame con metricas (filas=evaluaciones, cols=metricas)\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con resultados del PCA\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Ejecutando PCA...\")\n",
        "\n",
        "        self.feature_names = df_metricas.columns.tolist()\n",
        "\n",
        "        # Estandarizar datos\n",
        "        self.X_scaled = self.scaler.fit_transform(df_metricas)\n",
        "\n",
        "        # Ejecutar PCA\n",
        "        n_components = min(self.config.n_componentes_pca, len(self.feature_names))\n",
        "        self.pca = PCA(n_components=n_components, random_state=self.config.random_state)\n",
        "        self.X_pca = self.pca.fit_transform(self.X_scaled)\n",
        "\n",
        "        # Calcular varianza explicada\n",
        "        varianza_explicada = self.pca.explained_variance_ratio_\n",
        "        varianza_acumulada = np.cumsum(varianza_explicada)\n",
        "\n",
        "        self.logger.info(f\"  Componentes retenidos: {n_components}\")\n",
        "        self.logger.info(f\"  Varianza explicada total: {varianza_acumulada[-1]*100:.1f}%\")\n",
        "\n",
        "        for i, (var, acum) in enumerate(zip(varianza_explicada, varianza_acumulada)):\n",
        "            self.logger.info(f\"    PC{i+1}: {var*100:.1f}% (acumulada: {acum*100:.1f}%)\")\n",
        "\n",
        "        # Preparar resultados\n",
        "        resultados = {\n",
        "            'n_componentes': n_components,\n",
        "            'varianza_explicada': varianza_explicada.tolist(),\n",
        "            'varianza_acumulada': varianza_acumulada.tolist(),\n",
        "            'loadings': self.pca.components_.tolist(),\n",
        "            'feature_names': self.feature_names,\n",
        "            'scores': self.X_pca.tolist()\n",
        "        }\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def obtener_dataframe_scores(self, df_meta: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Genera DataFrame con scores PCA y metadata.\n",
        "\n",
        "        Args:\n",
        "            df_meta: DataFrame con metadata (modelo, config, foto)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con PC1, PC2, ... y metadata\n",
        "        \"\"\"\n",
        "        # Crear DataFrame de scores\n",
        "        cols_pc = [f'PC{i+1}' for i in range(self.X_pca.shape[1])]\n",
        "        df_scores = pd.DataFrame(self.X_pca, columns=cols_pc)\n",
        "\n",
        "        # Combinar con metadata\n",
        "        df_scores = pd.concat([df_meta.reset_index(drop=True), df_scores], axis=1)\n",
        "\n",
        "        return df_scores\n",
        "\n",
        "    def obtener_dataframe_loadings(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Genera DataFrame con loadings (pesos de cada variable en cada PC).\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con loadings\n",
        "        \"\"\"\n",
        "        cols_pc = [f'PC{i+1}' for i in range(self.pca.n_components_)]\n",
        "\n",
        "        df_loadings = pd.DataFrame(\n",
        "            self.pca.components_.T,\n",
        "            index=self.feature_names,\n",
        "            columns=cols_pc\n",
        "        )\n",
        "        df_loadings.index.name = 'metrica'\n",
        "\n",
        "        return df_loadings\n",
        "\n",
        "    def obtener_dataframe_varianza(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Genera DataFrame con varianza explicada por componente.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con varianza explicada\n",
        "        \"\"\"\n",
        "        n_comp = self.pca.n_components_\n",
        "\n",
        "        df_varianza = pd.DataFrame({\n",
        "            'componente': [f'PC{i+1}' for i in range(n_comp)],\n",
        "            'varianza_explicada': self.pca.explained_variance_ratio_,\n",
        "            'varianza_acumulada': np.cumsum(self.pca.explained_variance_ratio_),\n",
        "            'eigenvalue': self.pca.explained_variance_\n",
        "        })\n",
        "\n",
        "        return df_varianza\n",
        "\n",
        "    def identificar_variables_importantes(self, n_top: int = 5) -> Dict[str, List[Tuple[str, float]]]:\n",
        "        \"\"\"\n",
        "        Identifica las variables mas importantes para cada componente.\n",
        "\n",
        "        Args:\n",
        "            n_top: Numero de variables top a retornar por componente\n",
        "\n",
        "        Returns:\n",
        "            Diccionario {componente: [(variable, loading), ...]}\n",
        "        \"\"\"\n",
        "        resultado = {}\n",
        "\n",
        "        for i in range(self.pca.n_components_):\n",
        "            loadings = self.pca.components_[i]\n",
        "\n",
        "            # Ordenar por valor absoluto\n",
        "            indices_ordenados = np.argsort(np.abs(loadings))[::-1]\n",
        "\n",
        "            top_vars = [\n",
        "                (self.feature_names[idx], float(loadings[idx]))\n",
        "                for idx in indices_ordenados[:n_top]\n",
        "            ]\n",
        "\n",
        "            resultado[f'PC{i+1}'] = top_vars\n",
        "\n",
        "        return resultado\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASE: ANALIZADOR CLUSTERING\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorClustering:\n",
        "    \"\"\"\n",
        "    Ejecuta clustering de fotografias por nivel de dificultad de segmentacion.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionFase2C, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        Inicializa el analizador de clustering.\n",
        "\n",
        "        Args:\n",
        "            config: Configuracion de Fase 2C\n",
        "            logger: Logger para registro\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "        self.kmeans = None\n",
        "        self.features_clustering = None\n",
        "        self.labels = None\n",
        "\n",
        "    def preparar_features_fotografias(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Prepara features por fotografia para clustering.\n",
        "\n",
        "        Features utilizadas:\n",
        "        - IoU medio de todos los modelos\n",
        "        - Desviacion estandar del IoU entre modelos\n",
        "        - IoU maximo alcanzado\n",
        "        - IoU minimo alcanzado\n",
        "        - Numero de modelos con IoU > 0.8\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame fusionado\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con features por fotografia\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Preparando features de fotografias para clustering...\")\n",
        "\n",
        "        # Agregar por fotografia\n",
        "        features_foto = df.groupby('codigo_foto').agg({\n",
        "            'iou': ['mean', 'std', 'min', 'max', 'count']\n",
        "        }).round(4)\n",
        "\n",
        "        features_foto.columns = ['iou_mean', 'iou_std', 'iou_min', 'iou_max', 'n_evaluaciones']\n",
        "        features_foto = features_foto.reset_index()\n",
        "\n",
        "        # Calcular numero de modelos con IoU > 0.8\n",
        "        modelos_buenos = df[df['iou'] > 0.8].groupby('codigo_foto')['modelo'].nunique()\n",
        "        modelos_buenos = modelos_buenos.reset_index()\n",
        "        modelos_buenos.columns = ['codigo_foto', 'n_modelos_iou_alto']\n",
        "\n",
        "        # Combinar\n",
        "        features_foto = features_foto.merge(modelos_buenos, on='codigo_foto', how='left')\n",
        "        features_foto['n_modelos_iou_alto'].fillna(0, inplace=True)\n",
        "\n",
        "        # Calcular rango de IoU\n",
        "        features_foto['iou_rango'] = features_foto['iou_max'] - features_foto['iou_min']\n",
        "\n",
        "        self.logger.info(f\"  Fotografias: {len(features_foto)}\")\n",
        "        self.logger.info(f\"  Features: {features_foto.columns.tolist()}\")\n",
        "\n",
        "        return features_foto\n",
        "\n",
        "    def ejecutar_kmeans(self, df_features: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ejecuta clustering K-means sobre fotografias.\n",
        "\n",
        "        Args:\n",
        "            df_features: DataFrame con features por fotografia\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con resultados del clustering\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Ejecutando K-means con k={self.config.n_clusters}...\")\n",
        "\n",
        "        # Seleccionar features numericas para clustering\n",
        "        cols_features = ['iou_mean', 'iou_std', 'iou_rango', 'n_modelos_iou_alto']\n",
        "        self.features_clustering = df_features[cols_features].values\n",
        "\n",
        "        # Estandarizar\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(self.features_clustering)\n",
        "\n",
        "        # K-means\n",
        "        self.kmeans = KMeans(\n",
        "            n_clusters=self.config.n_clusters,\n",
        "            random_state=self.config.random_state,\n",
        "            n_init=10\n",
        "        )\n",
        "        self.labels = self.kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        # Metricas de calidad del clustering\n",
        "        silhouette = silhouette_score(X_scaled, self.labels)\n",
        "        calinski = calinski_harabasz_score(X_scaled, self.labels)\n",
        "\n",
        "        self.logger.info(f\"  Silhouette Score: {silhouette:.3f}\")\n",
        "        self.logger.info(f\"  Calinski-Harabasz Index: {calinski:.1f}\")\n",
        "\n",
        "        # Reordenar clusters por IoU medio (0=facil, 2=dificil)\n",
        "        df_temp = df_features.copy()\n",
        "        df_temp['cluster_original'] = self.labels\n",
        "\n",
        "        medias_cluster = df_temp.groupby('cluster_original')['iou_mean'].mean()\n",
        "        orden_clusters = medias_cluster.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "        mapeo_clusters = {old: new for new, old in enumerate(orden_clusters)}\n",
        "        self.labels = np.array([mapeo_clusters[l] for l in self.labels])\n",
        "\n",
        "        self.logger.info(\"  Clusters reordenados por IoU medio (0=Facil, 2=Dificil)\")\n",
        "\n",
        "        # Caracterizar clusters\n",
        "        df_temp['cluster'] = self.labels\n",
        "        caracterizacion = {}\n",
        "\n",
        "        for cluster_id in range(self.config.n_clusters):\n",
        "            df_c = df_temp[df_temp['cluster'] == cluster_id]\n",
        "            caracterizacion[cluster_id] = {\n",
        "                'nombre': NOMBRES_CLUSTERS.get(cluster_id, f'Cluster {cluster_id}'),\n",
        "                'n_fotos': int(len(df_c)),\n",
        "                'iou_mean': float(df_c['iou_mean'].mean()),\n",
        "                'iou_std_mean': float(df_c['iou_std'].mean()),\n",
        "                'fotos': df_c['codigo_foto'].tolist()\n",
        "            }\n",
        "\n",
        "            self.logger.info(\n",
        "                f\"    Cluster {cluster_id} ({NOMBRES_CLUSTERS.get(cluster_id, '')}): \"\n",
        "                f\"{len(df_c)} fotos, IoU medio={df_c['iou_mean'].mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        resultados = {\n",
        "            'n_clusters': self.config.n_clusters,\n",
        "            'silhouette_score': float(silhouette),\n",
        "            'calinski_harabasz': float(calinski),\n",
        "            'caracterizacion': caracterizacion\n",
        "        }\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def ejecutar_clustering_jerarquico(self, df_features: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Ejecuta clustering jerarquico para generar dendrograma.\n",
        "\n",
        "        Args:\n",
        "            df_features: DataFrame con features por fotografia\n",
        "\n",
        "        Returns:\n",
        "            Matriz de linkage para dendrograma\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Ejecutando clustering jerarquico...\")\n",
        "\n",
        "        cols_features = ['iou_mean', 'iou_std', 'iou_rango']\n",
        "        X = df_features[cols_features].values\n",
        "\n",
        "        # Estandarizar\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # Linkage jerarquico (Ward)\n",
        "        linkage_matrix = linkage(X_scaled, method='ward')\n",
        "\n",
        "        return linkage_matrix\n",
        "\n",
        "    def obtener_dataframe_clusters(self, df_features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Genera DataFrame con asignacion de clusters.\n",
        "\n",
        "        Args:\n",
        "            df_features: DataFrame con features por fotografia\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con codigo_foto, cluster, nombre_cluster\n",
        "        \"\"\"\n",
        "        df_clusters = df_features[['codigo_foto']].copy()\n",
        "        df_clusters['cluster'] = self.labels\n",
        "        df_clusters['nombre_cluster'] = df_clusters['cluster'].map(NOMBRES_CLUSTERS)\n",
        "\n",
        "        # Anadir features originales\n",
        "        for col in ['iou_mean', 'iou_std', 'iou_rango']:\n",
        "            if col in df_features.columns:\n",
        "                df_clusters[col] = df_features[col].values\n",
        "\n",
        "        return df_clusters\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASE: ANALIZADOR DE CORRELACIONES\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorCorrelacionesMetricas:\n",
        "    \"\"\"\n",
        "    Analiza correlaciones entre metricas para identificar redundancias.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionFase2C, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        Inicializa el analizador de correlaciones.\n",
        "\n",
        "        Args:\n",
        "            config: Configuracion de Fase 2C\n",
        "            logger: Logger para registro\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "        self.matriz_correlacion = None\n",
        "\n",
        "    def calcular_matriz_correlacion(self, df_metricas: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calcula matriz de correlacion de Pearson entre metricas.\n",
        "\n",
        "        Args:\n",
        "            df_metricas: DataFrame con metricas\n",
        "\n",
        "        Returns:\n",
        "            Matriz de correlacion\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Calculando matriz de correlacion entre metricas...\")\n",
        "\n",
        "        self.matriz_correlacion = df_metricas.corr(method='pearson')\n",
        "\n",
        "        self.logger.info(f\"  Dimensiones: {self.matriz_correlacion.shape}\")\n",
        "\n",
        "        return self.matriz_correlacion\n",
        "\n",
        "    def identificar_metricas_redundantes(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Identifica pares de metricas con correlacion |r| > umbral.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame con pares redundantes\n",
        "        \"\"\"\n",
        "        self.logger.info(\n",
        "            f\"Identificando metricas redundantes (|r| > {self.config.umbral_redundancia})...\"\n",
        "        )\n",
        "\n",
        "        if self.matriz_correlacion is None:\n",
        "            raise ValueError(\"Ejecute calcular_matriz_correlacion primero\")\n",
        "\n",
        "        redundantes = []\n",
        "\n",
        "        # Recorrer triangulo superior de la matriz\n",
        "        n = len(self.matriz_correlacion)\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                r = self.matriz_correlacion.iloc[i, j]\n",
        "\n",
        "                if abs(r) > self.config.umbral_redundancia:\n",
        "                    redundantes.append({\n",
        "                        'metrica_1': self.matriz_correlacion.index[i],\n",
        "                        'metrica_2': self.matriz_correlacion.columns[j],\n",
        "                        'correlacion': float(r),\n",
        "                        'correlacion_abs': float(abs(r))\n",
        "                    })\n",
        "\n",
        "        df_redundantes = pd.DataFrame(redundantes)\n",
        "\n",
        "        if len(df_redundantes) > 0:\n",
        "            df_redundantes = df_redundantes.sort_values('correlacion_abs', ascending=False)\n",
        "            self.logger.info(f\"  Pares redundantes encontrados: {len(df_redundantes)}\")\n",
        "        else:\n",
        "            self.logger.info(\"  No se encontraron metricas redundantes\")\n",
        "\n",
        "        return df_redundantes\n",
        "\n",
        "    def sugerir_metricas_a_eliminar(self, df_redundantes: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"\n",
        "        Sugiere metricas a eliminar para reducir redundancia.\n",
        "\n",
        "        Criterio: Eliminar la metrica menos interpretable del par.\n",
        "\n",
        "        Args:\n",
        "            df_redundantes: DataFrame con pares redundantes\n",
        "\n",
        "        Returns:\n",
        "            Lista de metricas sugeridas para eliminacion\n",
        "        \"\"\"\n",
        "        # Metricas prioritarias a mantener (mas interpretables)\n",
        "        prioridad = ['iou', 'dice', 'precision', 'recall', 'boundary_iou']\n",
        "\n",
        "        a_eliminar = set()\n",
        "\n",
        "        for _, row in df_redundantes.iterrows():\n",
        "            m1, m2 = row['metrica_1'], row['metrica_2']\n",
        "\n",
        "            # Si una esta en prioridad y otra no, eliminar la no prioritaria\n",
        "            m1_prioritaria = m1 in prioridad\n",
        "            m2_prioritaria = m2 in prioridad\n",
        "\n",
        "            if m1_prioritaria and not m2_prioritaria:\n",
        "                a_eliminar.add(m2)\n",
        "            elif m2_prioritaria and not m1_prioritaria:\n",
        "                a_eliminar.add(m1)\n",
        "            else:\n",
        "                # Si ambas o ninguna es prioritaria, eliminar la segunda\n",
        "                a_eliminar.add(m2)\n",
        "\n",
        "        return list(a_eliminar)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASE: GENERADOR DE VISUALIZACIONES\n",
        "# =============================================================================\n",
        "\n",
        "class GeneradorVisualizaciones:\n",
        "    \"\"\"\n",
        "    Genera visualizaciones para Fase 2C.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionFase2C, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        Inicializa el generador de visualizaciones.\n",
        "\n",
        "        Args:\n",
        "            config: Configuracion de Fase 2C\n",
        "            logger: Logger para registro\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "        self.ruta_visualizaciones = config.ruta_salida / \"visualizaciones\"\n",
        "\n",
        "    def _crear_directorio(self):\n",
        "        \"\"\"Crea el directorio de visualizaciones si no existe.\"\"\"\n",
        "        self.ruta_visualizaciones.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def generar_varianza_explicada(self, df_varianza: pd.DataFrame) -> Path:\n",
        "        \"\"\"\n",
        "        Genera grafico de varianza explicada por componente (scree plot).\n",
        "\n",
        "        Args:\n",
        "            df_varianza: DataFrame con varianza por componente\n",
        "\n",
        "        Returns:\n",
        "            Ruta al archivo generado\n",
        "        \"\"\"\n",
        "        self._crear_directorio()\n",
        "        self.logger.info(\"Generando grafico de varianza explicada...\")\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        x = range(1, len(df_varianza) + 1)\n",
        "\n",
        "        # Barras de varianza individual\n",
        "        bars = ax.bar(x, df_varianza['varianza_explicada'] * 100,\n",
        "                     color='steelblue', alpha=0.7, label='Varianza individual')\n",
        "\n",
        "        # Linea de varianza acumulada\n",
        "        ax.plot(x, df_varianza['varianza_acumulada'] * 100,\n",
        "               'ro-', linewidth=2, markersize=8, label='Varianza acumulada')\n",
        "\n",
        "        # Linea de referencia 80%\n",
        "        ax.axhline(y=80, color='gray', linestyle='--', alpha=0.5, label='80% varianza')\n",
        "\n",
        "        # Etiquetas en barras\n",
        "        for bar, val in zip(bars, df_varianza['varianza_explicada']):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                   f'{val*100:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        ax.set_xlabel('Componente Principal')\n",
        "        ax.set_ylabel('Varianza Explicada (%)')\n",
        "        ax.set_title('PCA: Varianza Explicada por Componente', fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels([f'PC{i}' for i in x])\n",
        "        ax.legend(loc='center right')\n",
        "        ax.set_ylim(0, 105)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        ruta = self.ruta_visualizaciones / 'pca_varianza_explicada.png'\n",
        "        fig.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        return ruta\n",
        "\n",
        "    def generar_biplot(self, analizador_pca: AnalizadorPCA,\n",
        "                       df_meta: pd.DataFrame) -> Path:\n",
        "        \"\"\"\n",
        "        Genera biplot de PCA (scores + loadings).\n",
        "\n",
        "        Args:\n",
        "            analizador_pca: Analizador PCA ejecutado\n",
        "            df_meta: DataFrame con metadata (modelo)\n",
        "\n",
        "        Returns:\n",
        "            Ruta al archivo generado\n",
        "        \"\"\"\n",
        "        self._crear_directorio()\n",
        "        self.logger.info(\"Generando biplot PCA...\")\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        # Scores (puntos)\n",
        "        scores = analizador_pca.X_pca[:, :2]\n",
        "        modelos = df_meta['modelo'].values\n",
        "\n",
        "        for modelo in np.unique(modelos):\n",
        "            mask = modelos == modelo\n",
        "            color = COLORES_MODELOS.get(modelo, '#666666')\n",
        "            ax.scatter(scores[mask, 0], scores[mask, 1],\n",
        "                      c=color, label=modelo, alpha=0.6, s=30)\n",
        "\n",
        "        # Loadings (flechas)\n",
        "        loadings = analizador_pca.pca.components_[:2, :].T\n",
        "        feature_names = analizador_pca.feature_names\n",
        "\n",
        "        # Escalar loadings para visualizacion\n",
        "        scale = np.abs(scores).max() / np.abs(loadings).max() * 0.8\n",
        "\n",
        "        for i, (name, loading) in enumerate(zip(feature_names, loadings)):\n",
        "            ax.arrow(0, 0, loading[0] * scale, loading[1] * scale,\n",
        "                    head_width=0.15, head_length=0.1, fc='red', ec='red', alpha=0.7)\n",
        "\n",
        "            # Etiqueta\n",
        "            ax.text(loading[0] * scale * 1.15, loading[1] * scale * 1.15,\n",
        "                   name, fontsize=8, ha='center', va='center', color='darkred')\n",
        "\n",
        "        ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
        "        ax.axvline(x=0, color='gray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel(f'PC1 ({analizador_pca.pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
        "        ax.set_ylabel(f'PC2 ({analizador_pca.pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
        "        ax.set_title('PCA Biplot: Evaluaciones de Segmentacion', fontweight='bold')\n",
        "        ax.legend(loc='upper right', title='Modelo')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        ruta = self.ruta_visualizaciones / 'pca_biplot.png'\n",
        "        fig.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        return ruta\n",
        "\n",
        "    def generar_scatter_pca_clusters(self, df_scores: pd.DataFrame,\n",
        "                                      df_clusters: pd.DataFrame) -> Path:\n",
        "        \"\"\"\n",
        "        Genera scatter plot de PC1 vs PC2 coloreado por cluster de foto.\n",
        "\n",
        "        Args:\n",
        "            df_scores: DataFrame con scores PCA\n",
        "            df_clusters: DataFrame con asignacion de clusters\n",
        "\n",
        "        Returns:\n",
        "            Ruta al archivo generado\n",
        "        \"\"\"\n",
        "        self._crear_directorio()\n",
        "        self.logger.info(\"Generando scatter PCA con clusters...\")\n",
        "\n",
        "        # Merge scores con clusters\n",
        "        df_plot = df_scores.merge(\n",
        "            df_clusters[['codigo_foto', 'cluster', 'nombre_cluster']],\n",
        "            on='codigo_foto',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        for cluster_id in sorted(df_plot['cluster'].unique()):\n",
        "            mask = df_plot['cluster'] == cluster_id\n",
        "            nombre = NOMBRES_CLUSTERS.get(cluster_id, f'Cluster {cluster_id}')\n",
        "            color = COLORES_CLUSTERS.get(cluster_id, '#666666')\n",
        "\n",
        "            ax.scatter(\n",
        "                df_plot.loc[mask, 'PC1'],\n",
        "                df_plot.loc[mask, 'PC2'],\n",
        "                c=color, label=f'{nombre}', alpha=0.6, s=40, edgecolors='white'\n",
        "            )\n",
        "\n",
        "        ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
        "        ax.axvline(x=0, color='gray', linestyle='-', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel('PC1')\n",
        "        ax.set_ylabel('PC2')\n",
        "        ax.set_title('PCA: Evaluaciones por Nivel de Dificultad de Fotografia', fontweight='bold')\n",
        "        ax.legend(title='Dificultad')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        ruta = self.ruta_visualizaciones / 'pca_scatter_clusters.png'\n",
        "        fig.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        return ruta\n",
        "\n",
        "    def generar_dendrograma(self, linkage_matrix: np.ndarray,\n",
        "                            codigos_foto: List[str]) -> Path:\n",
        "        \"\"\"\n",
        "        Genera dendrograma del clustering jerarquico.\n",
        "\n",
        "        Args:\n",
        "            linkage_matrix: Matriz de linkage\n",
        "            codigos_foto: Lista de codigos de fotografia\n",
        "\n",
        "        Returns:\n",
        "            Ruta al archivo generado\n",
        "        \"\"\"\n",
        "        self._crear_directorio()\n",
        "        self.logger.info(\"Generando dendrograma...\")\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "        # Simplificar etiquetas\n",
        "        etiquetas = [c.replace('_DSC', '').replace('_', '') for c in codigos_foto]\n",
        "\n",
        "        dendrogram(\n",
        "            linkage_matrix,\n",
        "            labels=etiquetas,\n",
        "            leaf_rotation=90,\n",
        "            leaf_font_size=9,\n",
        "            ax=ax,\n",
        "            color_threshold=0.7 * max(linkage_matrix[:, 2])\n",
        "        )\n",
        "\n",
        "        ax.set_xlabel('Fotografia')\n",
        "        ax.set_ylabel('Distancia (Ward)')\n",
        "        ax.set_title('Dendrograma: Clustering Jerarquico de Fotografias por Dificultad',\n",
        "                    fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        ruta = self.ruta_visualizaciones / 'dendrograma_fotografias.png'\n",
        "        fig.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        return ruta\n",
        "\n",
        "    def generar_heatmap_correlaciones(self, matriz_corr: pd.DataFrame) -> Path:\n",
        "        \"\"\"\n",
        "        Genera heatmap de la matriz de correlaciones.\n",
        "\n",
        "        Args:\n",
        "            matriz_corr: Matriz de correlacion\n",
        "\n",
        "        Returns:\n",
        "            Ruta al archivo generado\n",
        "        \"\"\"\n",
        "        self._crear_directorio()\n",
        "        self.logger.info(\"Generando heatmap de correlaciones...\")\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(14, 12))\n",
        "\n",
        "        # Mascara para triangulo superior\n",
        "        mask = np.triu(np.ones_like(matriz_corr, dtype=bool), k=1)\n",
        "\n",
        "        sns.heatmap(\n",
        "            matriz_corr,\n",
        "            mask=mask,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='RdBu_r',\n",
        "            center=0,\n",
        "            vmin=-1,\n",
        "            vmax=1,\n",
        "            square=True,\n",
        "            linewidths=0.5,\n",
        "            cbar_kws={'shrink': 0.8, 'label': 'Correlacion de Pearson'},\n",
        "            ax=ax,\n",
        "            annot_kws={'size': 8}\n",
        "        )\n",
        "\n",
        "        ax.set_title('Matriz de Correlaciones entre Metricas de Segmentacion',\n",
        "                    fontweight='bold', pad=20)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        ruta = self.ruta_visualizaciones / 'heatmap_correlaciones.png'\n",
        "        fig.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        return ruta\n",
        "\n",
        "    def generar_caracterizacion_clusters(self, df_features: pd.DataFrame,\n",
        "                                          labels: np.ndarray) -> Path:\n",
        "        \"\"\"\n",
        "        Genera visualizacion de caracterizacion de clusters.\n",
        "\n",
        "        Args:\n",
        "            df_features: DataFrame con features por fotografia\n",
        "            labels: Array con etiquetas de cluster\n",
        "\n",
        "        Returns:\n",
        "            Ruta al archivo generado\n",
        "        \"\"\"\n",
        "        self._crear_directorio()\n",
        "        self.logger.info(\"Generando caracterizacion de clusters...\")\n",
        "\n",
        "        df_plot = df_features.copy()\n",
        "        df_plot['cluster'] = labels\n",
        "        df_plot['nombre_cluster'] = df_plot['cluster'].map(NOMBRES_CLUSTERS)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "        # 1. Box plot IoU medio por cluster\n",
        "        ax = axes[0, 0]\n",
        "        colores = [COLORES_CLUSTERS[i] for i in sorted(df_plot['cluster'].unique())]\n",
        "\n",
        "        df_plot.boxplot(column='iou_mean', by='nombre_cluster', ax=ax,\n",
        "                       patch_artist=True)\n",
        "\n",
        "        for patch, color in zip(ax.patches, colores):\n",
        "            patch.set_facecolor(color)\n",
        "            patch.set_alpha(0.7)\n",
        "\n",
        "        ax.set_title('IoU Medio por Cluster')\n",
        "        ax.set_xlabel('Cluster')\n",
        "        ax.set_ylabel('IoU Medio')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        # 2. Box plot variabilidad por cluster\n",
        "        ax = axes[0, 1]\n",
        "        df_plot.boxplot(column='iou_std', by='nombre_cluster', ax=ax,\n",
        "                       patch_artist=True)\n",
        "\n",
        "        for patch, color in zip(ax.patches, colores):\n",
        "            patch.set_facecolor(color)\n",
        "            patch.set_alpha(0.7)\n",
        "\n",
        "        ax.set_title('Variabilidad IoU por Cluster')\n",
        "        ax.set_xlabel('Cluster')\n",
        "        ax.set_ylabel('Desviacion Estandar IoU')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        # 3. Scatter IoU medio vs variabilidad\n",
        "        ax = axes[1, 0]\n",
        "        for cluster_id in sorted(df_plot['cluster'].unique()):\n",
        "            mask = df_plot['cluster'] == cluster_id\n",
        "            ax.scatter(\n",
        "                df_plot.loc[mask, 'iou_mean'],\n",
        "                df_plot.loc[mask, 'iou_std'],\n",
        "                c=COLORES_CLUSTERS[cluster_id],\n",
        "                label=NOMBRES_CLUSTERS[cluster_id],\n",
        "                s=80, alpha=0.7, edgecolors='white'\n",
        "            )\n",
        "\n",
        "            # Etiquetas de fotos\n",
        "            for _, row in df_plot[mask].iterrows():\n",
        "                ax.annotate(\n",
        "                    row['codigo_foto'].replace('_DSC', ''),\n",
        "                    (row['iou_mean'], row['iou_std']),\n",
        "                    fontsize=7, alpha=0.7\n",
        "                )\n",
        "\n",
        "        ax.set_xlabel('IoU Medio')\n",
        "        ax.set_ylabel('Variabilidad IoU (std)')\n",
        "        ax.set_title('Relacion IoU Medio vs Variabilidad')\n",
        "        ax.legend()\n",
        "\n",
        "        # 4. Barras de conteo por cluster\n",
        "        ax = axes[1, 1]\n",
        "        conteos = df_plot.groupby('nombre_cluster').size()\n",
        "        conteos = conteos.reindex(['Facil', 'Medio', 'Dificil'])\n",
        "\n",
        "        bars = ax.bar(conteos.index, conteos.values,\n",
        "                     color=[COLORES_CLUSTERS[i] for i in range(len(conteos))],\n",
        "                     alpha=0.7, edgecolor='black')\n",
        "\n",
        "        for bar, val in zip(bars, conteos.values):\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
        "                   str(val), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel('Cluster')\n",
        "        ax.set_ylabel('Numero de Fotografias')\n",
        "        ax.set_title('Distribucion de Fotografias por Cluster')\n",
        "\n",
        "        fig.suptitle('Caracterizacion de Clusters de Dificultad',\n",
        "                    fontsize=14, fontweight='bold', y=1.02)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        ruta = self.ruta_visualizaciones / 'clusters_caracterizacion.png'\n",
        "        fig.savefig(ruta, dpi=300, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        return ruta\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASE: GENERADOR DE REPORTES\n",
        "# =============================================================================\n",
        "\n",
        "class GeneradorReporte:\n",
        "    \"\"\"\n",
        "    Genera reporte markdown con resultados de Fase 2C.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionFase2C, logger: logging.Logger):\n",
        "        \"\"\"\n",
        "        Inicializa el generador de reportes.\n",
        "\n",
        "        Args:\n",
        "            config: Configuracion de Fase 2C\n",
        "            logger: Logger para registro\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def generar_reporte(self, resultados_pca: Dict,\n",
        "                        resultados_clustering: Dict,\n",
        "                        df_redundantes: pd.DataFrame,\n",
        "                        rutas_visualizaciones: Dict[str, Path]) -> Path:\n",
        "        \"\"\"\n",
        "        Genera reporte markdown completo.\n",
        "\n",
        "        Args:\n",
        "            resultados_pca: Resultados del analisis PCA\n",
        "            resultados_clustering: Resultados del clustering\n",
        "            df_redundantes: DataFrame con metricas redundantes\n",
        "            rutas_visualizaciones: Diccionario con rutas a visualizaciones\n",
        "\n",
        "        Returns:\n",
        "            Ruta al reporte generado\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Generando reporte markdown...\")\n",
        "\n",
        "        lineas = [\n",
        "            \"# REPORTE FASE 2C - PCA Y CLUSTERING\",\n",
        "            \"\",\n",
        "            f\"**Fecha de generacion:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            \"\",\n",
        "            \"---\",\n",
        "            \"\",\n",
        "            \"## 1. Resumen Ejecutivo\",\n",
        "            \"\",\n",
        "            \"Este reporte presenta los resultados del analisis exploratorio multivariante\",\n",
        "            \"sobre las metricas de segmentacion, incluyendo reduccion de dimensionalidad (PCA),\",\n",
        "            \"clustering de fotografias por dificultad, y analisis de redundancia entre metricas.\",\n",
        "            \"\",\n",
        "            \"---\",\n",
        "            \"\",\n",
        "            \"## 2. Analisis de Componentes Principales (PCA)\",\n",
        "            \"\",\n",
        "            f\"**Componentes retenidos:** {resultados_pca['n_componentes']}\",\n",
        "            \"\",\n",
        "            \"### Varianza Explicada\",\n",
        "            \"\",\n",
        "            \"| Componente | Varianza (%) | Acumulada (%) |\",\n",
        "            \"|------------|--------------|---------------|\",\n",
        "        ]\n",
        "\n",
        "        for i, (var, acum) in enumerate(zip(\n",
        "            resultados_pca['varianza_explicada'],\n",
        "            resultados_pca['varianza_acumulada']\n",
        "        )):\n",
        "            lineas.append(f\"| PC{i+1} | {var*100:.2f} | {acum*100:.2f} |\")\n",
        "\n",
        "        lineas.extend([\n",
        "            \"\",\n",
        "            f\"![Varianza Explicada](visualizaciones/pca_varianza_explicada.png)\",\n",
        "            \"\",\n",
        "            \"### Interpretacion\",\n",
        "            \"\",\n",
        "            f\"Los primeros {resultados_pca['n_componentes']} componentes explican el \"\n",
        "            f\"{resultados_pca['varianza_acumulada'][-1]*100:.1f}% de la varianza total.\",\n",
        "            \"\",\n",
        "            \"---\",\n",
        "            \"\",\n",
        "            \"## 3. Clustering de Fotografias\",\n",
        "            \"\",\n",
        "            f\"**Metodo:** K-means con k={resultados_clustering['n_clusters']}\",\n",
        "            \"\",\n",
        "            f\"**Silhouette Score:** {resultados_clustering['silhouette_score']:.3f}\",\n",
        "            \"\",\n",
        "            f\"**Calinski-Harabasz Index:** {resultados_clustering['calinski_harabasz']:.1f}\",\n",
        "            \"\",\n",
        "            \"### Caracterizacion de Clusters\",\n",
        "            \"\",\n",
        "            \"| Cluster | Nombre | N Fotos | IoU Medio |\",\n",
        "            \"|---------|--------|---------|-----------|\",\n",
        "        ])\n",
        "\n",
        "        for cluster_id, info in resultados_clustering['caracterizacion'].items():\n",
        "            lineas.append(\n",
        "                f\"| {cluster_id} | {info['nombre']} | {info['n_fotos']} | \"\n",
        "                f\"{info['iou_mean']:.4f} |\"\n",
        "            )\n",
        "\n",
        "        lineas.extend([\n",
        "            \"\",\n",
        "            f\"![Caracterizacion Clusters](visualizaciones/clusters_caracterizacion.png)\",\n",
        "            \"\",\n",
        "            \"### Fotografias por Cluster\",\n",
        "            \"\",\n",
        "        ])\n",
        "\n",
        "        for cluster_id, info in resultados_clustering['caracterizacion'].items():\n",
        "            fotos = \", \".join(info['fotos'][:5])\n",
        "            if len(info['fotos']) > 5:\n",
        "                fotos += f\" ... (+{len(info['fotos'])-5} mas)\"\n",
        "            lineas.append(f\"**{info['nombre']}:** {fotos}\")\n",
        "            lineas.append(\"\")\n",
        "\n",
        "        lineas.extend([\n",
        "            \"---\",\n",
        "            \"\",\n",
        "            \"## 4. Analisis de Redundancia\",\n",
        "            \"\",\n",
        "            f\"**Umbral de redundancia:** |r| > {self.config.umbral_redundancia}\",\n",
        "            \"\",\n",
        "        ])\n",
        "\n",
        "        if len(df_redundantes) > 0:\n",
        "            lineas.extend([\n",
        "                f\"**Pares redundantes encontrados:** {len(df_redundantes)}\",\n",
        "                \"\",\n",
        "                \"| Metrica 1 | Metrica 2 | Correlacion |\",\n",
        "                \"|-----------|-----------|-------------|\",\n",
        "            ])\n",
        "\n",
        "            for _, row in df_redundantes.head(10).iterrows():\n",
        "                lineas.append(\n",
        "                    f\"| {row['metrica_1']} | {row['metrica_2']} | {row['correlacion']:.3f} |\"\n",
        "                )\n",
        "\n",
        "            if len(df_redundantes) > 10:\n",
        "                lineas.append(f\"| ... | ... | ... |\")\n",
        "                lineas.append(f\"| (Total: {len(df_redundantes)} pares) | | |\")\n",
        "        else:\n",
        "            lineas.append(\"No se encontraron metricas redundantes con el umbral especificado.\")\n",
        "\n",
        "        lineas.extend([\n",
        "            \"\",\n",
        "            f\"![Heatmap Correlaciones](visualizaciones/heatmap_correlaciones.png)\",\n",
        "            \"\",\n",
        "            \"---\",\n",
        "            \"\",\n",
        "            \"## 5. Archivos Generados\",\n",
        "            \"\",\n",
        "            \"| Archivo | Descripcion |\",\n",
        "            \"|---------|-------------|\",\n",
        "            \"| pca_componentes.csv | Scores PC1-PCn por evaluacion |\",\n",
        "            \"| pca_varianza_explicada.csv | Varianza por componente |\",\n",
        "            \"| pca_loadings.csv | Pesos de metricas en cada PC |\",\n",
        "            \"| clusters_fotografias.csv | Asignacion de cluster por foto |\",\n",
        "            \"| correlaciones_metricas.csv | Matriz de correlaciones |\",\n",
        "            \"| metricas_redundantes.csv | Pares con alta correlacion |\",\n",
        "            \"\",\n",
        "            \"---\",\n",
        "            \"\",\n",
        "            \"*Reporte generado automaticamente por Fase 2C*\",\n",
        "        ])\n",
        "\n",
        "        contenido = \"\\n\".join(lineas)\n",
        "\n",
        "        ruta = self.config.ruta_salida / \"REPORTE_FASE2C.md\"\n",
        "        with open(ruta, 'w', encoding='utf-8') as f:\n",
        "            f.write(contenido)\n",
        "\n",
        "        return ruta\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASE: ORQUESTADOR FASE 2C\n",
        "# =============================================================================\n",
        "\n",
        "class OrquestadorFase2C:\n",
        "    \"\"\"\n",
        "    Orquesta la ejecucion completa de Fase 2C.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionFase2C):\n",
        "        \"\"\"\n",
        "        Inicializa el orquestador.\n",
        "\n",
        "        Args:\n",
        "            config: Configuracion de Fase 2C\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = self._configurar_logger()\n",
        "\n",
        "        # Componentes\n",
        "        self.cargador = CargadorDatosFase2C(config, self.logger)\n",
        "        self.analizador_pca = AnalizadorPCA(config, self.logger)\n",
        "        self.analizador_clustering = AnalizadorClustering(config, self.logger)\n",
        "        self.analizador_correlaciones = AnalizadorCorrelacionesMetricas(config, self.logger)\n",
        "        self.generador_viz = GeneradorVisualizaciones(config, self.logger)\n",
        "        self.generador_reporte = GeneradorReporte(config, self.logger)\n",
        "\n",
        "        # Resultados\n",
        "        self.df_original = None\n",
        "        self.df_metricas = None\n",
        "        self.df_meta = None\n",
        "        self.metricas_disponibles = None\n",
        "        self.resultados_pca = None\n",
        "        self.resultados_clustering = None\n",
        "        self.df_redundantes = None\n",
        "        self.rutas_visualizaciones = {}\n",
        "\n",
        "    def _configurar_logger(self) -> logging.Logger:\n",
        "        \"\"\"Configura el logger para Fase 2C.\"\"\"\n",
        "        logger = logging.getLogger('Fase2C')\n",
        "        logger.setLevel(logging.INFO)\n",
        "\n",
        "        # Limpiar handlers existentes\n",
        "        logger.handlers = []\n",
        "\n",
        "        handler = logging.StreamHandler(sys.stdout)\n",
        "        handler.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter(\n",
        "            '[%(asctime)s] %(levelname)-8s | %(message)s',\n",
        "            datefmt='%H:%M:%S'\n",
        "        )\n",
        "        handler.setFormatter(formatter)\n",
        "        logger.addHandler(handler)\n",
        "\n",
        "        return logger\n",
        "\n",
        "    def ejecutar(self) -> None:\n",
        "        \"\"\"\n",
        "        Ejecuta el pipeline completo de Fase 2C.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"=\" * 70)\n",
        "        self.logger.info(\"FASE 2C - PCA, CLUSTERING Y ANALISIS DE REDUNDANCIA\")\n",
        "        self.logger.info(\"=\" * 70)\n",
        "\n",
        "        inicio = datetime.now()\n",
        "\n",
        "        # Paso 1: Cargar datos\n",
        "        self.logger.info(\"\\n[PASO 1/8] Cargando datos de Fase 2B...\")\n",
        "        self.df_original = self.cargador.cargar_datos_fusionados()\n",
        "\n",
        "        # Paso 2: Identificar metricas disponibles\n",
        "        self.logger.info(\"\\n[PASO 2/8] Identificando metricas disponibles...\")\n",
        "        self.metricas_disponibles = self.cargador.identificar_metricas_disponibles(\n",
        "            self.df_original\n",
        "        )\n",
        "\n",
        "        if len(self.metricas_disponibles) < 3:\n",
        "            raise ValueError(\n",
        "                f\"Insuficientes metricas para PCA: {len(self.metricas_disponibles)}\"\n",
        "            )\n",
        "\n",
        "        # Paso 3: Preparar matriz de metricas\n",
        "        self.logger.info(\"\\n[PASO 3/8] Preparando matriz de metricas...\")\n",
        "        self.df_metricas, self.df_meta = self.cargador.preparar_matriz_metricas(\n",
        "            self.df_original, self.metricas_disponibles\n",
        "        )\n",
        "\n",
        "        # Paso 4: Ejecutar PCA\n",
        "        self.logger.info(\"\\n[PASO 4/8] Ejecutando PCA...\")\n",
        "        self.resultados_pca = self.analizador_pca.ejecutar_pca(self.df_metricas)\n",
        "\n",
        "        # Paso 5: Ejecutar clustering\n",
        "        self.logger.info(\"\\n[PASO 5/8] Ejecutando clustering de fotografias...\")\n",
        "        df_features_foto = self.analizador_clustering.preparar_features_fotografias(\n",
        "            self.df_original\n",
        "        )\n",
        "        self.resultados_clustering = self.analizador_clustering.ejecutar_kmeans(df_features_foto)\n",
        "        linkage_matrix = self.analizador_clustering.ejecutar_clustering_jerarquico(df_features_foto)\n",
        "\n",
        "        # Paso 6: Analizar correlaciones\n",
        "        self.logger.info(\"\\n[PASO 6/8] Analizando correlaciones entre metricas...\")\n",
        "        matriz_corr = self.analizador_correlaciones.calcular_matriz_correlacion(self.df_metricas)\n",
        "        self.df_redundantes = self.analizador_correlaciones.identificar_metricas_redundantes()\n",
        "\n",
        "        # Paso 7: Generar visualizaciones\n",
        "        self.logger.info(\"\\n[PASO 7/8] Generando visualizaciones...\")\n",
        "\n",
        "        df_varianza = self.analizador_pca.obtener_dataframe_varianza()\n",
        "        self.rutas_visualizaciones['varianza'] = self.generador_viz.generar_varianza_explicada(\n",
        "            df_varianza\n",
        "        )\n",
        "\n",
        "        self.rutas_visualizaciones['biplot'] = self.generador_viz.generar_biplot(\n",
        "            self.analizador_pca, self.df_meta\n",
        "        )\n",
        "\n",
        "        df_scores = self.analizador_pca.obtener_dataframe_scores(self.df_meta)\n",
        "        df_clusters = self.analizador_clustering.obtener_dataframe_clusters(df_features_foto)\n",
        "\n",
        "        self.rutas_visualizaciones['scatter_clusters'] = self.generador_viz.generar_scatter_pca_clusters(\n",
        "            df_scores, df_clusters\n",
        "        )\n",
        "\n",
        "        self.rutas_visualizaciones['dendrograma'] = self.generador_viz.generar_dendrograma(\n",
        "            linkage_matrix, df_features_foto['codigo_foto'].tolist()\n",
        "        )\n",
        "\n",
        "        self.rutas_visualizaciones['heatmap'] = self.generador_viz.generar_heatmap_correlaciones(\n",
        "            matriz_corr\n",
        "        )\n",
        "\n",
        "        self.rutas_visualizaciones['caracterizacion'] = self.generador_viz.generar_caracterizacion_clusters(\n",
        "            df_features_foto, self.analizador_clustering.labels\n",
        "        )\n",
        "\n",
        "        # Paso 8: Guardar resultados\n",
        "        self.logger.info(\"\\n[PASO 8/8] Guardando resultados...\")\n",
        "        self._guardar_resultados(df_scores, df_clusters, matriz_corr, df_features_foto)\n",
        "\n",
        "        # Generar reporte\n",
        "        self.generador_reporte.generar_reporte(\n",
        "            self.resultados_pca,\n",
        "            self.resultados_clustering,\n",
        "            self.df_redundantes,\n",
        "            self.rutas_visualizaciones\n",
        "        )\n",
        "\n",
        "        duracion = (datetime.now() - inicio).total_seconds()\n",
        "\n",
        "        self.logger.info(\"\\n\" + \"=\" * 70)\n",
        "        self.logger.info(\"FASE 2C COMPLETADA\")\n",
        "        self.logger.info(\"=\" * 70)\n",
        "        self.logger.info(f\"Duracion: {duracion:.1f} segundos\")\n",
        "        self.logger.info(f\"Metricas analizadas: {len(self.metricas_disponibles)}\")\n",
        "        self.logger.info(f\"Componentes PCA: {self.resultados_pca['n_componentes']}\")\n",
        "        self.logger.info(f\"Clusters identificados: {self.config.n_clusters}\")\n",
        "        self.logger.info(f\"Pares redundantes: {len(self.df_redundantes)}\")\n",
        "        self.logger.info(f\"Resultados en: {self.config.ruta_salida}\")\n",
        "\n",
        "    def _guardar_resultados(self, df_scores: pd.DataFrame,\n",
        "                            df_clusters: pd.DataFrame,\n",
        "                            matriz_corr: pd.DataFrame,\n",
        "                            df_features_foto: pd.DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Guarda todos los resultados en archivos.\n",
        "\n",
        "        Args:\n",
        "            df_scores: DataFrame con scores PCA\n",
        "            df_clusters: DataFrame con clusters\n",
        "            matriz_corr: Matriz de correlacion\n",
        "            df_features_foto: Features de fotografias\n",
        "        \"\"\"\n",
        "        # Crear directorio de salida\n",
        "        self.config.ruta_salida.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Scores PCA\n",
        "        ruta = self.config.ruta_salida / 'pca_componentes.csv'\n",
        "        df_scores.to_csv(ruta, index=False)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "        # Varianza explicada\n",
        "        ruta = self.config.ruta_salida / 'pca_varianza_explicada.csv'\n",
        "        self.analizador_pca.obtener_dataframe_varianza().to_csv(ruta, index=False)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "        # Loadings\n",
        "        ruta = self.config.ruta_salida / 'pca_loadings.csv'\n",
        "        self.analizador_pca.obtener_dataframe_loadings().to_csv(ruta)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "        # Clusters\n",
        "        ruta = self.config.ruta_salida / 'clusters_fotografias.csv'\n",
        "        df_clusters.to_csv(ruta, index=False)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "        # Features de fotografias (con cluster)\n",
        "        df_features_foto['cluster'] = self.analizador_clustering.labels\n",
        "        df_features_foto['nombre_cluster'] = df_features_foto['cluster'].map(NOMBRES_CLUSTERS)\n",
        "        ruta = self.config.ruta_salida / 'features_fotografias.csv'\n",
        "        df_features_foto.to_csv(ruta, index=False)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "        # Matriz de correlaciones\n",
        "        ruta = self.config.ruta_salida / 'correlaciones_metricas.csv'\n",
        "        matriz_corr.to_csv(ruta)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "        # Metricas redundantes\n",
        "        ruta = self.config.ruta_salida / 'metricas_redundantes.csv'\n",
        "        self.df_redundantes.to_csv(ruta, index=False)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "        # Resumen JSON\n",
        "        resumen = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'metricas_analizadas': self.metricas_disponibles,\n",
        "            'pca': {\n",
        "                'n_componentes': self.resultados_pca['n_componentes'],\n",
        "                'varianza_total': self.resultados_pca['varianza_acumulada'][-1]\n",
        "            },\n",
        "            'clustering': {\n",
        "                'n_clusters': self.config.n_clusters,\n",
        "                'silhouette': self.resultados_clustering['silhouette_score'],\n",
        "                'caracterizacion': self.resultados_clustering['caracterizacion']\n",
        "            },\n",
        "            'redundancia': {\n",
        "                'umbral': self.config.umbral_redundancia,\n",
        "                'pares_encontrados': len(self.df_redundantes)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        ruta = self.config.ruta_salida / 'resumen_fase2c.json'\n",
        "        with open(ruta, 'w', encoding='utf-8') as f:\n",
        "            json.dump(resumen, f, indent=2, ensure_ascii=False, default=str)\n",
        "        self.logger.info(f\"  Guardado: {ruta}\")\n",
        "\n",
        "    def imprimir_resumen(self) -> None:\n",
        "        \"\"\"Imprime resumen de resultados.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"RESUMEN DE RESULTADOS FASE 2C\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # PCA\n",
        "        print(\"\\nANALISIS DE COMPONENTES PRINCIPALES:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"  Metricas analizadas: {len(self.metricas_disponibles)}\")\n",
        "        print(f\"  Componentes retenidos: {self.resultados_pca['n_componentes']}\")\n",
        "        print(f\"  Varianza explicada: {self.resultados_pca['varianza_acumulada'][-1]*100:.1f}%\")\n",
        "\n",
        "        # Clustering\n",
        "        print(\"\\nCLUSTERING DE FOTOGRAFIAS:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"  Silhouette Score: {self.resultados_clustering['silhouette_score']:.3f}\")\n",
        "\n",
        "        for cluster_id, info in self.resultados_clustering['caracterizacion'].items():\n",
        "            print(f\"  {info['nombre']}: {info['n_fotos']} fotos (IoU medio: {info['iou_mean']:.4f})\")\n",
        "\n",
        "        # Redundancia\n",
        "        print(\"\\nANALISIS DE REDUNDANCIA:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"  Umbral: |r| > {self.config.umbral_redundancia}\")\n",
        "        print(f\"  Pares redundantes: {len(self.df_redundantes)}\")\n",
        "\n",
        "        if len(self.df_redundantes) > 0:\n",
        "            print(\"  Top 5 pares mas correlacionados:\")\n",
        "            for _, row in self.df_redundantes.head(5).iterrows():\n",
        "                print(f\"    {row['metrica_1']} <-> {row['metrica_2']}: r={row['correlacion']:.3f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# FUNCION PRINCIPAL\n",
        "# =============================================================================\n",
        "\n",
        "def ejecutar_fase2c(ruta_base_tfm: str,\n",
        "                    ruta_datos_fase2b: str = None,\n",
        "                    n_clusters: int = 3) -> OrquestadorFase2C:\n",
        "    \"\"\"\n",
        "    Ejecuta el pipeline completo de Fase 2C.\n",
        "\n",
        "    Args:\n",
        "        ruta_base_tfm: Ruta base del proyecto TFM\n",
        "        ruta_datos_fase2b: Ruta al CSV fusionado (opcional)\n",
        "        n_clusters: Numero de clusters para K-means\n",
        "\n",
        "    Returns:\n",
        "        OrquestadorFase2C con resultados\n",
        "    \"\"\"\n",
        "    config = ConfiguracionFase2C(\n",
        "        ruta_base_tfm=Path(ruta_base_tfm),\n",
        "        n_clusters=n_clusters\n",
        "    )\n",
        "\n",
        "    if ruta_datos_fase2b:\n",
        "        config.ruta_datos_fase2b = Path(ruta_datos_fase2b)\n",
        "\n",
        "    orquestador = OrquestadorFase2C(config)\n",
        "    orquestador.ejecutar()\n",
        "    orquestador.imprimir_resumen()\n",
        "\n",
        "    return orquestador\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PUNTO DE ENTRADA\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ==========================================================================\n",
        "    # CONFIGURACION PARA GOOGLE COLAB\n",
        "    # ==========================================================================\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Rutas\n",
        "    RUTA_BASE_TFM = Path(\"/content/drive/MyDrive/TFM\")\n",
        "    RUTA_FASE2B = RUTA_BASE_TFM / \"3_Analisis\" / \"fase2b_correlaciones\" / \"metricas_fusionadas.csv\"\n",
        "\n",
        "    # Verificar archivo de entrada\n",
        "    if not RUTA_FASE2B.exists():\n",
        "        print(f\"ERROR: No se encuentra {RUTA_FASE2B}\")\n",
        "        print(\"Ejecute Fase 2B primero.\")\n",
        "    else:\n",
        "        print(f\"Archivo encontrado: {RUTA_FASE2B}\")\n",
        "\n",
        "        # Ejecutar Fase 2C\n",
        "        orquestador = ejecutar_fase2c(\n",
        "            ruta_base_tfm=str(RUTA_BASE_TFM),\n",
        "            ruta_datos_fase2b=str(RUTA_FASE2B),\n",
        "            n_clusters=3\n",
        "        )"
      ]
    }
  ]
}