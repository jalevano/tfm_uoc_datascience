{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd+0Ub5dkkNgBas1XrGfaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/01_Mask2Former_Implementacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_9cjv7sAb2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5f2bb245-a325-4819-ae00-5af5478f9eb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nEvaluación de Modelos Mask2Former para Segmentación de Personas\\n================================================================\\n\\nEste módulo implementa la evaluación sistemática del modelo Mask2Former para \\nsegmentación de instancias de personas. Parte del framework de evaluación \\ncomparativa para TFM sobre técnicas de segmentación.\\n\\nAutor: Jesús L.\\nFecha: 2025\\nProyecto: TFM - Evaluación Comparativa de Técnicas de Segmentación\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "Evaluación de Modelos Mask2Former para Segmentación de Personas\n",
        "================================================================\n",
        "\n",
        "Este módulo implementa la evaluación sistemática del modelo Mask2Former para\n",
        "segmentación de instancias de personas. Parte del framework de evaluación\n",
        "comparativa para TFM sobre técnicas de segmentación.\n",
        "\n",
        "Autor: Jesús L.\n",
        "Fecha: 2025\n",
        "Proyecto: TFM - Evaluación Comparativa de Técnicas de Segmentación\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import Mask2FormerImageProcessor, Mask2FormerForUniversalSegmentation\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import directed_hausdorff\n",
        "from skimage import measure, morphology"
      ],
      "metadata": {
        "id": "7gf3kDfrarKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class EvaluationConfig:\n",
        "   \"\"\"\n",
        "   Configuración para la evaluación de modelos de segmentación.\n",
        "\n",
        "   Attributes:\n",
        "       +model_name: Identificador del modelo en Hugging Face\n",
        "       +device: Dispositivo (cuda/cpu)\n",
        "       +confidence_threshold: Umbral mínimo de confianza para detecciones\n",
        "       +output_dir: Directorio para almacenar resultados\n",
        "       +save_masks: Para guardar las máscaras de segmentación\n",
        "       +enable_panoptic: Para habilitar evaluación panóptica adicional\n",
        "       +panoptic_overlap_threshold: Umbral para solapamiento en panóptica\n",
        "   \"\"\"\n",
        "   model_name: str = \"facebook/mask2former-swin-base-coco-instance\"\n",
        "   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "   confidence_threshold: float = 0.5\n",
        "   output_dir: str = \"./results\"\n",
        "   save_masks: bool = True\n",
        "   enable_panoptic: bool = True\n",
        "   panoptic_overlap_threshold: float = 0.5"
      ],
      "metadata": {
        "id": "mKVmEPAScHhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PhotoContextClassifier:\n",
        "   \"\"\"\n",
        "   Clasificador de contextos fotográficos para análisis.\n",
        "\n",
        "   Implementa la categorización automática de imágenes según diferentes\n",
        "   contextos fotográficos relevantes para evaluación de segmentación.\n",
        "   \"\"\"\n",
        "\n",
        "   CONTEXT_CATEGORIES = {\n",
        "       'retrato_simple': 'Persona individual con fondo uniforme',\n",
        "       'retrato_complejo': 'Persona individual con fondo detallado',\n",
        "       'multiples_personas': 'Dos o más personas en la imagen',\n",
        "       'exterior_natural': 'Entorno natural (parques, paisajes)',\n",
        "       'exterior_urbano': 'Entorno urbano (calles, edificios)',\n",
        "       'iluminacion_dificil': 'Contraluz, sombras, poca luz',\n",
        "       'poses_complejas': 'Posturas no estándar, oclusiones'\n",
        "   }\n",
        "\n",
        "   # Mapeo de clases COCO relevantes para contexto fotográfico\n",
        "   COCO_CONTEXT_CLASSES = {\n",
        "       'natural': [16, 17, 18, 19],  # bird, cat, dog, horse\n",
        "       'urban': [2, 3, 5, 6, 7, 8],  # bicycle, car, bus, train, truck, boat\n",
        "       'indoor': [56, 57, 58, 59, 60, 61, 62]  # chair, couch, bed, table, etc.\n",
        "   }\n",
        "\n",
        "   @classmethod\n",
        "   def get_valid_categories(cls) -> List[str]:\n",
        "       \"\"\"Retorna lista de categorías válidas.\"\"\"\n",
        "       return list(cls.CONTEXT_CATEGORIES.keys())\n",
        "\n",
        "   @classmethod\n",
        "   def validate_category(cls, category: str) -> str:\n",
        "       \"\"\"Valida y corrige categoría si es necesario.\"\"\"\n",
        "       if category not in cls.CONTEXT_CATEGORIES:\n",
        "           print(f\"Warning: Categoría '{category}' inválida, usando 'retrato_simple'\")\n",
        "           return 'retrato_simple'\n",
        "       return category\n",
        "\n",
        "   @classmethod\n",
        "   def get_category_description(cls, category: str) -> str:\n",
        "       \"\"\"Obtiene la descripción de una categoría.\"\"\"\n",
        "       return cls.CONTEXT_CATEGORIES.get(category, \"Categoría no válida\")\n",
        "\n",
        "   @staticmethod\n",
        "   def classify_context(image: np.ndarray, num_persons: int,\n",
        "                       panoptic_info: Optional[Dict] = None) -> Tuple[str, float]:\n",
        "       \"\"\"\n",
        "       Clasifica el contexto fotográfico de una imagen usando información visual\n",
        "       y panóptica.\n",
        "\n",
        "       Args:\n",
        "           image: Imagen en formato numpy array (H, W, C)\n",
        "           num_persons: Número de personas detectadas\n",
        "           panoptic_info: Información panóptica opcional para contexto mejorado\n",
        "\n",
        "       Returns:\n",
        "           Tuple con (categoría, puntuación de complejidad)\n",
        "       \"\"\"\n",
        "       h, w = image.shape[:2]\n",
        "\n",
        "       # Análisis básico de complejidad visual\n",
        "       gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "       edges = cv2.Canny(gray, 50, 150)\n",
        "       edge_density = np.sum(edges > 0) / (h * w)\n",
        "\n",
        "       # Análisis de varianza de color\n",
        "       color_variance = np.var(image.reshape(-1, 3), axis=0).mean()\n",
        "\n",
        "       # Análisis de contexto usando información panóptica\n",
        "       context_complexity = 0.0\n",
        "       context_category = 'retrato_simple'  # Categoría por defecto\n",
        "\n",
        "       if panoptic_info:\n",
        "           # Calcular diversidad de clases detectadas\n",
        "           segments = panoptic_info.get('segments_info', [])\n",
        "           unique_classes = len(set(seg.get('category_id', 0) for seg in segments))\n",
        "           context_complexity += min(unique_classes / 10.0, 0.3)\n",
        "\n",
        "           # Detectar contextos específicos usando las clases COCO\n",
        "           class_ids = [seg.get('category_id', 0) for seg in segments]\n",
        "\n",
        "           if any(cls in PhotoContextClassifier.COCO_CONTEXT_CLASSES['urban'] for cls in class_ids):\n",
        "               context_category = 'exterior_urbano'\n",
        "               context_complexity += 0.2\n",
        "           elif any(cls in PhotoContextClassifier.COCO_CONTEXT_CLASSES['natural'] for cls in class_ids):\n",
        "               context_category = 'exterior_natural'\n",
        "               context_complexity += 0.15\n",
        "           elif any(cls in PhotoContextClassifier.COCO_CONTEXT_CLASSES['indoor'] for cls in class_ids):\n",
        "               # Si hay muchos elementos indoor, mantener como complejo pero no cambiar categoría\n",
        "               context_complexity += 0.1\n",
        "\n",
        "       # Complejidad base de la imagen\n",
        "       base_complexity = (edge_density * 0.6 + min(color_variance / 1000, 1.0) * 0.4)\n",
        "       total_complexity = min(base_complexity + context_complexity, 1.0)\n",
        "\n",
        "       # Clasificación jerárquica usando SOLO las claves del diccionario CONTEXT_CATEGORIES\n",
        "       if num_persons > 1:\n",
        "           context_category = 'multiples_personas'\n",
        "           total_complexity = min(total_complexity + 0.2, 1.0)\n",
        "       elif total_complexity > 0.8:\n",
        "           # Muy complejo - posiblemente poses difíciles o iluminación complicada\n",
        "           if edge_density > 0.15:  # Muchos bordes = poses complejas\n",
        "               context_category = 'poses_complejas'\n",
        "           else:  # Poca definición = problemas de iluminación\n",
        "               context_category = 'iluminacion_dificil'\n",
        "       elif base_complexity > 0.5:\n",
        "           # Contexto no modificado por panóptica = retrato complejo\n",
        "           if context_category == 'retrato_simple':\n",
        "               context_category = 'retrato_complejo'\n",
        "       # Si base_complexity <= 0.5 y no hay múltiples personas, mantener 'retrato_simple'\n",
        "\n",
        "       # Validar que la categoría existe en CONTEXT_CATEGORIES\n",
        "       context_category = PhotoContextClassifier.validate_category(context_category)\n",
        "\n",
        "       return context_category, total_complexity"
      ],
      "metadata": {
        "id": "_u7tORaVuafr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedSegmentationMetrics:\n",
        "   \"\"\"\n",
        "   Calculadora de métricas avanzadas para evaluación de segmentación.\n",
        "\n",
        "   Implementa métricas estándar, especializadas para fotografía y panópticas\n",
        "   para evaluación cuantitativa completa de la calidad de segmentación.\n",
        "   \"\"\"\n",
        "\n",
        "   @staticmethod\n",
        "   def intersection_over_union(pred_mask: np.ndarray, gt_mask: np.ndarray) -> float:\n",
        "       \"\"\"Calcula Intersection over Union (IoU) entre máscaras.\"\"\"\n",
        "\n",
        "       if pred_mask.shape != gt_mask.shape:\n",
        "           raise ValueError(f\"Las máscaras deben tener la misma forma: {pred_mask.shape} vs {gt_mask.shape}\")\n",
        "\n",
        "       intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "       union = np.logical_or(pred_mask, gt_mask).sum()\n",
        "\n",
        "       if union == 0:\n",
        "           return 1.0 if intersection == 0 else 0.0\n",
        "\n",
        "       return float(intersection / union)\n",
        "\n",
        "   @staticmethod\n",
        "   def dice_coefficient(pred_mask: np.ndarray, gt_mask: np.ndarray) -> float:\n",
        "       \"\"\"Calcula coeficiente de Dice entre máscaras.\"\"\"\n",
        "\n",
        "       if pred_mask.shape != gt_mask.shape:\n",
        "           raise ValueError(f\"Las máscaras deben tener la misma forma: {pred_mask.shape} vs {gt_mask.shape}\")\n",
        "\n",
        "       intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "       total = pred_mask.sum() + gt_mask.sum()\n",
        "\n",
        "       if total == 0:\n",
        "           return 1.0 if intersection == 0 else 0.0\n",
        "\n",
        "       return float(2 * intersection / total)\n",
        "\n",
        "   @staticmethod\n",
        "   def boundary_iou(pred_mask: np.ndarray, gt_mask: np.ndarray,\n",
        "                   dilation_ratio: float = 0.02) -> float:\n",
        "       \"\"\"\n",
        "       Calcula IoU de los contornos de las máscaras.\n",
        "\n",
        "       Métrica importante para fotografía ya que evalúa la precisión de los bordes,\n",
        "       crucial para aplicaciones como cambio de fondo o efectos de retrato.\n",
        "       \"\"\"\n",
        "\n",
        "       def get_boundary(mask: np.ndarray, dilation_ratio: float) -> np.ndarray:\n",
        "           h, w = mask.shape\n",
        "           dilation_pixels = max(1, int(dilation_ratio * np.sqrt(h * w)))\n",
        "\n",
        "           kernel = np.ones((dilation_pixels, dilation_pixels), np.uint8)\n",
        "           dilated = cv2.dilate(mask.astype(np.uint8), kernel, iterations=1)\n",
        "           boundary = dilated - mask.astype(np.uint8)\n",
        "\n",
        "           return boundary.astype(bool)\n",
        "\n",
        "       try:\n",
        "           pred_boundary = get_boundary(pred_mask, dilation_ratio)\n",
        "           gt_boundary = get_boundary(gt_mask, dilation_ratio)\n",
        "\n",
        "           return AdvancedSegmentationMetrics.intersection_over_union(pred_boundary, gt_boundary)\n",
        "       except Exception as e:\n",
        "           print(f\"Error calculando Boundary IoU: {e}\")\n",
        "           return 0.0\n",
        "\n",
        "   @staticmethod\n",
        "   def hausdorff_distance(pred_mask: np.ndarray, gt_mask: np.ndarray) -> float:\n",
        "       \"\"\"\n",
        "       Calcula distancia de Hausdorff entre contornos de máscaras.\n",
        "\n",
        "       Métrica robusta para evaluar la similitud de formas, especialmente útil\n",
        "       para detectar errores en poses complejas o contornos irregulares.\n",
        "       \"\"\"\n",
        "\n",
        "       def get_contour_points(mask: np.ndarray) -> np.ndarray:\n",
        "           contours, _ = cv2.findContours(\n",
        "               mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "           )\n",
        "           if not contours:\n",
        "               return np.array([[0, 0]])\n",
        "\n",
        "           # Concatenar todos los puntos de contorno\n",
        "           points = np.vstack([contour.reshape(-1, 2) for contour in contours])\n",
        "           return points\n",
        "\n",
        "       try:\n",
        "           pred_points = get_contour_points(pred_mask)\n",
        "           gt_points = get_contour_points(gt_mask)\n",
        "\n",
        "           # Manejar casos edge donde no hay contornos\n",
        "           if len(pred_points) <= 1 or len(gt_points) <= 1:\n",
        "               return float('inf')\n",
        "\n",
        "           dist_1 = directed_hausdorff(pred_points, gt_points)[0]\n",
        "           dist_2 = directed_hausdorff(gt_points, pred_points)[0]\n",
        "\n",
        "           return float(max(dist_1, dist_2))\n",
        "       except Exception as e:\n",
        "           print(f\"Error calculando Hausdorff distance: {e}\")\n",
        "           return float('inf')\n",
        "\n",
        "   @staticmethod\n",
        "   def coherence_metrics(mask: np.ndarray) -> Dict[str, float]:\n",
        "       \"\"\"\n",
        "       Calcula métricas de coherencia espacial de la máscara.\n",
        "\n",
        "       Evalúa la calidad estructural de la segmentación, importante para\n",
        "       determinar si la máscara es utilizable en aplicaciones fotográficas.\n",
        "       \"\"\"\n",
        "\n",
        "       if np.sum(mask) == 0:\n",
        "           return {\n",
        "               'connected_components': 0.0,\n",
        "               'hole_ratio': 0.0,\n",
        "               'compactness': 0.0,\n",
        "               'solidity': 0.0,\n",
        "               'extent': 0.0\n",
        "           }\n",
        "\n",
        "       try:\n",
        "           # Componentes conectados\n",
        "           labeled_mask = measure.label(mask)\n",
        "           num_components = int(labeled_mask.max())\n",
        "\n",
        "           # Propiedades geométricas usando contornos\n",
        "           contours, _ = cv2.findContours(\n",
        "               mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "           )\n",
        "\n",
        "           hole_ratio = 0.0\n",
        "           solidity = 0.0\n",
        "           compactness = 0.0\n",
        "           extent = 0.0\n",
        "\n",
        "           if contours:\n",
        "               # Calcular convex hull y propiedades\n",
        "               largest_contour = max(contours, key=cv2.contourArea)\n",
        "               hull = cv2.convexHull(largest_contour)\n",
        "\n",
        "               # Crear máscaras para cálculos\n",
        "               hull_mask = np.zeros_like(mask)\n",
        "               cv2.fillPoly(hull_mask, [hull], 1)\n",
        "\n",
        "               convex_area = np.sum(hull_mask)\n",
        "               actual_area = np.sum(mask)\n",
        "\n",
        "               if convex_area > 0:\n",
        "                   hole_ratio = max(0.0, float((convex_area - actual_area) / convex_area))\n",
        "                   solidity = float(actual_area / convex_area)\n",
        "\n",
        "               # Compacidad (4π * área / perímetro²)\n",
        "               perimeter = cv2.arcLength(largest_contour, True)\n",
        "               if perimeter > 0:\n",
        "                   compactness = float((4 * np.pi * actual_area) / (perimeter ** 2))\n",
        "\n",
        "               # Extent (área / área del bounding box)\n",
        "               x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "               bbox_area = w * h\n",
        "               if bbox_area > 0:\n",
        "                   extent = float(actual_area / bbox_area)\n",
        "\n",
        "           return {\n",
        "               'connected_components': float(num_components),\n",
        "               'hole_ratio': hole_ratio,\n",
        "               'compactness': compactness,\n",
        "               'solidity': solidity,\n",
        "               'extent': extent\n",
        "           }\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error calculando métricas de coherencia: {e}\")\n",
        "           return {\n",
        "               'connected_components': 0.0,\n",
        "               'hole_ratio': 0.0,\n",
        "               'compactness': 0.0,\n",
        "               'solidity': 0.0,\n",
        "               'extent': 0.0\n",
        "           }\n",
        "\n",
        "   @staticmethod\n",
        "   def panoptic_quality_metrics(pred_panoptic: Dict, gt_panoptic: Dict,\n",
        "                              num_classes: int = 133) -> Dict[str, float]:\n",
        "       \"\"\"\n",
        "       Calcula métricas de Panoptic Quality (PQ) para evaluación panóptica.\n",
        "\n",
        "       PQ = SQ * RQ donde:\n",
        "       - SQ (Segmentation Quality): Calidad promedio de IoU de segmentos matched\n",
        "       - RQ (Recognition Quality): Fracción de ground truth segments que fueron\n",
        "       matched\n",
        "\n",
        "       Fundamental para evaluar la capacidad del modelo de entender escenas\n",
        "       completas.\n",
        "       \"\"\"\n",
        "\n",
        "       try:\n",
        "           pred_segments = {seg['id']: seg for seg in pred_panoptic.get('segments_info', [])}\n",
        "           gt_segments = {seg['id']: seg for seg in gt_panoptic.get('segments_info', [])}\n",
        "\n",
        "           pred_mask = pred_panoptic.get('segmentation')\n",
        "           gt_mask = gt_panoptic.get('segmentation')\n",
        "\n",
        "           # Manejar diferentes tipos de máscaras\n",
        "           if hasattr(pred_mask, 'cpu'):\n",
        "               pred_mask = pred_mask.cpu().numpy()\n",
        "           if hasattr(gt_mask, 'cpu'):\n",
        "               gt_mask = gt_mask.cpu().numpy()\n",
        "\n",
        "           if pred_mask is None or gt_mask is None or pred_mask.size == 0 or gt_mask.size == 0:\n",
        "               return {'PQ': 0.0, 'SQ': 0.0, 'RQ': 0.0, 'matched_segments': 0, 'total_predicted': 0, 'total_ground_truth': 0}\n",
        "\n",
        "           # Asegurar que las máscaras sean numpy arrays\n",
        "           pred_mask = np.array(pred_mask)\n",
        "           gt_mask = np.array(gt_mask)\n",
        "\n",
        "           # Calcular intersecciones para cada par de segmentos\n",
        "           intersections = {}\n",
        "           pred_areas = {}\n",
        "           gt_areas = {}\n",
        "\n",
        "           for pred_id, pred_seg in pred_segments.items():\n",
        "               pred_areas[pred_id] = (pred_mask == pred_id).sum()\n",
        "\n",
        "               for gt_id, gt_seg in gt_segments.items():\n",
        "                   if gt_seg['category_id'] == pred_seg['category_id']:\n",
        "                       intersection = ((pred_mask == pred_id) & (gt_mask == gt_id)).sum()\n",
        "                       if intersection > 0:\n",
        "                           intersections[(pred_id, gt_id)] = intersection\n",
        "                           if gt_id not in gt_areas:\n",
        "                               gt_areas[gt_id] = (gt_mask == gt_id).sum()\n",
        "\n",
        "           # Calcular matches usando threshold de IoU > 0.5\n",
        "           matches = []\n",
        "           for (pred_id, gt_id), intersection in intersections.items():\n",
        "               union = pred_areas[pred_id] + gt_areas[gt_id] - intersection\n",
        "               iou = intersection / union if union > 0 else 0\n",
        "\n",
        "               if iou > 0.5:\n",
        "                   matches.append({\n",
        "                       'pred_id': pred_id,\n",
        "                       'gt_id': gt_id,\n",
        "                       'iou': iou,\n",
        "                       'intersection': intersection,\n",
        "                       'union': union\n",
        "                   })\n",
        "\n",
        "           # Calcular métricas\n",
        "           if len(matches) == 0:\n",
        "               return {\n",
        "                   'PQ': 0.0, 'SQ': 0.0, 'RQ': 0.0,\n",
        "                   'matched_segments': 0,\n",
        "                   'total_predicted': len(pred_segments),\n",
        "                   'total_ground_truth': len(gt_segments)\n",
        "               }\n",
        "\n",
        "           # SQ (Segmentation Quality): IoU promedio de matches\n",
        "           sq = float(np.mean([match['iou'] for match in matches]))\n",
        "\n",
        "           # RQ (Recognition Quality): matches / (total_gt + total_pred - matches)\n",
        "           total_pred = len(pred_segments)\n",
        "           total_gt = len(gt_segments)\n",
        "           rq = float(len(matches) / (total_pred + total_gt - len(matches))) if (total_pred + total_gt) > 0 else 0.0\n",
        "\n",
        "           # PQ = SQ * RQ\n",
        "           pq = sq * rq\n",
        "\n",
        "           return {\n",
        "               'PQ': pq,\n",
        "               'SQ': sq,\n",
        "               'RQ': rq,\n",
        "               'matched_segments': len(matches),\n",
        "               'total_predicted': total_pred,\n",
        "               'total_ground_truth': total_gt\n",
        "           }\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error calculando métricas panópticas: {e}\")\n",
        "           return {\n",
        "               'PQ': 0.0, 'SQ': 0.0, 'RQ': 0.0,\n",
        "               'matched_segments': 0,\n",
        "               'total_predicted': 0,\n",
        "               'total_ground_truth': 0\n",
        "           }\n"
      ],
      "metadata": {
        "id": "joBMuOi5xXIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mask2FormerEvaluator:\n",
        "   \"\"\"\n",
        "   Evaluador principal para el modelo Mask2Former con capacidades panópticas.\n",
        "\n",
        "   Implementa el pipeline completo de evaluación: carga del modelo,\n",
        "   procesamiento de imágenes, cálculo de métricas y almacenamiento de resultados.\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "   def __init__(self, config: EvaluationConfig):\n",
        "       \"\"\"\n",
        "       Inicializa el evaluador con la configuración especificada.\n",
        "\n",
        "       Args:\n",
        "           config: Configuración de evaluación\n",
        "       \"\"\"\n",
        "       self.config = config\n",
        "       self.device = torch.device(config.device)\n",
        "\n",
        "       # Crear directorio de salida\n",
        "       Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "       # Inicializar modelo y procesador\n",
        "       self._load_model()\n",
        "\n",
        "       # Inicializar componentes auxiliares\n",
        "       self.context_classifier = PhotoContextClassifier()\n",
        "       self.metrics_calculator = AdvancedSegmentationMetrics()\n",
        "\n",
        "       # Almacenamiento de resultados\n",
        "       self.results = []\n",
        "\n",
        "       # Información del modelo cargado\n",
        "       self._log_model_info()\n",
        "\n",
        "   def _load_model(self) -> None:\n",
        "\n",
        "       \"\"\"Carga el modelo Mask2Former universal y el procesador de imágenes.\"\"\"\n",
        "\n",
        "       print(f\"Cargando modelo {self.config.model_name} en {self.device}\")\n",
        "\n",
        "       try:\n",
        "           self.processor = Mask2FormerImageProcessor.from_pretrained(\n",
        "               self.config.model_name\n",
        "           )\n",
        "           self.model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
        "               self.config.model_name\n",
        "           ).to(self.device)\n",
        "\n",
        "           self.model.eval()\n",
        "           print(\"Modelo cargado correctamente\")\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error cargando modelo: {e}\")\n",
        "           raise\n",
        "\n",
        "   def _log_model_info(self) -> None:\n",
        "\n",
        "       \"\"\"Registra información detallada del modelo cargado para trazabilidad.\"\"\"\n",
        "\n",
        "       total_params = sum(p.numel() for p in self.model.parameters())\n",
        "       trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "       print(f\"Parámetros totales: {total_params:,}\")\n",
        "       print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
        "       print(f\"Memoria del modelo: {total_params * 4 / (1024**3):.2f} GB (estimado)\")\n",
        "\n",
        "       # Detectar tipo de entrenamiento del modelo basado en el nombre\n",
        "       model_type = \"instance\" if \"instance\" in self.config.model_name else \\\n",
        "                   \"panoptic\" if \"panoptic\" in self.config.model_name else \\\n",
        "                   \"semantic\" if \"semantic\" in self.config.model_name else \"universal\"\n",
        "\n",
        "       print(f\"Tipo de entrenamiento detectado: {model_type}\")\n",
        "       print(f\"Evaluación panóptica: {'Habilitada' if self.config.enable_panoptic else 'Deshabilitada'}\")\n",
        "\n",
        "       # Almacenar para uso posterior\n",
        "       self.model_training_type = model_type\n",
        "\n",
        "   def _monitor_resources(self) -> Dict[str, float]:\n",
        "       \"\"\"\n",
        "       Monitorea el uso de recursos del sistema para análisis de eficiencia.\n",
        "\n",
        "       Returns:\n",
        "           Diccionario con métricas de uso de recursos\n",
        "       \"\"\"\n",
        "       try:\n",
        "           memory_info = psutil.virtual_memory()\n",
        "\n",
        "           resources = {\n",
        "               'cpu_percent': float(psutil.cpu_percent(interval=0.1)),\n",
        "               'memory_used_mb': float(memory_info.used / (1024 * 1024)),\n",
        "               'memory_percent': float(memory_info.percent),\n",
        "               'memory_available_mb': float(memory_info.available / (1024 * 1024))\n",
        "           }\n",
        "\n",
        "           # Añadir información de GPU si está disponible\n",
        "           if torch.cuda.is_available():\n",
        "               resources['gpu_memory_allocated_mb'] = float(torch.cuda.memory_allocated() / (1024 * 1024))\n",
        "               resources['gpu_memory_reserved_mb'] = float(torch.cuda.memory_reserved() / (1024 * 1024))\n",
        "               resources['gpu_memory_cached_mb'] = float(torch.cuda.memory_cached() / (1024 * 1024))\n",
        "\n",
        "           return resources\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error monitoreando recursos: {e}\")\n",
        "           return {'error': str(e)}\n",
        "\n",
        "   def _process_single_image(self, image_path: str,\n",
        "                            ground_truth_mask: Optional[np.ndarray] = None,\n",
        "                            ground_truth_panoptic: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "       \"\"\"\n",
        "       Procesa una imagen individual con análisis completo de instancias y panóptico.\n",
        "\n",
        "       Args:\n",
        "           image_path: Ruta a la imagen\n",
        "           ground_truth_mask: Máscara ground truth para instancias de personas\n",
        "           ground_truth_panoptic: Información panóptica ground truth\n",
        "\n",
        "       Returns:\n",
        "           Diccionario estructurado con todos los resultados del procesamiento\n",
        "       \"\"\"\n",
        "\n",
        "       try:\n",
        "           # Cargar y validar imagen\n",
        "           if not os.path.exists(image_path):\n",
        "               raise FileNotFoundError(f\"Imagen no encontrada: {image_path}\")\n",
        "\n",
        "           image = Image.open(image_path).convert(\"RGB\")\n",
        "           image_np = np.array(image)\n",
        "\n",
        "           # Validar dimensiones de imagen\n",
        "           if image_np.shape[0] < 32 or image_np.shape[1] < 32:\n",
        "               raise ValueError(f\"Imagen demasiado pequeña: {image_np.shape}\")\n",
        "\n",
        "           # Monitorear recursos antes del procesamiento\n",
        "           resources_before = self._monitor_resources()\n",
        "           start_time = time.time()\n",
        "\n",
        "           # Procesar imagen con el modelo\n",
        "           inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "           with torch.no_grad():\n",
        "               outputs = self.model(**inputs)\n",
        "\n",
        "           processing_time = (time.time() - start_time) * 1000  # en ms\n",
        "           resources_after = self._monitor_resources()\n",
        "\n",
        "           # Post-procesamiento para segmentación de instancias\n",
        "           instance_result = self.processor.post_process_instance_segmentation(\n",
        "               outputs, target_sizes=[image.size[::-1]]\n",
        "           )[0]\n",
        "\n",
        "           # Post-procesamiento panóptico (si está habilitado)\n",
        "           panoptic_result = None\n",
        "           if self.config.enable_panoptic:\n",
        "               try:\n",
        "                   panoptic_result = self.processor.post_process_panoptic_segmentation(\n",
        "                       outputs, target_sizes=[image.size[::-1]]\n",
        "                   )[0]\n",
        "               except Exception as e:\n",
        "                   print(f\"Warning: Segmentación panóptica falló para {image_path}: {e}\")\n",
        "                   panoptic_result = None\n",
        "\n",
        "           # Extraer y procesar máscaras de personas (clase 0 en COCO)\n",
        "           person_masks = []\n",
        "           confidences = []\n",
        "           person_scores = []\n",
        "\n",
        "           for i, (label, score) in enumerate(zip(\n",
        "               instance_result[\"labels\"],\n",
        "               instance_result[\"scores\"]\n",
        "           )):\n",
        "               if label == 0 and score > self.config.confidence_threshold:  # Persona\n",
        "                   mask = instance_result[\"masks\"][i].cpu().numpy().squeeze()\n",
        "                   # Asegurar que la máscara sea booleana\n",
        "                   mask = mask > 0.5\n",
        "                   person_masks.append(mask)\n",
        "                   confidences.append(float(score.item()))\n",
        "                   person_scores.append(float(score.item()))\n",
        "\n",
        "           # Combinar todas las máscaras de personas\n",
        "           if person_masks:\n",
        "               combined_mask = np.logical_or.reduce(person_masks)\n",
        "           else:\n",
        "               combined_mask = np.zeros(image_np.shape[:2], dtype=bool)\n",
        "\n",
        "           # Clasificar contexto fotográfico con información panóptica\n",
        "           context, complexity = self.context_classifier.classify_context(\n",
        "               image_np, len(person_masks), panoptic_result\n",
        "           )\n",
        "\n",
        "           # Estructura base de resultados\n",
        "           result = {\n",
        "               'experiment_metadata': {\n",
        "                   'model_name': 'mask2former',\n",
        "                   'model_version': self.config.model_name,\n",
        "                   'model_training_type': self.model_training_type,\n",
        "                   'timestamp': datetime.now().isoformat(),\n",
        "                   'device': str(self.device),\n",
        "                   'confidence_threshold': self.config.confidence_threshold,\n",
        "                   'panoptic_enabled': self.config.enable_panoptic,\n",
        "                   'framework_version': 'transformers'  # Para trazabilidad\n",
        "               },\n",
        "               'image_data': {\n",
        "                   'filename': os.path.basename(image_path),\n",
        "                   'full_path': os.path.abspath(image_path),\n",
        "                   'resolution': list(image.size),\n",
        "                   'context_category': context,\n",
        "                   'context_description': self.context_classifier.get_category_description(context),\n",
        "                   'complexity_score': float(complexity),\n",
        "                   'file_size_mb': float(os.path.getsize(image_path) / (1024 * 1024))\n",
        "               },\n",
        "               'segmentation_results': {\n",
        "                   'instance_masks_detected': len(person_masks),\n",
        "                   'mean_confidence': float(np.mean(confidences)) if confidences else 0.0,\n",
        "                   'max_confidence': float(np.max(confidences)) if confidences else 0.0,\n",
        "                   'min_confidence': float(np.min(confidences)) if confidences else 0.0,\n",
        "                   'confidence_std': float(np.std(confidences)) if len(confidences) > 1 else 0.0,\n",
        "                   'processing_time_ms': float(processing_time),\n",
        "                   'memory_delta_mb': float(resources_after.get('memory_used_mb', 0) - resources_before.get('memory_used_mb', 0)),\n",
        "                   'individual_scores': person_scores\n",
        "               },\n",
        "               'performance_metrics': {\n",
        "                   'resources_before': resources_before,\n",
        "                   'resources_after': resources_after,\n",
        "                   'gpu_memory_peak_mb': float(torch.cuda.max_memory_allocated() / (1024 * 1024)) if torch.cuda.is_available() else 0.0\n",
        "               },\n",
        "               'metrics': {\n",
        "                   'coherence_metrics': self.metrics_calculator.coherence_metrics(combined_mask)\n",
        "               }\n",
        "           }\n",
        "\n",
        "           # Añadir información panóptica detallada si está disponible\n",
        "           if panoptic_result:\n",
        "               segments_info = panoptic_result.get('segments_info', [])\n",
        "               person_segments = [seg for seg in segments_info if seg.get('category_id') == 0]\n",
        "\n",
        "               # Análisis de diversidad de clases detectadas\n",
        "               category_counts = {}\n",
        "               for seg in segments_info:\n",
        "                   cat_id = seg.get('category_id', -1)\n",
        "                   category_counts[cat_id] = category_counts.get(cat_id, 0) + 1\n",
        "\n",
        "               result['segmentation_results'].update({\n",
        "                   'panoptic_person_segments': len(person_segments),\n",
        "                   'total_panoptic_segments': len(segments_info),\n",
        "                   'panoptic_categories': len(set(seg.get('category_id', -1) for seg in segments_info)),\n",
        "                   'category_distribution': category_counts,\n",
        "                   'average_segment_area': float(np.mean([seg.get('area', 0) for seg in segments_info])) if segments_info else 0.0\n",
        "               })\n",
        "\n",
        "               # Métricas de calidad panóptica con ground truth\n",
        "               if ground_truth_panoptic:\n",
        "                   panoptic_metrics = self.metrics_calculator.panoptic_quality_metrics(\n",
        "                       panoptic_result, ground_truth_panoptic\n",
        "                   )\n",
        "                   result['metrics']['panoptic_metrics'] = panoptic_metrics\n",
        "\n",
        "           # Calcular métricas con ground truth de instancias\n",
        "           if ground_truth_mask is not None:\n",
        "               try:\n",
        "                   # Validar compatibilidad de máscaras\n",
        "                   if ground_truth_mask.shape != combined_mask.shape:\n",
        "                       print(f\"Warning: Redimensionando ground truth mask de {ground_truth_mask.shape} a {combined_mask.shape}\")\n",
        "                       ground_truth_mask = cv2.resize(\n",
        "                           ground_truth_mask.astype(np.uint8),\n",
        "                           (combined_mask.shape[1], combined_mask.shape[0])\n",
        "                       ) > 0.5\n",
        "\n",
        "                   overlap_metrics = {\n",
        "                       'iou': self.metrics_calculator.intersection_over_union(\n",
        "                           combined_mask, ground_truth_mask\n",
        "                       ),\n",
        "                       'dice_coefficient': self.metrics_calculator.dice_coefficient(\n",
        "                           combined_mask, ground_truth_mask\n",
        "                       )\n",
        "                   }\n",
        "\n",
        "                   boundary_metrics = {\n",
        "                       'boundary_iou': self.metrics_calculator.boundary_iou(\n",
        "                           combined_mask, ground_truth_mask\n",
        "                       ),\n",
        "                       'hausdorff_distance': self.metrics_calculator.hausdorff_distance(\n",
        "                           combined_mask, ground_truth_mask\n",
        "                       )\n",
        "                   }\n",
        "\n",
        "                   result['metrics']['overlap_metrics'] = overlap_metrics\n",
        "                   result['metrics']['boundary_metrics'] = boundary_metrics\n",
        "                   result['metrics']['has_ground_truth'] = True\n",
        "\n",
        "               except Exception as e:\n",
        "                   print(f\"Error calculando métricas con ground truth: {e}\")\n",
        "                   result['metrics']['ground_truth_error'] = str(e)\n",
        "           else:\n",
        "               result['metrics']['has_ground_truth'] = False\n",
        "\n",
        "           # Guardar máscaras si se requiere\n",
        "           if self.config.save_masks:\n",
        "               base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "               # Guardar máscara de instancias\n",
        "               instance_mask_filename = f\"instance_mask_{base_filename}.png\"\n",
        "               instance_mask_path = os.path.join(self.config.output_dir, instance_mask_filename)\n",
        "               cv2.imwrite(instance_mask_path, (combined_mask * 255).astype(np.uint8))\n",
        "               result['segmentation_results']['instance_mask_path'] = instance_mask_path\n",
        "\n",
        "               # Guardar segmentación panóptica si está disponible\n",
        "               if panoptic_result:\n",
        "                   try:\n",
        "                       panoptic_mask = panoptic_result['segmentation']\n",
        "                       if hasattr(panoptic_mask, 'cpu'):\n",
        "                           panoptic_mask = panoptic_mask.cpu().numpy()\n",
        "\n",
        "                       panoptic_mask_filename = f\"panoptic_mask_{base_filename}.png\"\n",
        "                       panoptic_mask_path = os.path.join(self.config.output_dir, panoptic_mask_filename)\n",
        "\n",
        "                       # Convertir a imagen RGB para visualización\n",
        "                       panoptic_rgb = self._panoptic_to_rgb(panoptic_mask, panoptic_result['segments_info'])\n",
        "                       cv2.imwrite(panoptic_mask_path, cv2.cvtColor(panoptic_rgb, cv2.COLOR_RGB2BGR))\n",
        "                       result['segmentation_results']['panoptic_mask_path'] = panoptic_mask_path\n",
        "\n",
        "                   except Exception as e:\n",
        "                       print(f\"Error guardando máscara panóptica: {e}\")\n",
        "\n",
        "           return result\n",
        "\n",
        "       except Exception as e:\n",
        "           # Crear resultado de error estructurado\n",
        "           error_result = {\n",
        "               'experiment_metadata': {\n",
        "                   'model_name': 'mask2former',\n",
        "                   'model_version': self.config.model_name,\n",
        "                   'error': str(e),\n",
        "                   'timestamp': datetime.now().isoformat()\n",
        "               },\n",
        "               'image_data': {\n",
        "                   'filename': os.path.basename(image_path),\n",
        "                   'full_path': os.path.abspath(image_path) if os.path.exists(image_path) else image_path,\n",
        "                   'processing_failed': True,\n",
        "                   'error_type': type(e).__name__\n",
        "               },\n",
        "               'segmentation_results': {\n",
        "                   'instance_masks_detected': 0,\n",
        "                   'processing_time_ms': 0.0,\n",
        "                   'error_message': str(e)\n",
        "               },\n",
        "               'metrics': {\n",
        "                   'has_ground_truth': ground_truth_mask is not None,\n",
        "                   'processing_error': True\n",
        "               }\n",
        "           }\n",
        "           return error_result\n",
        "\n",
        "   def _panoptic_to_rgb(self, panoptic_mask: np.ndarray, segments_info: List[Dict]) -> np.ndarray:\n",
        "       \"\"\"\n",
        "       Convierte máscara panóptica a imagen RGB para visualización.\n",
        "\n",
        "       Args:\n",
        "           panoptic_mask: Máscara panóptica con IDs de segmentos\n",
        "           segments_info: Información de los segmentos\n",
        "\n",
        "       Returns:\n",
        "           Imagen RGB de la segmentación panóptica coloreada por categorías\n",
        "       \"\"\"\n",
        "       h, w = panoptic_mask.shape\n",
        "       rgb_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "\n",
        "       # Mapeo de colores para diferentes categorías\n",
        "       # Colores especiales para categorías importantes\n",
        "       category_colors = {\n",
        "           0: [255, 100, 100],   # Persona - Rojo brillante\n",
        "           1: [100, 255, 100],   # Vehículo - Verde\n",
        "           2: [100, 100, 255],   # Objeto - Azul\n",
        "       }\n",
        "\n",
        "       # Generar colores consistentes para el resto de categorías\n",
        "       np.random.seed(42)  # Para colores reproducibles\n",
        "       colors = np.random.randint(50, 255, (200, 3))  # Evitar colores muy oscuros\n",
        "\n",
        "       for segment in segments_info:\n",
        "           try:\n",
        "               segment_id = segment['id']\n",
        "               category_id = segment.get('category_id', -1)\n",
        "               mask = panoptic_mask == segment_id\n",
        "\n",
        "               # Usar color especial si existe, sino color generado\n",
        "               if category_id in category_colors:\n",
        "                   rgb_mask[mask] = category_colors[category_id]\n",
        "               else:\n",
        "                   color_idx = (category_id + segment_id) % len(colors)\n",
        "                   rgb_mask[mask] = colors[color_idx]\n",
        "\n",
        "           except Exception as e:\n",
        "               print(f\"Error coloreando segmento {segment.get('id', 'unknown')}: {e}\")\n",
        "               continue\n",
        "\n",
        "       return rgb_mask\n",
        "\n",
        "   def evaluate_dataset(self, image_paths: List[str],\n",
        "                       ground_truth_masks: Optional[List[np.ndarray]] = None,\n",
        "                       ground_truth_panoptic: Optional[List[Dict]] = None) -> List[Dict[str, Any]]:\n",
        "       \"\"\"\n",
        "       Evalúa un conjunto de imágenes con análisis completo de instancias y panóptico.\n",
        "\n",
        "       Args:\n",
        "           image_paths: Lista de rutas a las imágenes\n",
        "           ground_truth_masks: Lista opcional de máscaras ground truth para instancias\n",
        "           ground_truth_panoptic: Lista opcional de datos panópticos ground truth\n",
        "\n",
        "       Returns:\n",
        "           Lista con resultados de evaluación para cada imagen\n",
        "       \"\"\"\n",
        "       print(f\"Iniciando evaluación de {len(image_paths)} imágenes\")\n",
        "       print(f\"Modelo: {self.config.model_name}\")\n",
        "       print(f\"Modo panóptico: {'Habilitado' if self.config.enable_panoptic else 'Deshabilitado'}\")\n",
        "       print(f\"Dispositivo: {self.device}\")\n",
        "       print(f\"Umbral de confianza: {self.config.confidence_threshold}\")\n",
        "\n",
        "       # Validaciones de entrada\n",
        "       if ground_truth_masks and len(ground_truth_masks) != len(image_paths):\n",
        "           raise ValueError(f\"Número de máscaras GT ({len(ground_truth_masks)}) != número de imágenes ({len(image_paths)})\")\n",
        "\n",
        "       if ground_truth_panoptic and len(ground_truth_panoptic) != len(image_paths):\n",
        "           raise ValueError(f\"Número de datos panópticos GT ({len(ground_truth_panoptic)}) != número de imágenes ({len(image_paths)})\")\n",
        "\n",
        "       # Verificar que las imágenes existen\n",
        "       missing_images = [path for path in image_paths if not os.path.exists(path)]\n",
        "       if missing_images:\n",
        "           print(f\"Warning: {len(missing_images)} imágenes no encontradas:\")\n",
        "           for img in missing_images[:5]:  # Mostrar solo las primeras 5\n",
        "               print(f\"  - {img}\")\n",
        "           if len(missing_images) > 5:\n",
        "               print(f\"  ... y {len(missing_images) - 5} más\")\n",
        "\n",
        "       results = []\n",
        "       successful_count = 0\n",
        "       failed_count = 0\n",
        "       total_processing_time = 0.0\n",
        "\n",
        "       # Limpiar memoria GPU antes de empezar\n",
        "       if torch.cuda.is_available():\n",
        "           torch.cuda.empty_cache()\n",
        "           torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "       for i, image_path in enumerate(image_paths):\n",
        "           print(f\"Procesando [{i+1:4d}/{len(image_paths)}]: {os.path.basename(image_path)}\")\n",
        "\n",
        "           gt_mask = ground_truth_masks[i] if ground_truth_masks else None\n",
        "           gt_panoptic = ground_truth_panoptic[i] if ground_truth_panoptic else None\n",
        "\n",
        "           try:\n",
        "               result = self._process_single_image(image_path, gt_mask, gt_panoptic)\n",
        "\n",
        "               # Verificar si el procesamiento fue exitoso\n",
        "               if result.get('image_data', {}).get('processing_failed', False):\n",
        "                   failed_count += 1\n",
        "                   print(f\"Error: {result.get('segmentation_results', {}).get('error_message', 'Unknown error')}\")\n",
        "               else:\n",
        "                   successful_count += 1\n",
        "                   processing_time = result.get('segmentation_results', {}).get('processing_time_ms', 0.0)\n",
        "                   total_processing_time += processing_time\n",
        "                   masks_detected = result.get('segmentation_results', {}).get('instance_masks_detected', 0)\n",
        "                   print(f\"{masks_detected} personas detectadas ({processing_time:.1f}ms)\")\n",
        "\n",
        "               results.append(result)\n",
        "\n",
        "           except Exception as e:\n",
        "               failed_count += 1\n",
        "               print(f\"Excepción no controlada: {e}\")\n",
        "\n",
        "               # Crear resultado de error mínimo\n",
        "               error_result = {\n",
        "                   'experiment_metadata': {'model_name': 'mask2former', 'critical_error': str(e)},\n",
        "                   'image_data': {'filename': os.path.basename(image_path), 'processing_failed': True},\n",
        "                   'segmentation_results': {'instance_masks_detected': 0, 'error_message': str(e)},\n",
        "                   'metrics': {'processing_error': True}\n",
        "               }\n",
        "               results.append(error_result)\n",
        "\n",
        "           # Limpiar caché GPU periódicamente y mostrar progreso\n",
        "           if torch.cuda.is_available() and (i + 1) % 10 == 0:\n",
        "               torch.cuda.empty_cache()\n",
        "               gpu_memory = torch.cuda.memory_allocated() / (1024**3)  # GB\n",
        "               print(f\"GPU memoria actual: {gpu_memory:.2f} GB\")\n",
        "\n",
        "           # Mostrar progreso cada 25% del dataset\n",
        "           progress_milestones = [len(image_paths) * p // 100 for p in [25, 50, 75]]\n",
        "           if (i + 1) in progress_milestones:\n",
        "               progress = ((i + 1) / len(image_paths)) * 100\n",
        "               avg_time = total_processing_time / max(successful_count, 1)\n",
        "               print(f\"Progreso: {progress:.0f}% - Tiempo promedio: {avg_time:.1f}ms - Exitosas: {successful_count}\")\n",
        "\n",
        "       # Resumen final\n",
        "       self.results.extend(results)\n",
        "       success_rate = (successful_count / len(image_paths)) * 100 if image_paths else 0\n",
        "       avg_processing_time = total_processing_time / max(successful_count, 1)\n",
        "\n",
        "       print(f\"\\n=== RESUMEN DE EVALUACIÓN ===\")\n",
        "       print(f\"Total imágenes: {len(image_paths)}\")\n",
        "       print(f\"Procesadas exitosamente: {successful_count} ({success_rate:.1f}%)\")\n",
        "       print(f\"Fallos: {failed_count}\")\n",
        "       print(f\"Tiempo promedio por imagen: {avg_processing_time:.1f}ms\")\n",
        "       print(f\"Tiempo total de procesamiento: {total_processing_time/1000:.1f}s\")\n",
        "\n",
        "       if torch.cuda.is_available():\n",
        "           peak_gpu_memory = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "           print(f\"Memoria GPU pico: {peak_gpu_memory:.2f} GB\")\n",
        "\n",
        "       return results\n",
        "\n",
        "   def save_results(self, filename: Optional[str] = None) -> str:\n",
        "       \"\"\"\n",
        "       Guarda los resultados en formato JSON estructurado para análisis posterior.\n",
        "\n",
        "       Args:\n",
        "           filename: Nombre del archivo (opcional)\n",
        "\n",
        "       Returns:\n",
        "           Ruta del archivo guardado\n",
        "       \"\"\"\n",
        "       if not filename:\n",
        "           timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "           model_type = self.model_training_type\n",
        "           filename = f\"mask2former_{model_type}_evaluation_{timestamp}.json\"\n",
        "\n",
        "       filepath = os.path.join(self.config.output_dir, filename)\n",
        "\n",
        "       # Preparar resumen básico de la evaluación\n",
        "       successful_results = [r for r in self.results if not r.get('image_data', {}).get('processing_failed', False)]\n",
        "       failed_results = [r for r in self.results if r.get('image_data', {}).get('processing_failed', False)]\n",
        "\n",
        "       # Preparar datos para serialización JSON\n",
        "       def json_serializer(obj):\n",
        "           \"\"\"Serializa objetos no estándar para JSON.\"\"\"\n",
        "           if isinstance(obj, np.ndarray):\n",
        "               return obj.tolist()\n",
        "           elif isinstance(obj, np.integer):\n",
        "               return int(obj)\n",
        "           elif isinstance(obj, np.floating):\n",
        "               return float(obj)\n",
        "           elif isinstance(obj, torch.Tensor):\n",
        "               return obj.cpu().numpy().tolist()\n",
        "           elif hasattr(obj, 'isoformat'):  # datetime objects\n",
        "               return obj.isoformat()\n",
        "           else:\n",
        "               return str(obj)\n",
        "\n",
        "       serializable_results = []\n",
        "       for result in self.results:\n",
        "           try:\n",
        "               serializable_result = json.loads(json.dumps(result, default=json_serializer))\n",
        "               serializable_results.append(serializable_result)\n",
        "           except Exception as e:\n",
        "               print(f\"Error serializando resultado: {e}\")\n",
        "               simplified = {\n",
        "                   'image_data': {'filename': result.get('image_data', {}).get('filename', 'unknown')},\n",
        "                   'serialization_error': str(e)\n",
        "               }\n",
        "               serializable_results.append(simplified)\n",
        "\n",
        "       # Estructura final del archivo JSON\n",
        "       output_data = {\n",
        "           'evaluation_summary': {\n",
        "               'total_images': len(self.results),\n",
        "               'successful_images': len(successful_results),\n",
        "               'failed_images': len(failed_results),\n",
        "               'success_rate_percent': (len(successful_results) / len(self.results)) * 100 if self.results else 0,\n",
        "               'model_config': {\n",
        "                   'model_name': self.config.model_name,\n",
        "                   'model_training_type': self.model_training_type,\n",
        "                   'confidence_threshold': self.config.confidence_threshold,\n",
        "                   'panoptic_enabled': self.config.enable_panoptic,\n",
        "                   'device': str(self.device)\n",
        "               },\n",
        "               'evaluation_date': datetime.now().isoformat(),\n",
        "               'available_categories': list(PhotoContextClassifier.CONTEXT_CATEGORIES.keys())\n",
        "           },\n",
        "           'results': serializable_results\n",
        "       }\n",
        "\n",
        "       # Guardar archivo\n",
        "       try:\n",
        "           with open(filepath, 'w', encoding='utf-8') as f:\n",
        "               json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "           print(f\"Resultados guardados en: {filepath}\")\n",
        "           print(f\"Tamaño del archivo: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
        "           return filepath\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error guardando resultados: {e}\")\n",
        "           raise"
      ],
      "metadata": {
        "id": "DJZoksUo2-Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de uso del evaluador completo\n",
        "if __name__ == \"__main__\":\n",
        "   # Configuración de la evaluación\n",
        "   config = EvaluationConfig(\n",
        "       model_name=\"facebook/mask2former-swin-base-coco-instance\",\n",
        "       device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "       confidence_threshold=0.5,\n",
        "       output_dir=\"./mask2former_results\",\n",
        "       save_masks=True,\n",
        "       enable_panoptic=True  # Habilitar análisis panóptico\n",
        "   )\n",
        "\n",
        "   print(\"=== EVALUADOR MASK2FORMER PARA TFM ===\")\n",
        "   print(f\"Modelo: {config.model_name}\")\n",
        "   print(f\"Dispositivo: {config.device}\")\n",
        "   print(f\"Análisis panóptico: {'✓' if config.enable_panoptic else '✗'}\")\n",
        "\n",
        "   # Inicializar evaluador\n",
        "   evaluator = Mask2FormerEvaluator(config)\n",
        "\n",
        "   # Lista de imágenes a evaluar (personalizar según tu dataset)\n",
        "   image_paths = [\n",
        "       \"path/to/image1.jpg\",\n",
        "       \"path/to/image2.jpg\",\n",
        "       \"path/to/image3.jpg\",\n",
        "       # ... agregar más imágenes\n",
        "   ]\n",
        "\n",
        "   # PASO 1: Crear template para ground truth (opcional)\n",
        "   print(\"\\n--- Paso 1: Generando template de anotación ---\")\n",
        "   gt_template_path = create_ground_truth_template(image_paths, config.output_dir)\n",
        "   print(f\"Completar anotaciones en: {gt_template_path}\")\n",
        "\n",
        "   # PASO 2: Evaluar dataset sin ground truth (recolección inicial de datos)\n",
        "   print(\"\\n--- Paso 2: Evaluando dataset (sin ground truth) ---\")\n",
        "   results = evaluator.evaluate_dataset(image_paths)\n",
        "\n",
        "   # PASO 3: Guardar resultados estructurados\n",
        "   print(\"\\n--- Paso 3: Guardando resultados ---\")\n",
        "   results_file = evaluator.save_results()\n",
        "\n",
        "   print(f\"\\n=== EVALUACIÓN COMPLETADA ===\")\n",
        "   print(f\"Resultados guardados en: {results_file}\")\n",
        "   print(f\"Imágenes procesadas: {len([r for r in results if not r.get('image_data', {}).get('processing_failed', False)])}\")\n",
        "   print(f\"Template GT disponible en: {gt_template_path}\")"
      ],
      "metadata": {
        "id": "nZpN53by9npl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}