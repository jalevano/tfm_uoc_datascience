{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUPgMGsin3SSJYXc7VnNh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/00_ExtraerCaracteristicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ovT0Ecm_rny6",
        "outputId": "19259462-fd6c-48a7-91ac-7ddfd7f6db0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nEXTRACTOR DE CARACTERÍSTICAS DE IMÁGENES\\n================================================================================\\n\\nEste notebook extrae características completas de imágenes fotográficas \\nutilizando múltiples librerías especializadas. El análisis es independiente \\ndel modelo de segmentación y genera un JSON con toda la información relevante.\\n\\nCARACTERÍSTICAS ANALIZADAS:\\n- Metadatos EXIF (cámara, exposición, GPS, etc.)\\n- Estadísticas de color (RGB, HSV, LAB)\\n- Histogramas y distribuciones\\n- Texturas (Haralick features)\\n- Características visuales (bordes, esquinas, gradientes)\\n- Análisis de nitidez y calidad\\n- Dominancia de colores\\n- Información técnica de archivo\\n\\nLIBRERÍAS UTILIZADAS:\\n- PIL/Pillow: Carga y metadatos básicos\\n- piexif: Metadatos EXIF completos\\n- OpenCV: Procesamiento de imagen\\n- scikit-image: Análisis avanzado\\n- mahotas: Texturas Haralick\\n- numpy/scipy: Cálculos matemáticos\\n- colorthief: Paleta de colores dominantes\\n- imagehash: Hashes perceptuales\\n\\nAutor: Jesús L.\\nProyecto: TFM - Evaluación Comparativa de Técnicas de Segmentación\\nUniversidad: Universidad Oberta de Cataluña\\nFecha: 2025\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "================================================================================\n",
        "EXTRACTOR DE CARACTERÍSTICAS DE IMÁGENES\n",
        "================================================================================\n",
        "\n",
        "Este notebook extrae características completas de imágenes fotográficas\n",
        "utilizando múltiples librerías especializadas. El análisis es independiente\n",
        "del modelo de segmentación y genera un JSON con toda la información relevante.\n",
        "\n",
        "CARACTERÍSTICAS ANALIZADAS:\n",
        "- Metadatos EXIF (cámara, exposición, GPS, etc.)\n",
        "- Estadísticas de color (RGB, HSV, LAB)\n",
        "- Histogramas y distribuciones\n",
        "- Texturas (Haralick features)\n",
        "- Características visuales (bordes, esquinas, gradientes)\n",
        "- Análisis de nitidez y calidad\n",
        "- Dominancia de colores\n",
        "- Información técnica de archivo\n",
        "\n",
        "LIBRERÍAS UTILIZADAS:\n",
        "- PIL/Pillow: Carga y metadatos básicos\n",
        "- piexif: Metadatos EXIF completos\n",
        "- OpenCV: Procesamiento de imagen\n",
        "- scikit-image: Análisis avanzado\n",
        "- mahotas: Texturas Haralick\n",
        "- numpy/scipy: Cálculos matemáticos\n",
        "- colorthief: Paleta de colores dominantes\n",
        "- imagehash: Hashes perceptuales\n",
        "\n",
        "Autor: Jesús L.\n",
        "Proyecto: TFM - Evaluación Comparativa de Técnicas de Segmentación\n",
        "Universidad: Universidad Oberta de Cataluña\n",
        "Fecha: 2025\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "from dataclasses import dataclass, asdict, field\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageStat\n",
        "import piexif\n",
        "import exifread\n",
        "import imagehash\n",
        "\n",
        "from skimage import (\n",
        "    color, feature, filters, measure,\n",
        "    morphology, exposure\n",
        ")\n",
        "from scipy import ndimage, stats\n",
        "from sklearn.cluster import KMeans\n",
        "import mahotas as mh\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "W9SJlgEg5gj8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONFIGURACIÓN DEL SISTEMA\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ConfiguracionExtraccion:\n",
        "    \"\"\"\n",
        "    Configuración centralizada del sistema de extracción.\n",
        "\n",
        "    Attributes:\n",
        "        ruta_base: Directorio base del proyecto\n",
        "        ruta_imagenes: Directorio con imágenes a procesar\n",
        "        ruta_resultados: Directorio para almacenar resultados\n",
        "        max_dimension_procesamiento: Dimensión máxima para redimensionar (optimización)\n",
        "        calcular_exif: Activar extracción de metadatos EXIF\n",
        "        calcular_histogramas: Activar cálculo de histogramas de color\n",
        "        calcular_texturas: Activar análisis de texturas (Haralick, LBP, GLCM)\n",
        "        calcular_bordes: Activar detección de bordes y esquinas\n",
        "        calcular_frecuencias: Activar análisis en dominio de frecuencias\n",
        "        calcular_ruido: Activar estimación de ruido (SNR, PSNR)\n",
        "        calcular_colores_dominantes: Activar extracción de paleta de colores\n",
        "        calcular_hashes: Activar generación de hashes perceptuales\n",
        "        num_colores_paleta: Número de colores a extraer en paleta\n",
        "        guardar_json_individual: Guardar JSON por cada imagen\n",
        "        guardar_json_consolidado: Guardar JSON con todas las imágenes\n",
        "        nivel_log: Nivel de logging (DEBUG, INFO, WARNING, ERROR)\n",
        "    \"\"\"\n",
        "    ruta_base: Path = Path(\"/content/drive/MyDrive/TFM/\")\n",
        "    ruta_imagenes: Path = None\n",
        "    ruta_resultados: Path = None\n",
        "    max_dimension_procesamiento: int = 2048\n",
        "\n",
        "    # Flags de procesamiento\n",
        "    calcular_exif: bool = True\n",
        "    calcular_histogramas: bool = True\n",
        "    calcular_texturas: bool = True\n",
        "    calcular_bordes: bool = True\n",
        "    calcular_frecuencias: bool = True\n",
        "    calcular_ruido: bool = True\n",
        "    calcular_colores_dominantes: bool = True\n",
        "    calcular_hashes: bool = True\n",
        "\n",
        "    # Parámetros específicos\n",
        "    num_colores_paleta: int = 5\n",
        "\n",
        "    # Opciones de salida\n",
        "    guardar_json_individual: bool = True\n",
        "    guardar_json_consolidado: bool = True\n",
        "    nivel_log: str = \"INFO\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Inicialización post-creación del dataclass\"\"\"\n",
        "        if self.ruta_imagenes is None:\n",
        "            self.ruta_imagenes = self.ruta_base / \"0_Imagenes\"\n",
        "        if self.ruta_resultados is None:\n",
        "            self.ruta_resultados = self.ruta_base / \"1_Caracteristicas\"\n",
        "\n",
        "    def crear_estructura_directorios(self) -> None:\n",
        "        \"\"\"Crea la estructura de directorios necesaria\"\"\"\n",
        "        self.ruta_resultados.mkdir(parents=True, exist_ok=True)\n",
        "        (self.ruta_resultados / \"json\").mkdir(exist_ok=True)\n",
        "        (self.ruta_resultados / \"logs\").mkdir(exist_ok=True)\n",
        "\n",
        "    def validar(self) -> bool:\n",
        "        \"\"\"\n",
        "        Valida la configuración.\n",
        "\n",
        "        Returns:\n",
        "            True si la configuración es válida, False en caso contrario\n",
        "        \"\"\"\n",
        "        if not self.ruta_imagenes.exists():\n",
        "            logging.error(f\"Directorio de imágenes no existe: {self.ruta_imagenes}\")\n",
        "            return False\n",
        "\n",
        "        if self.max_dimension_procesamiento < 256:\n",
        "            logging.error(\"max_dimension_procesamiento debe ser >= 256\")\n",
        "            return False\n",
        "\n",
        "        if self.num_colores_paleta < 1 or self.num_colores_paleta > 20:\n",
        "            logging.error(\"num_colores_paleta debe estar entre 1 y 20\")\n",
        "            return False\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "id": "la_1vVbv5lDK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SISTEMA DE LOGGING\n",
        "# =============================================================================\n",
        "\n",
        "class LoggerManager:\n",
        "    \"\"\"\n",
        "    Gestor centralizado del sistema de logging.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, directorio_logs: Path, nivel: str = \"INFO\"):\n",
        "        \"\"\"\n",
        "        Inicializa el sistema de logging.\n",
        "\n",
        "        Args:\n",
        "            directorio_logs: Directorio donde guardar logs\n",
        "            nivel: Nivel de logging (DEBUG, INFO, WARNING, ERROR)\n",
        "        \"\"\"\n",
        "        self.directorio_logs = directorio_logs\n",
        "        self.directorio_logs.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        archivo_log = self.directorio_logs / f\"extraccion_{timestamp}.log\"\n",
        "\n",
        "        logging.basicConfig(\n",
        "            level=getattr(logging, nivel.upper()),\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(archivo_log, encoding='utf-8'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.logger = logging.getLogger('ExtractorCaracteristicas')\n",
        "        self.logger.info(f\"Sistema de logging inicializado. Archivo: {archivo_log.name}\")\n",
        "\n",
        "    def get_logger(self) -> logging.Logger:\n",
        "        \"\"\"Retorna el logger configurado\"\"\"\n",
        "        return self.logger\n"
      ],
      "metadata": {
        "id": "ACfuknRu6Dw8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# UTILIDADES\n",
        "# =============================================================================\n",
        "\n",
        "class Utilidades:\n",
        "    \"\"\"\n",
        "    Funciones de utilidad para conversión y serialización de datos.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def convertir_a_serializable(obj: Any) -> Any:\n",
        "        \"\"\"\n",
        "        Convierte objetos a tipos serializables en JSON.\n",
        "\n",
        "        Maneja tipos numpy, tuplas, listas, bytes y objetos complejos.\n",
        "\n",
        "        Args:\n",
        "            obj: Objeto a convertir\n",
        "\n",
        "        Returns:\n",
        "            Objeto convertido a tipo serializable\n",
        "        \"\"\"\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (tuple, list)):\n",
        "            return [Utilidades.convertir_a_serializable(item) for item in obj]\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: Utilidades.convertir_a_serializable(value)\n",
        "                   for key, value in obj.items()}\n",
        "        elif isinstance(obj, bytes):\n",
        "            try:\n",
        "                return obj.decode('utf-8', errors='ignore')\n",
        "            except:\n",
        "                return str(obj)\n",
        "        elif isinstance(obj, (int, float, str, bool, type(None))):\n",
        "            return obj\n",
        "        else:\n",
        "            return str(obj)\n",
        "\n",
        "    @staticmethod\n",
        "    def guardar_json(datos: Dict, ruta: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Guarda diccionario como JSON con manejo de errores.\n",
        "\n",
        "        Args:\n",
        "            datos: Diccionario a guardar\n",
        "            ruta: Ruta del archivo JSON\n",
        "\n",
        "        Returns:\n",
        "            True si se guardó correctamente, False en caso contrario\n",
        "        \"\"\"\n",
        "        try:\n",
        "            datos_serializables = Utilidades.convertir_a_serializable(datos)\n",
        "            with open(ruta, 'w', encoding='utf-8') as f:\n",
        "                json.dump(datos_serializables, f, indent=2, ensure_ascii=False)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error guardando JSON {ruta.name}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def cargar_imagen_robusta(ruta: Path, max_dimension: int = None) -> Tuple[Optional[Image.Image], Optional[np.ndarray], Optional[np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Carga imagen con múltiples fallbacks y redimensionamiento.\n",
        "\n",
        "        Args:\n",
        "            ruta: Path a la imagen\n",
        "            max_dimension: Dimensión máxima permitida (redimensiona si excede)\n",
        "\n",
        "        Returns:\n",
        "            Tupla (imagen_pil, imagen_rgb_array, imagen_gray_array)\n",
        "            Retorna (None, None, None) si falla la carga\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Cargar con PIL\n",
        "            img_pil = Image.open(ruta)\n",
        "\n",
        "            # Redimensionar si excede max_dimension\n",
        "            if max_dimension and max(img_pil.size) > max_dimension:\n",
        "                ratio = max_dimension / max(img_pil.size)\n",
        "                new_size = tuple(int(dim * ratio) for dim in img_pil.size)\n",
        "                img_pil = img_pil.resize(new_size, Image.LANCZOS)\n",
        "                logging.info(f\"Imagen {ruta.name} redimensionada de {img_pil.size} a {new_size}\")\n",
        "\n",
        "            # Convertir a RGB si no lo es\n",
        "            if img_pil.mode != 'RGB':\n",
        "                img_pil = img_pil.convert('RGB')\n",
        "\n",
        "            # Convertir a arrays numpy\n",
        "            img_rgb = np.array(img_pil)\n",
        "            img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            return img_pil, img_rgb, img_gray\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error cargando imagen {ruta.name}: {str(e)}\")\n",
        "            return None, None, None"
      ],
      "metadata": {
        "id": "BBI83bPe6H8Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXTRACTOR DE METADATOS\n",
        "# =============================================================================\n",
        "\n",
        "class ExtractorMetadatos:\n",
        "    \"\"\"\n",
        "    Extractor de metadatos de archivo y EXIF.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def extraer_metadatos_archivo(self, ruta: Path, img: Image.Image) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extrae metadatos básicos del archivo.\n",
        "\n",
        "        Args:\n",
        "            ruta: Path del archivo\n",
        "            img: Imagen PIL\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con metadatos del archivo\n",
        "        \"\"\"\n",
        "        try:\n",
        "            stats = ruta.stat()\n",
        "            return {\n",
        "                'nombre_archivo': str(ruta.name),\n",
        "                'extension': str(ruta.suffix.lower()),\n",
        "                'tamaño_bytes': int(stats.st_size),\n",
        "                'tamaño_mb': round(stats.st_size / (1024 * 1024), 3),\n",
        "                'fecha_modificacion': datetime.fromtimestamp(stats.st_mtime).isoformat(),\n",
        "                'formato_imagen': str(img.format) if img.format else 'desconocido',\n",
        "                'modo_color': str(img.mode),\n",
        "                'ancho_original': int(img.size[0]),\n",
        "                'alto_original': int(img.size[1])\n",
        "            }\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extrayendo metadatos de archivo: {str(e)}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def extraer_metadatos_exif(self, ruta: Path) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extrae metadatos EXIF completos con manejo robusto.\n",
        "\n",
        "        Args:\n",
        "            ruta: Path del archivo\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con metadatos EXIF\n",
        "        \"\"\"\n",
        "        exif_data = {}\n",
        "\n",
        "        try:\n",
        "            # Intentar con piexif\n",
        "            exif_dict = piexif.load(str(ruta))\n",
        "\n",
        "            for ifd_name in [\"0th\", \"Exif\", \"GPS\", \"1st\"]:\n",
        "                if ifd_name not in exif_dict:\n",
        "                    continue\n",
        "\n",
        "                for tag, value in exif_dict[ifd_name].items():\n",
        "                    try:\n",
        "                        tag_name = piexif.TAGS[ifd_name][tag][\"name\"]\n",
        "                        exif_data[f\"{ifd_name}_{tag_name}\"] = self._convertir_valor_exif(value)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "        except:\n",
        "            # Fallback con exifread\n",
        "            try:\n",
        "                with open(ruta, 'rb') as f:\n",
        "                    tags = exifread.process_file(f, details=False)\n",
        "                    for tag, value in tags.items():\n",
        "                        if not tag.startswith('Thumbnail'):\n",
        "                            exif_data[tag] = str(value)\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"No se pudieron extraer metadatos EXIF de {ruta.name}: {str(e)}\")\n",
        "\n",
        "        return exif_data\n",
        "\n",
        "    def _convertir_valor_exif(self, valor: Any) -> Any:\n",
        "        \"\"\"\n",
        "        Convierte valores EXIF a tipos serializables.\n",
        "\n",
        "        Args:\n",
        "            valor: Valor EXIF a convertir\n",
        "\n",
        "        Returns:\n",
        "            Valor convertido\n",
        "        \"\"\"\n",
        "        if isinstance(valor, bytes):\n",
        "            try:\n",
        "                return valor.decode('utf-8', errors='ignore')\n",
        "            except:\n",
        "                return str(valor)\n",
        "        elif isinstance(valor, (tuple, list)):\n",
        "            return [self._convertir_valor_exif(v) for v in valor]\n",
        "        elif isinstance(valor, (int, float, str, bool)):\n",
        "            return valor\n",
        "        else:\n",
        "            return str(valor)"
      ],
      "metadata": {
        "id": "nlfkTe_c6Oo8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ANALIZADOR DE COLOR\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorColor:\n",
        "    \"\"\"\n",
        "    Análisis exhaustivo de características de color.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def analizar_estadisticas_color(self, img_pil: Image.Image, img_rgb: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza estadísticas de color en múltiples espacios (RGB, HSV, LAB).\n",
        "\n",
        "        Args:\n",
        "            img_pil: Imagen PIL\n",
        "            img_rgb: Imagen como array RGB\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con estadísticas de color\n",
        "        \"\"\"\n",
        "        try:\n",
        "            resultado = {}\n",
        "\n",
        "            # RGB statistics usando PIL\n",
        "            stat = ImageStat.Stat(img_pil)\n",
        "            resultado['rgb'] = {\n",
        "                'mean': [round(float(x), 2) for x in stat.mean],\n",
        "                'median': [round(float(x), 2) for x in stat.median],\n",
        "                'stddev': [round(float(x), 2) for x in stat.stddev],\n",
        "                'variance': [round(float(x), 2) for x in stat.var]\n",
        "            }\n",
        "\n",
        "            # HSV statistics\n",
        "            img_hsv = color.rgb2hsv(img_rgb / 255.0)\n",
        "            resultado['hsv'] = {\n",
        "                'hue_mean': round(float(np.mean(img_hsv[:,:,0])), 4),\n",
        "                'hue_std': round(float(np.std(img_hsv[:,:,0])), 4),\n",
        "                'saturation_mean': round(float(np.mean(img_hsv[:,:,1])), 4),\n",
        "                'saturation_std': round(float(np.std(img_hsv[:,:,1])), 4),\n",
        "                'value_mean': round(float(np.mean(img_hsv[:,:,2])), 4),\n",
        "                'value_std': round(float(np.std(img_hsv[:,:,2])), 4)\n",
        "            }\n",
        "\n",
        "            # LAB statistics\n",
        "            img_lab = color.rgb2lab(img_rgb)\n",
        "            resultado['lab'] = {\n",
        "                'l_mean': round(float(np.mean(img_lab[:,:,0])), 2),\n",
        "                'l_std': round(float(np.std(img_lab[:,:,0])), 2),\n",
        "                'a_mean': round(float(np.mean(img_lab[:,:,1])), 2),\n",
        "                'a_std': round(float(np.std(img_lab[:,:,1])), 2),\n",
        "                'b_mean': round(float(np.mean(img_lab[:,:,2])), 2),\n",
        "                'b_std': round(float(np.std(img_lab[:,:,2])), 2)\n",
        "            }\n",
        "\n",
        "            # Métricas globales\n",
        "            resultado['global'] = {\n",
        "                'brillo_promedio': round(float(np.mean(img_rgb)), 2),\n",
        "                'contraste': round(float(np.std(img_rgb)), 2),\n",
        "                'entropia': round(float(stats.entropy(img_rgb.flatten())), 4)\n",
        "            }\n",
        "\n",
        "            return resultado\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error en análisis de color: {str(e)}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def calcular_histogramas(self, img_rgb: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Calcula histogramas de color por canal.\n",
        "\n",
        "        Args:\n",
        "            img_rgb: Imagen RGB como array\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con histogramas\n",
        "        \"\"\"\n",
        "        try:\n",
        "            resultado = {}\n",
        "\n",
        "            canales = ['rojo', 'verde', 'azul']\n",
        "            for i, canal in enumerate(canales):\n",
        "                hist, _ = np.histogram(img_rgb[:,:,i], bins=256, range=(0, 256))\n",
        "                resultado[canal] = {\n",
        "                    'histograma': hist.tolist(),\n",
        "                    'pico_principal': int(np.argmax(hist)),\n",
        "                    'media': round(float(np.mean(img_rgb[:,:,i])), 2),\n",
        "                    'std': round(float(np.std(img_rgb[:,:,i])), 2)\n",
        "                }\n",
        "\n",
        "            # Histograma de intensidad\n",
        "            img_gray = color.rgb2gray(img_rgb)\n",
        "            hist_intensity, _ = np.histogram(img_gray, bins=256, range=(0, 1))\n",
        "            resultado['intensidad'] = {\n",
        "                'histograma': hist_intensity.tolist(),\n",
        "                'entropia': round(float(stats.entropy(hist_intensity + 1)), 4)\n",
        "            }\n",
        "\n",
        "            return resultado\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculando histogramas: {str(e)}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def extraer_paleta_colores(self, img_pil: Image.Image, n_colores: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extrae paleta de colores dominantes usando KMeans.\n",
        "\n",
        "        Args:\n",
        "            img_pil: Imagen PIL\n",
        "            n_colores: Número de colores a extraer\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con paleta de colores\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Redimensionar para acelerar clustering\n",
        "            img_small = img_pil.copy()\n",
        "            img_small.thumbnail((150, 150))\n",
        "\n",
        "            if img_small.mode != 'RGB':\n",
        "                img_small = img_small.convert('RGB')\n",
        "\n",
        "            pixels = np.array(img_small).reshape(-1, 3)\n",
        "\n",
        "            # KMeans clustering\n",
        "            n_clusters = min(n_colores, len(pixels))\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "            kmeans.fit(pixels)\n",
        "\n",
        "            colors = kmeans.cluster_centers_.astype(int)\n",
        "            counts = np.bincount(kmeans.labels_)\n",
        "\n",
        "            # Ordenar por frecuencia\n",
        "            indices = np.argsort(counts)[::-1]\n",
        "            colors_ordenados = colors[indices]\n",
        "            frecuencias = counts[indices] / np.sum(counts)\n",
        "\n",
        "            return {\n",
        "                'color_dominante': colors_ordenados[0].tolist(),\n",
        "                'paleta': colors_ordenados.tolist(),\n",
        "                'frecuencias': [round(float(f), 4) for f in frecuencias]\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extrayendo paleta: {str(e)}\")\n",
        "            return {'error': str(e)}"
      ],
      "metadata": {
        "id": "0hnpnVdS6Wcb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ANALIZADOR DE TEXTURAS\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorTexturas:\n",
        "    \"\"\"\n",
        "    Análisis de texturas usando Haralick, GLCM y LBP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def analizar_texturas(self, img_gray: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Análisis completo de texturas.\n",
        "\n",
        "        Args:\n",
        "            img_gray: Imagen en escala de grises\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con características de textura\n",
        "        \"\"\"\n",
        "        try:\n",
        "            resultado = {}\n",
        "\n",
        "            # Normalizar a uint8\n",
        "            if img_gray.dtype != np.uint8:\n",
        "                img_gray = (img_gray / img_gray.max() * 255).astype(np.uint8)\n",
        "\n",
        "            # Haralick features\n",
        "            resultado['haralick'] = self._calcular_haralick(img_gray)\n",
        "\n",
        "            # GLCM features\n",
        "            resultado['glcm'] = self._calcular_glcm(img_gray)\n",
        "\n",
        "            # LBP features\n",
        "            resultado['lbp'] = self._calcular_lbp(img_gray)\n",
        "\n",
        "            # Zernike moments\n",
        "            resultado['zernike'] = self._calcular_zernike(img_gray)\n",
        "\n",
        "            return resultado\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error en análisis de texturas: {str(e)}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def _calcular_haralick(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcula características de Haralick\"\"\"\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(img, return_mean=True)\n",
        "\n",
        "            nombres = [\n",
        "                'angular_second_moment', 'contrast', 'correlation',\n",
        "                'sum_of_squares', 'inverse_diff_moment', 'sum_average',\n",
        "                'sum_variance', 'sum_entropy', 'entropy', 'difference_variance',\n",
        "                'difference_entropy', 'info_measure_correlation_1',\n",
        "                'info_measure_correlation_2'\n",
        "            ]\n",
        "\n",
        "            return {nombre: round(float(valor), 6)\n",
        "                   for nombre, valor in zip(nombres, haralick_features)}\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _calcular_glcm(self, img: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"Calcula características GLCM multi-escala\"\"\"\n",
        "        try:\n",
        "            distances = [1, 3, 5]\n",
        "            angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
        "\n",
        "            glcm = feature.graycomatrix(\n",
        "                img, distances=distances, angles=angles,\n",
        "                levels=256, symmetric=True, normed=True\n",
        "            )\n",
        "\n",
        "            resultado = {}\n",
        "            for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']:\n",
        "                valores = feature.graycoprops(glcm, prop)\n",
        "                resultado[f'{prop}_mean'] = round(float(np.mean(valores)), 6)\n",
        "                resultado[f'{prop}_std'] = round(float(np.std(valores)), 6)\n",
        "\n",
        "            return resultado\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _calcular_lbp(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcula Local Binary Patterns\"\"\"\n",
        "        try:\n",
        "            lbp = feature.local_binary_pattern(img, P=8, R=1, method='uniform')\n",
        "\n",
        "            return {\n",
        "                'mean': round(float(np.mean(lbp)), 4),\n",
        "                'std': round(float(np.std(lbp)), 4),\n",
        "                'entropy': round(float(stats.entropy(lbp.flatten())), 6)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _calcular_zernike(self, img: np.ndarray) -> List[float]:\n",
        "        \"\"\"Calcula momentos de Zernike\"\"\"\n",
        "        try:\n",
        "            zernike = mh.features.zernike_moments(img, radius=20, degree=8)\n",
        "            return [round(float(z), 6) for z in zernike]\n",
        "        except:\n",
        "            return []"
      ],
      "metadata": {
        "id": "Noi_aoHF6bt-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ANALIZADOR DE BORDES Y CARACTERÍSTICAS VISUALES\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorBordes:\n",
        "    \"\"\"\n",
        "    Análisis de bordes, esquinas y características geométricas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def analizar_bordes(self, img_gray: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Análisis completo de bordes y esquinas.\n",
        "\n",
        "        Args:\n",
        "            img_gray: Imagen en escala de grises\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con características de bordes\n",
        "        \"\"\"\n",
        "        try:\n",
        "            resultado = {}\n",
        "\n",
        "            # Detección de bordes multi-escala\n",
        "            resultado['canny'] = self._analizar_canny(img_gray)\n",
        "\n",
        "            # Detección de esquinas\n",
        "            resultado['esquinas'] = self._detectar_esquinas(img_gray)\n",
        "\n",
        "            # Análisis de gradientes\n",
        "            resultado['gradientes'] = self._analizar_gradientes(img_gray)\n",
        "\n",
        "            # HOG features\n",
        "            resultado['hog'] = self._calcular_hog(img_gray)\n",
        "\n",
        "            return resultado\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error en análisis de bordes: {str(e)}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def _analizar_canny(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Detección de bordes Canny multi-escala\"\"\"\n",
        "        try:\n",
        "            sigmas = [1.0, 2.0, 3.0]\n",
        "            densidades = []\n",
        "\n",
        "            for sigma in sigmas:\n",
        "                edges = feature.canny(img, sigma=sigma)\n",
        "                densidad = float(np.sum(edges) / edges.size)\n",
        "                densidades.append(densidad)\n",
        "\n",
        "            return {\n",
        "                f'densidad_sigma_{sigma}': round(dens, 6)\n",
        "                for sigma, dens in zip(sigmas, densidades)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _detectar_esquinas(self, img: np.ndarray) -> Dict[str, int]:\n",
        "        \"\"\"Detección de esquinas Harris y Shi-Tomasi\"\"\"\n",
        "        try:\n",
        "            harris = feature.corner_peaks(\n",
        "                feature.corner_harris(img),\n",
        "                min_distance=5,\n",
        "                threshold_rel=0.02\n",
        "            )\n",
        "\n",
        "            shi_tomasi = feature.corner_peaks(\n",
        "                feature.corner_shi_tomasi(img),\n",
        "                min_distance=5\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'harris': int(len(harris)),\n",
        "                'shi_tomasi': int(len(shi_tomasi)),\n",
        "                'densidad_harris': round(float(len(harris) / img.size), 8)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _analizar_gradientes(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Análisis de gradientes con múltiples operadores\"\"\"\n",
        "        try:\n",
        "            grad_sobel = filters.sobel(img)\n",
        "            grad_scharr = filters.scharr(img)\n",
        "            grad_prewitt = filters.prewitt(img)\n",
        "\n",
        "            return {\n",
        "                'sobel_mean': round(float(np.mean(grad_sobel)), 6),\n",
        "                'sobel_max': round(float(np.max(grad_sobel)), 6),\n",
        "                'sobel_std': round(float(np.std(grad_sobel)), 6),\n",
        "                'scharr_mean': round(float(np.mean(grad_scharr)), 6),\n",
        "                'prewitt_mean': round(float(np.mean(grad_prewitt)), 6)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _calcular_hog(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcula Histogram of Oriented Gradients\"\"\"\n",
        "        try:\n",
        "            hog_features = feature.hog(\n",
        "                img, orientations=9,\n",
        "                pixels_per_cell=(8, 8),\n",
        "                cells_per_block=(2, 2),\n",
        "                visualize=False\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'mean': round(float(np.mean(hog_features)), 6),\n",
        "                'std': round(float(np.std(hog_features)), 6),\n",
        "                'max': round(float(np.max(hog_features)), 6)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n"
      ],
      "metadata": {
        "id": "FrLW5q-W6iyg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ANALIZADOR DE CALIDAD\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorCalidad:\n",
        "    \"\"\"\n",
        "    Análisis de calidad, nitidez y exposición de imagen.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def analizar_calidad(self, img_gray: np.ndarray, img_rgb: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Análisis completo de calidad de imagen.\n",
        "\n",
        "        Args:\n",
        "            img_gray: Imagen en escala de grises\n",
        "            img_rgb: Imagen RGB\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con métricas de calidad\n",
        "        \"\"\"\n",
        "        try:\n",
        "            resultado = {}\n",
        "\n",
        "            # Nitidez\n",
        "            resultado['nitidez'] = self._analizar_nitidez(img_gray)\n",
        "\n",
        "            # Exposición\n",
        "            resultado['exposicion'] = self._analizar_exposicion(img_gray)\n",
        "\n",
        "            # Contraste\n",
        "            resultado['contraste'] = self._analizar_contraste(img_gray)\n",
        "\n",
        "            # Balance de blancos\n",
        "            resultado['balance_blancos'] = self._analizar_balance_blancos(img_rgb)\n",
        "\n",
        "            # Ruido\n",
        "            resultado['ruido'] = self._estimar_ruido(img_gray)\n",
        "\n",
        "            return resultado\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error en análisis de calidad: {str(e)}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def _analizar_nitidez(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Análisis de nitidez con múltiples métricas\"\"\"\n",
        "        try:\n",
        "            # Laplacian variance\n",
        "            laplacian = cv2.Laplacian(img.astype(np.float64), cv2.CV_64F)\n",
        "            nitidez_laplacian = float(np.var(laplacian))\n",
        "\n",
        "            # Tenengrad (gradiente al cuadrado)\n",
        "            gx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
        "            gy = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
        "            nitidez_tenengrad = float(np.mean(gx**2 + gy**2))\n",
        "\n",
        "            # Varianza normalizada\n",
        "            nitidez_varianza = float(np.var(img) / (np.mean(img) + 1e-6))\n",
        "\n",
        "            return {\n",
        "                'laplacian': round(nitidez_laplacian, 4),\n",
        "                'tenengrad': round(nitidez_tenengrad, 4),\n",
        "                'varianza_normalizada': round(nitidez_varianza, 4)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _analizar_exposicion(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Análisis de exposición\"\"\"\n",
        "        try:\n",
        "            brillo = float(np.mean(img))\n",
        "            sobre_exp = float(np.sum(img > 250) / img.size * 100)\n",
        "            sub_exp = float(np.sum(img < 5) / img.size * 100)\n",
        "\n",
        "            hist, _ = np.histogram(img, bins=256, range=(0, 256))\n",
        "            hist_norm = hist / np.sum(hist)\n",
        "            entropia = float(-np.sum(hist_norm * np.log2(hist_norm + 1e-10)))\n",
        "\n",
        "            return {\n",
        "                'brillo_medio': round(brillo, 2),\n",
        "                'sobre_expuesto_pct': round(sobre_exp, 3),\n",
        "                'sub_expuesto_pct': round(sub_exp, 3),\n",
        "                'entropia': round(entropia, 4),\n",
        "                'rango_dinamico': int(img.max() - img.min())\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _analizar_contraste(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Análisis de contraste\"\"\"\n",
        "        try:\n",
        "            contraste_rms = float(np.sqrt(np.mean((img - np.mean(img))**2)))\n",
        "            michelson = float((img.max() - img.min()) / (img.max() + img.min() + 1e-6))\n",
        "\n",
        "            return {\n",
        "                'rms': round(contraste_rms, 4),\n",
        "                'michelson': round(michelson, 6)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _analizar_balance_blancos(self, img_rgb: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Análisis de balance de blancos\"\"\"\n",
        "        try:\n",
        "            mean_r = float(np.mean(img_rgb[:,:,0]))\n",
        "            mean_g = float(np.mean(img_rgb[:,:,1]))\n",
        "            mean_b = float(np.mean(img_rgb[:,:,2]))\n",
        "\n",
        "            desviacion = float(np.std([mean_r, mean_g, mean_b]))\n",
        "\n",
        "            return {\n",
        "                'mean_red': round(mean_r, 2),\n",
        "                'mean_green': round(mean_g, 2),\n",
        "                'mean_blue': round(mean_b, 2),\n",
        "                'desviacion_canales': round(desviacion, 4)\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _estimar_ruido(self, img: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Estimación de nivel de ruido\"\"\"\n",
        "        try:\n",
        "            # Laplacian noise estimate\n",
        "            laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
        "            ruido_lap = float(np.std(laplacian))\n",
        "\n",
        "            # Median filter noise estimate\n",
        "            median_filtered = ndimage.median_filter(img, size=3)\n",
        "            noise_estimate = img.astype(float) - median_filtered.astype(float)\n",
        "            ruido_median = float(np.std(noise_estimate))\n",
        "\n",
        "            # SNR\n",
        "            signal = np.mean(img)\n",
        "            noise = np.std(img)\n",
        "            snr = float(signal / (noise + 1e-6))\n",
        "            snr_db = float(20 * np.log10(snr + 1e-6))\n",
        "\n",
        "            return {\n",
        "                'laplacian': round(ruido_lap, 4),\n",
        "                'median_filter': round(ruido_median, 4),\n",
        "                'snr': round(snr, 4),\n",
        "                'snr_db': round(snr_db, 2)\n",
        "            }\n",
        "        except:\n",
        "            return {}"
      ],
      "metadata": {
        "id": "0DRZO2FA6pzm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ANALIZADOR DE FRECUENCIAS\n",
        "# =============================================================================\n",
        "\n",
        "class AnalizadorFrecuencias:\n",
        "    \"\"\"\n",
        "    Análisis en el dominio de frecuencias usando FFT.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logger: logging.Logger):\n",
        "        self.logger = logger\n",
        "\n",
        "    def analizar_frecuencias(self, img_gray: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Análisis de frecuencias mediante FFT 2D.\n",
        "\n",
        "        Args:\n",
        "            img_gray: Imagen en escala de grises\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con características espectrales\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # FFT 2D\n",
        "            fft = np.fft.fft2(img_gray)\n",
        "            fft_shift = np.fft.fftshift(fft)\n",
        "            magnitude = np.abs(fft_shift)\n",
        "\n",
        "            # Dividir en bandas de frecuencia\n",
        "            rows, cols = img_gray.shape\n",
        "            crow, ccol = rows // 2, cols // 2\n",
        "\n",
        "            y, x = np.ogrid[:rows, :cols]\n",
        "            distance = np.sqrt((x - ccol)**2 + (y - crow)**2)\n",
        "\n",
        "            # Máscaras para bandas\n",
        "            mask_low = distance <= 30\n",
        "            mask_mid = (distance > 30) & (distance <= 100)\n",
        "            mask_high = distance > 100\n",
        "\n",
        "            # Energías por banda\n",
        "            energia_baja = float(np.sum(magnitude[mask_low]))\n",
        "            energia_media = float(np.sum(magnitude[mask_mid]))\n",
        "            energia_alta = float(np.sum(magnitude[mask_high]))\n",
        "            energia_total = energia_baja + energia_media + energia_alta\n",
        "\n",
        "            # Ratios\n",
        "            ratio_baja = float(energia_baja / energia_total) if energia_total > 0 else 0\n",
        "            ratio_media = float(energia_media / energia_total) if energia_total > 0 else 0\n",
        "            ratio_alta = float(energia_alta / energia_total) if energia_total > 0 else 0\n",
        "\n",
        "            # Entropía espectral\n",
        "            spectrum_norm = magnitude / (np.sum(magnitude) + 1e-10)\n",
        "            entropia = float(-np.sum(spectrum_norm * np.log2(spectrum_norm + 1e-10)))\n",
        "\n",
        "            return {\n",
        "                'energia_frecuencia_baja': round(energia_baja, 2),\n",
        "                'energia_frecuencia_media': round(energia_media, 2),\n",
        "                'energia_frecuencia_alta': round(energia_alta, 2),\n",
        "                'ratio_baja': round(ratio_baja, 6),\n",
        "                'ratio_media': round(ratio_media, 6),\n",
        "                'ratio_alta': round(ratio_alta, 6),\n",
        "                'entropia_espectral': round(entropia, 6),\n",
        "                'pico_dominante': round(float(np.max(magnitude)), 2)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error en análisis de frecuencias: {str(e)}\")\n",
        "            return {'error': str(e)}"
      ],
      "metadata": {
        "id": "KbL-k0J56x9f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXTRACTOR PRINCIPAL\n",
        "# =============================================================================\n",
        "\n",
        "class ExtractorCaracteristicasImagen:\n",
        "    \"\"\"\n",
        "    Clase principal que orquesta la extracción de características.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ConfiguracionExtraccion):\n",
        "        \"\"\"\n",
        "        Inicializa el extractor.\n",
        "\n",
        "        Args:\n",
        "            config: Configuración del sistema\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.config.crear_estructura_directorios()\n",
        "\n",
        "        # Inicializar logger\n",
        "        self.logger_manager = LoggerManager(\n",
        "            self.config.ruta_resultados / \"logs\",\n",
        "            self.config.nivel_log\n",
        "        )\n",
        "        self.logger = self.logger_manager.get_logger()\n",
        "\n",
        "        # Inicializar analizadores\n",
        "        self.extractor_metadatos = ExtractorMetadatos(self.logger)\n",
        "        self.analizador_color = AnalizadorColor(self.logger)\n",
        "        self.analizador_texturas = AnalizadorTexturas(self.logger)\n",
        "        self.analizador_bordes = AnalizadorBordes(self.logger)\n",
        "        self.analizador_calidad = AnalizadorCalidad(self.logger)\n",
        "        self.analizador_frecuencias = AnalizadorFrecuencias(self.logger)\n",
        "\n",
        "        self.resultados = []\n",
        "\n",
        "        self.logger.info(\"Sistema de extracción inicializado correctamente\")\n",
        "\n",
        "    def procesar_imagen(self, ruta_imagen: Path) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Procesa una imagen completa.\n",
        "\n",
        "        Args:\n",
        "            ruta_imagen: Path a la imagen\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con todas las características extraídas\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Procesando imagen: {ruta_imagen.name}\")\n",
        "        inicio = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Cargar imagen\n",
        "            img_pil, img_rgb, img_gray = Utilidades.cargar_imagen_robusta(\n",
        "                ruta_imagen,\n",
        "                self.config.max_dimension_procesamiento\n",
        "            )\n",
        "\n",
        "            if img_pil is None:\n",
        "                raise ValueError(\"Error cargando imagen\")\n",
        "\n",
        "            # Estructura de resultados\n",
        "            caracteristicas = {\n",
        "                'metadatos_archivo': self.extractor_metadatos.extraer_metadatos_archivo(ruta_imagen, img_pil)\n",
        "            }\n",
        "\n",
        "            # Metadatos EXIF\n",
        "            if self.config.calcular_exif:\n",
        "                caracteristicas['metadatos_exif'] = self.extractor_metadatos.extraer_metadatos_exif(ruta_imagen)\n",
        "\n",
        "            # Estadísticas de color\n",
        "            caracteristicas['estadisticas_color'] = self.analizador_color.analizar_estadisticas_color(img_pil, img_rgb)\n",
        "\n",
        "            # Histogramas\n",
        "            if self.config.calcular_histogramas:\n",
        "                caracteristicas['histogramas'] = self.analizador_color.calcular_histogramas(img_rgb)\n",
        "\n",
        "            # Paleta de colores\n",
        "            if self.config.calcular_colores_dominantes:\n",
        "                caracteristicas['colores_dominantes'] = self.analizador_color.extraer_paleta_colores(\n",
        "                    img_pil,\n",
        "                    self.config.num_colores_paleta\n",
        "                )\n",
        "\n",
        "            # Texturas\n",
        "            if self.config.calcular_texturas:\n",
        "                caracteristicas['texturas'] = self.analizador_texturas.analizar_texturas(img_gray)\n",
        "\n",
        "            # Bordes\n",
        "            if self.config.calcular_bordes:\n",
        "                caracteristicas['bordes_caracteristicas'] = self.analizador_bordes.analizar_bordes(img_gray)\n",
        "\n",
        "            # Calidad\n",
        "            caracteristicas['calidad'] = self.analizador_calidad.analizar_calidad(img_gray, img_rgb)\n",
        "\n",
        "            # Frecuencias\n",
        "            if self.config.calcular_frecuencias:\n",
        "                caracteristicas['analisis_frecuencia'] = self.analizador_frecuencias.analizar_frecuencias(img_gray)\n",
        "\n",
        "            # Hashes perceptuales\n",
        "            if self.config.calcular_hashes:\n",
        "                caracteristicas['hashes_perceptuales'] = self._calcular_hashes(img_pil)\n",
        "\n",
        "            # Timestamp y tiempo de procesamiento\n",
        "            tiempo_procesamiento = (datetime.now() - inicio).total_seconds()\n",
        "            caracteristicas['procesamiento'] = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'tiempo_segundos': round(tiempo_procesamiento, 3)\n",
        "            }\n",
        "\n",
        "            self.logger.info(f\"Imagen procesada exitosamente en {tiempo_procesamiento:.2f}s\")\n",
        "            return caracteristicas\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error procesando {ruta_imagen.name}: {str(e)}\")\n",
        "            return {\n",
        "                'archivo': str(ruta_imagen.name),\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "    def _calcular_hashes(self, img: Image.Image) -> Dict[str, str]:\n",
        "        \"\"\"Calcula hashes perceptuales para similitud\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'average_hash': str(imagehash.average_hash(img)),\n",
        "                'perceptual_hash': str(imagehash.phash(img)),\n",
        "                'difference_hash': str(imagehash.dhash(img)),\n",
        "                'wavelet_hash': str(imagehash.whash(img)),\n",
        "                'color_hash': str(imagehash.colorhash(img))\n",
        "            }\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def procesar_directorio(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Procesa todas las imágenes del directorio configurado.\n",
        "\n",
        "        Returns:\n",
        "            Lista de diccionarios con características de cada imagen\n",
        "        \"\"\"\n",
        "        extensiones = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
        "        imagenes = []\n",
        "\n",
        "        for ext in extensiones:\n",
        "            imagenes.extend(self.config.ruta_imagenes.glob(ext))\n",
        "\n",
        "        if not imagenes:\n",
        "            self.logger.warning(f\"No se encontraron imágenes en {self.config.ruta_imagenes}\")\n",
        "            return []\n",
        "\n",
        "        self.logger.info(f\"Iniciando procesamiento de {len(imagenes)} imágenes\")\n",
        "\n",
        "        resultados = []\n",
        "        for i, img_path in enumerate(imagenes, 1):\n",
        "            self.logger.info(f\"[{i}/{len(imagenes)}] Procesando {img_path.name}\")\n",
        "\n",
        "            resultado = self.procesar_imagen(img_path)\n",
        "            resultados.append(resultado)\n",
        "\n",
        "            # Guardar JSON individual si está configurado\n",
        "            if self.config.guardar_json_individual:\n",
        "                json_path = self.config.ruta_resultados / \"json\" / f\"{img_path.stem}_caracteristicas.json\"\n",
        "                Utilidades.guardar_json(resultado, json_path)\n",
        "\n",
        "        self.resultados = resultados\n",
        "        self.logger.info(f\"Procesamiento completado: {len(resultados)} imágenes\")\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    def guardar_consolidado(self, nombre_archivo: str = None) -> Optional[Path]:\n",
        "        \"\"\"\n",
        "        Guarda todos los resultados en un JSON consolidado.\n",
        "\n",
        "        Args:\n",
        "            nombre_archivo: Nombre del archivo (opcional)\n",
        "\n",
        "        Returns:\n",
        "            Path del archivo guardado o None si falla\n",
        "        \"\"\"\n",
        "        if not self.resultados:\n",
        "            self.logger.warning(\"No hay resultados para guardar\")\n",
        "            return None\n",
        "\n",
        "        if nombre_archivo is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            nombre_archivo = f\"caracteristicas_consolidadas_{timestamp}.json\"\n",
        "\n",
        "        ruta_salida = self.config.ruta_resultados / \"json\" / nombre_archivo\n",
        "\n",
        "        datos_consolidados = {\n",
        "            'metadata': {\n",
        "                'timestamp_generacion': datetime.now().isoformat(),\n",
        "                'total_imagenes': len(self.resultados),\n",
        "                'imagenes_exitosas': len([r for r in self.resultados if 'error' not in r]),\n",
        "                'imagenes_con_error': len([r for r in self.resultados if 'error' in r]),\n",
        "                'ruta_origen': str(self.config.ruta_imagenes),\n",
        "                'configuracion': Utilidades.convertir_a_serializable(asdict(self.config))\n",
        "            },\n",
        "            'imagenes': self.resultados\n",
        "        }\n",
        "\n",
        "        if Utilidades.guardar_json(datos_consolidados, ruta_salida):\n",
        "            tamaño_kb = ruta_salida.stat().st_size / 1024\n",
        "            self.logger.info(f\"Archivo consolidado guardado: {ruta_salida.name} ({tamaño_kb:.2f} KB)\")\n",
        "            return ruta_salida\n",
        "\n",
        "        return None\n",
        "\n",
        "    def generar_reporte(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera reporte estadístico del procesamiento.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con estadísticas del procesamiento\n",
        "        \"\"\"\n",
        "        if not self.resultados:\n",
        "            return {}\n",
        "\n",
        "        exitosas = [r for r in self.resultados if 'error' not in r]\n",
        "        con_error = [r for r in self.resultados if 'error' in r]\n",
        "\n",
        "        tiempos = [r.get('procesamiento', {}).get('tiempo_segundos', 0)\n",
        "                  for r in exitosas]\n",
        "\n",
        "        return {\n",
        "            'total_imagenes': len(self.resultados),\n",
        "            'exitosas': len(exitosas),\n",
        "            'con_error': len(con_error),\n",
        "            'tiempo_total_segundos': round(sum(tiempos), 2),\n",
        "            'tiempo_promedio_segundos': round(np.mean(tiempos), 2) if tiempos else 0,\n",
        "            'tiempo_min_segundos': round(min(tiempos), 2) if tiempos else 0,\n",
        "            'tiempo_max_segundos': round(max(tiempos), 2) if tiempos else 0\n",
        "        }"
      ],
      "metadata": {
        "id": "gW2td6go63FS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FUNCIÓN PRINCIPAL\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Función principal de ejecución del sistema.\n",
        "    \"\"\"\n",
        "    # Montar Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Configurar sistema\n",
        "    config = ConfiguracionExtraccion(\n",
        "        ruta_base=Path(\"/content/drive/MyDrive/TFM/\"),\n",
        "        max_dimension_procesamiento=2048,\n",
        "        calcular_exif=True,\n",
        "        calcular_histogramas=True,\n",
        "        calcular_texturas=True,\n",
        "        calcular_bordes=True,\n",
        "        calcular_frecuencias=True,\n",
        "        calcular_ruido=True,\n",
        "        calcular_colores_dominantes=True,\n",
        "        calcular_hashes=True,\n",
        "        num_colores_paleta=5,\n",
        "        guardar_json_individual=True,\n",
        "        guardar_json_consolidado=True,\n",
        "        nivel_log=\"INFO\"\n",
        "    )\n",
        "\n",
        "    # Validar configuración\n",
        "    if not config.validar():\n",
        "        print(\"ERROR: Configuración inválida\")\n",
        "        return None\n",
        "\n",
        "    # Crear extractor\n",
        "    extractor = ExtractorCaracteristicasImagen(config)\n",
        "\n",
        "    # Procesar imágenes\n",
        "    resultados = extractor.procesar_directorio()\n",
        "\n",
        "    # Guardar consolidado\n",
        "    if resultados and config.guardar_json_consolidado:\n",
        "        extractor.guardar_consolidado()\n",
        "\n",
        "    # Generar reporte\n",
        "    reporte = extractor.generar_reporte()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESUMEN DE PROCESAMIENTO\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total imágenes: {reporte.get('total_imagenes', 0)}\")\n",
        "    print(f\"Exitosas: {reporte.get('exitosas', 0)}\")\n",
        "    print(f\"Con errores: {reporte.get('con_error', 0)}\")\n",
        "    print(f\"Tiempo total: {reporte.get('tiempo_total_segundos', 0):.2f}s\")\n",
        "    print(f\"Tiempo promedio: {reporte.get('tiempo_promedio_segundos', 0):.2f}s\")\n",
        "    print(f\"Resultados: {config.ruta_resultados / 'json'}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return resultados"
      ],
      "metadata": {
        "id": "cLPRfeuL7E1Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    resultados = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE18ByrA7HeK",
        "outputId": "f96f06a4-6b8d-4dbf-d109-ea72f1fe84d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "================================================================================\n",
            "RESUMEN DE PROCESAMIENTO\n",
            "================================================================================\n",
            "Total imágenes: 23\n",
            "Exitosas: 23\n",
            "Con errores: 0\n",
            "Tiempo total: 299.23s\n",
            "Tiempo promedio: 13.01s\n",
            "Resultados: /content/drive/MyDrive/TFM/1_Caracteristicas/json\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}