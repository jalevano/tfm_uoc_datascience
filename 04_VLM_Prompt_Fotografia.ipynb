{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO6y/akdFpR93JA/rgIcO1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/04_VLM_Prompt_Fotografia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "FASE VLM - PROMPT ENRIQUECIDO PARA MENTOR DE FOTOGRAF√çA\n",
        "================================================================================\n",
        "Trabajo Fin de M√°ster - Evaluaci√≥n Comparativa de T√©cnicas de Segmentaci√≥n\n",
        "Universidad Oberta de Catalunya (UOC)\n",
        "Autor: Jes√∫s L.\n",
        "Fecha: Diciembre 2025\n",
        "\n",
        "Objetivo:\n",
        "    Generar recomendaciones did√°cticas para mejorar la fotograf√≠a de retrato\n",
        "    utilizando datos EXIF, m√©tricas de calidad y resultados de segmentaci√≥n.\n",
        "\n",
        "Enfoque:\n",
        "    - VLM como mentor, no como evaluador\n",
        "    - Tono did√°ctico-accesible con datos t√©cnicos entre par√©ntesis\n",
        "    - Orientado a mejora pr√°ctica del fot√≥grafo\n",
        "================================================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "e5PPJwhOFKNg",
        "outputId": "db07e248-c765-4458-9221-4b6c38059f00"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nFASE VLM - PROMPT ENRIQUECIDO PARA MENTOR DE FOTOGRAF√çA\\n================================================================================\\nTrabajo Fin de M√°ster - Evaluaci√≥n Comparativa de T√©cnicas de Segmentaci√≥n\\nUniversidad Oberta de Catalunya (UOC)\\nAutor: Jes√∫s L.\\nFecha: Diciembre 2025\\n\\nObjetivo:\\n    Generar recomendaciones did√°cticas para mejorar la fotograf√≠a de retrato\\n    utilizando datos EXIF, m√©tricas de calidad y resultados de segmentaci√≥n.\\n\\nEnfoque:\\n    - VLM como mentor, no como evaluador\\n    - Tono did√°ctico-accesible con datos t√©cnicos entre par√©ntesis\\n    - Orientado a mejora pr√°ctica del fot√≥grafo\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, Optional\n",
        "from PIL import Image\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6kpYXu3FOzR",
        "outputId": "d80cfeca-c2c3-4191-e495-d592100d0068"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURACI√ìN\n",
        "# ==============================================================================\n",
        "\n",
        "class ConfigVLM:\n",
        "    \"\"\"Configuraci√≥n centralizada para el pipeline VLM.\"\"\"\n",
        "\n",
        "    # Rutas base\n",
        "    BASE_PATH = Path(\"/content/drive/MyDrive/TFM\")\n",
        "\n",
        "    # Recursos de entrada\n",
        "    INDICE_MAESTRO = BASE_PATH / \"3_Analisis\" / \"fase1_integracion\" / \"indice_maestro.json\"\n",
        "    FOTOS_EDITADAS = BASE_PATH / \"0_Imagenes\"\n",
        "    METRICAS_CSV = BASE_PATH / \"3_Analisis\" / \"fase2b_correlaciones\" / \"metricas_fusionadas.csv\"\n",
        "\n",
        "    # Salidas VLM\n",
        "    OUTPUT_DIR = BASE_PATH / \"3_Analisis\" / \"fase_vlm\"\n",
        "    OUTPUT_IMAGENES = OUTPUT_DIR / \"imagenes_compuestas\"\n",
        "    OUTPUT_RESPUESTAS = OUTPUT_DIR / \"respuestas_vlm\"\n",
        "\n",
        "    # Configuraci√≥n del modelo\n",
        "    GEMINI_MODEL = \"gemini-2.5-flash\"\n",
        "\n",
        "    # Par√°metros de generaci√≥n\n",
        "    GENERATION_CONFIG = {\n",
        "        \"temperature\": 0.4,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def crear_directorios(cls) -> None:\n",
        "        \"\"\"Crea estructura de directorios para outputs.\"\"\"\n",
        "        for directorio in [cls.OUTPUT_DIR, cls.OUTPUT_IMAGENES, cls.OUTPUT_RESPUESTAS]:\n",
        "            directorio.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "VBYj-uafFTqN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PROMPT DEL MENTOR DE FOTOGRAF√çA\n",
        "# ==============================================================================\n",
        "\n",
        "PROMPT_MENTOR_FOTOGRAFIA = \"\"\"Eres un mentor de fotograf√≠a de retrato que usa datos cuantitativos para dar feedback ultra-espec√≠fico.\n",
        "\n",
        "IMPORTANTE: Las m√©tricas de calidad (brillo, contraste, saturaci√≥n, nitidez) fueron extra√≠das del archivo RAW ANTES de la edici√≥n. La imagen que ves es la versi√≥n EDITADA. Esto te permite evaluar las decisiones de edici√≥n del fot√≥grafo.\n",
        "\n",
        "=== IMAGEN ===\n",
        "Izquierda: Fotograf√≠a editada final\n",
        "Derecha: M√°scara de segmentaci√≥n (verde = persona detectada)\n",
        "\n",
        "=== PAR√ÅMETROS DE CAPTURA ===\n",
        "- C√°mara: {modelo_camara}\n",
        "- Apertura: f/{apertura}\n",
        "- ISO: {iso}\n",
        "- Velocidad: {tiempo_exposicion}s\n",
        "- Distancia focal: {distancia_focal}mm\n",
        "- Dimensiones: {ancho}x{alto} px\n",
        "\n",
        "=== M√âTRICAS DEL RAW (pre-edici√≥n) ===\n",
        "- Brillo: {brillo:.1f}/255 (ideal retratos: 120-140)\n",
        "- Contraste RMS: {contraste:.1f} (rango t√≠pico: 40-80)\n",
        "- Saturaci√≥n: {saturacion:.1%}\n",
        "- Nitidez Laplacian: {nitidez:.1f} (>30 = buena)\n",
        "- SNR: {snr_db:.1f} dB (>20 = bajo ruido)\n",
        "\n",
        "=== COMPOSICI√ìN Y SALIENCIA ===\n",
        "- Posici√≥n sujeto: ({centroide_x:.1%}, {centroide_y:.1%})\n",
        "- Espacio negativo: {espacio_negativo:.1%}\n",
        "- Centro atenci√≥n visual: ({sal_centroide_x:.1%}, {sal_centroide_y:.1%})\n",
        "- Concentraci√≥n atenci√≥n: {sal_ratio:.2f} (>1 = centrada en sujeto)\n",
        "\n",
        "=== M√âTRICAS DE SEGMENTACI√ìN (imagen editada) ===\n",
        "Estas m√©tricas indican qu√© tan bien se separa visualmente la persona del fondo:\n",
        "\n",
        "- IoU: {iou:.1%} (>90% excelente, 80-90% bueno, <80% mejorable)\n",
        "- Boundary IoU: {boundary_iou:.1%} (precisi√≥n en bordes: cabello, ropa)\n",
        "- Precision: {precision:.1%} (bajo = fondo invade al sujeto)\n",
        "- Recall: {recall:.1%} (bajo = partes del sujeto se pierden)\n",
        "- Solidez silueta: {solidity:.1%} (alto = contorno definido)\n",
        "- Recorte bordes: {recorte_bordes:.1%}\n",
        "- Contraste interior/borde: {contraste_interior:.1f} / {contraste_borde:.1f}\n",
        "\n",
        "=== TU TAREA ===\n",
        "\n",
        "Analiza la imagen y los datos. Proporciona feedback ULTRA-ESPEC√çFICO en JSON:\n",
        "\n",
        "{{\n",
        "  \"evaluacion_edicion\": {{\n",
        "    \"mejoras_detectadas\": [\n",
        "      {{\n",
        "        \"aspecto\": \"<qu√© mejor√≥ respecto al RAW>\",\n",
        "        \"valor_raw\": \"<dato del RAW>\",\n",
        "        \"resultado_visible\": \"<qu√© ves en la imagen editada>\",\n",
        "        \"valoracion\": \"<bien logrado / parcialmente logrado / insuficiente>\"\n",
        "      }}\n",
        "    ],\n",
        "    \"resumen\": \"<1 frase sobre las decisiones de edici√≥n>\"\n",
        "  }},\n",
        "\n",
        "  \"fortalezas\": [\n",
        "    {{\n",
        "      \"aspecto\": \"<nombre>\",\n",
        "      \"dato_que_lo_demuestra\": \"<m√©trica espec√≠fica y su valor>\",\n",
        "      \"por_que_funciona\": \"<explicaci√≥n t√©cnica breve>\"\n",
        "    }}\n",
        "  ],\n",
        "\n",
        "  \"recomendaciones\": [\n",
        "    {{\n",
        "      \"prioridad\": <1-3>,\n",
        "      \"problema_detectado\": \"<descripci√≥n espec√≠fica>\",\n",
        "      \"dato_que_lo_evidencia\": \"<m√©trica y valor exacto>\",\n",
        "      \"ajuste_recomendado\": \"<acci√≥n CONCRETA con valores espec√≠ficos>\",\n",
        "      \"valores_sugeridos\": {{\n",
        "        \"parametro\": \"<nombre>\",\n",
        "        \"valor_actual\": \"<valor>\",\n",
        "        \"valor_recomendado\": \"<valor espec√≠fico>\",\n",
        "        \"alternativa\": \"<si aplica>\"\n",
        "      }},\n",
        "      \"impacto_esperado\": {{\n",
        "        \"en_la_foto\": \"<efecto visual concreto>\",\n",
        "        \"en_metricas\": \"<qu√© m√©tricas mejorar√≠an y cu√°nto aproximadamente>\"\n",
        "      }}\n",
        "    }}\n",
        "  ],\n",
        "\n",
        "\"sugerencia_fondo\": {{\n",
        "    \"analisis_sujeto\": {{\n",
        "      \"tono_piel\": \"<c√°lido/neutro/fr√≠o con subtono>\",\n",
        "      \"color_cabello\": \"<descripci√≥n del color>\",\n",
        "      \"color_ropa_dominante\": \"<color y tono>\",\n",
        "      \"colores_a_evitar\": [\"<color que competir√≠a>\", \"<otro>\"]\n",
        "    }},\n",
        "    \"fondos_recomendados\": [\n",
        "      {{\n",
        "        \"color\": \"<nombre del color>\",\n",
        "        \"hex\": \"<#XXXXXX>\",\n",
        "        \"teoria_color\": \"<complementario/an√°logo/tri√°dico/neutro>\",\n",
        "        \"por_que_funciona\": \"<explicaci√≥n espec√≠fica para este sujeto>\",\n",
        "        \"impacto_segmentacion\": \"<efecto en Boundary IoU y contraste>\"\n",
        "      }},\n",
        "      {{\n",
        "        \"color\": \"<segunda opci√≥n>\",\n",
        "        \"hex\": \"<#XXXXXX>\",\n",
        "        \"teoria_color\": \"<tipo>\",\n",
        "        \"por_que_funciona\": \"<explicaci√≥n>\",\n",
        "        \"impacto_segmentacion\": \"<efecto esperado>\"\n",
        "      }}\n",
        "    ],\n",
        "    \"fondo_actual\": {{\n",
        "      \"descripcion\": \"<qu√© ves en el fondo de la imagen>\",\n",
        "      \"valoracion\": \"<funciona bien/mejorable/problem√°tico>\",\n",
        "      \"problema_si_existe\": \"<explicaci√≥n con dato de m√©trica>\"\n",
        "    }}\n",
        "  }}\n",
        "}}\n",
        "\n",
        "REGLAS CR√çTICAS:\n",
        "1. NO des consejos gen√©ricos. Cada recomendaci√≥n DEBE citar un dato espec√≠fico.\n",
        "2. Los valores recomendados deben ser N√öMEROS CONCRETOS (no \"aumenta\", sino \"sube a f/2.8\").\n",
        "3. El impacto en m√©tricas debe ser cuantificado (no \"mejorar√°\", sino \"aumentar√≠a ~10-15%\").\n",
        "4. M√°ximo 3 recomendaciones, ordenadas por prioridad.\n",
        "5. Responde SOLO con el JSON, sin texto adicional.\"\"\""
      ],
      "metadata": {
        "id": "0-0d2zfgF3Ba"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EXTRACCI√ìN DE CONTEXTO ENRIQUECIDO\n",
        "# ==============================================================================\n",
        "\n",
        "def obtener_contexto_enriquecido(df_metricas: pd.DataFrame,\n",
        "                                   codigo_foto: str,\n",
        "                                   config_modelo: str = None) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extrae contexto completo para el prompt VLM incluyendo m√©tricas de modelo.\n",
        "\n",
        "    Args:\n",
        "        df_metricas: DataFrame con m√©tricas fusionadas\n",
        "        codigo_foto: Identificador de la foto\n",
        "        config_modelo: Configuraci√≥n espec√≠fica del modelo (opcional)\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con todo el contexto o None si no encuentra datos\n",
        "    \"\"\"\n",
        "    df_foto = df_metricas[df_metricas['codigo_foto'] == codigo_foto]\n",
        "\n",
        "    if df_foto.empty:\n",
        "        return None\n",
        "\n",
        "    # Si hay config espec√≠fica, filtrar por ella\n",
        "    if config_modelo:\n",
        "        df_config = df_foto[df_foto['config_codigo'] == config_modelo]\n",
        "        if not df_config.empty:\n",
        "            fila = df_config.iloc[0]\n",
        "        else:\n",
        "            fila = df_foto.iloc[0]\n",
        "    else:\n",
        "        # Tomar la primera fila (los datos EXIF son iguales para todas las configs)\n",
        "        fila = df_foto.iloc[0]\n",
        "\n",
        "    def safe_get(key, default=0):\n",
        "        \"\"\"Obtiene valor de forma segura, convirtiendo numpy types.\"\"\"\n",
        "        val = fila.get(key, default)\n",
        "        if pd.isna(val):\n",
        "            return default\n",
        "        if isinstance(val, (np.floating, np.integer)):\n",
        "            return float(val)\n",
        "        return val\n",
        "\n",
        "    return {\n",
        "        # === EXIF / Par√°metros de captura ===\n",
        "        \"modelo_camara\": safe_get('exif_modelo_camara', 'No disponible'),\n",
        "        \"apertura\": safe_get('exif_apertura', 'N/A'),\n",
        "        \"iso\": safe_get('exif_iso', 'N/A'),\n",
        "        \"tiempo_exposicion\": safe_get('exif_exposicion_seg', 'N/A'),\n",
        "        \"distancia_focal\": safe_get('exif_focal', 'N/A'),\n",
        "        \"ancho\": safe_get('meta_ancho', 0),\n",
        "        \"alto\": safe_get('meta_alto', 0),\n",
        "\n",
        "        # === M√©tricas de calidad t√©cnica ===\n",
        "        \"nitidez\": safe_get('calidad_nitidez_laplacian', 0),\n",
        "        \"snr_db\": safe_get('calidad_snr_db', 0),\n",
        "        \"contraste\": safe_get('calidad_contraste_rms', 0),\n",
        "        \"saturacion\": safe_get('color_hsv_sat_mean', 0),\n",
        "        \"brillo\": safe_get('calidad_brillo_medio', 0),\n",
        "\n",
        "        # === Composici√≥n y saliencia ===\n",
        "        \"centroide_x\": safe_get('centroide_x', 0.5),\n",
        "        \"centroide_y\": safe_get('centroide_y', 0.5),\n",
        "        \"espacio_negativo\": safe_get('espacio_negativo', 0),\n",
        "        \"sal_centroide_x\": safe_get('sal_centroide_x', 0.5),\n",
        "        \"sal_centroide_y\": safe_get('sal_centroide_y', 0.5),\n",
        "        \"sal_ratio\": safe_get('sal_ratio_centro_periferia', 1.0),\n",
        "\n",
        "        # === Resultados del modelo de segmentaci√≥n ===\n",
        "        \"iou\": safe_get('iou', 0),\n",
        "        \"boundary_iou\": safe_get('boundary_iou', 0),\n",
        "        \"precision\": safe_get('precision', 0),\n",
        "        \"recall\": safe_get('recall', 0),\n",
        "        \"solidity\": safe_get('solidity', 0),\n",
        "        \"recorte_bordes\": safe_get('recorte_bordes_porcentaje', 0),\n",
        "        \"contraste_interior\": safe_get('haralick_interior_contrast', 0),\n",
        "        \"contraste_borde\": safe_get('haralick_borde_contrast', 0),\n",
        "    }\n",
        "\n",
        "def generar_prompt_mentor(contexto: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Genera el prompt completo con el contexto de la foto.\n",
        "\n",
        "    Args:\n",
        "        contexto: Diccionario con todos los datos de contexto\n",
        "\n",
        "    Returns:\n",
        "        Prompt formateado listo para enviar al VLM\n",
        "    \"\"\"\n",
        "    return PROMPT_MENTOR_FOTOGRAFIA.format(**contexto)"
      ],
      "metadata": {
        "id": "1Wn--p5zF6_g"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FUNCIONES DE IMAGEN Y M√ÅSCARA\n",
        "# ==============================================================================\n",
        "\n",
        "def cargar_mascara(ruta_npz: Path) -> np.ndarray:\n",
        "    \"\"\"Carga m√°scara de segmentaci√≥n desde archivo NPZ.\"\"\"\n",
        "    datos = np.load(ruta_npz, allow_pickle=True)\n",
        "\n",
        "    claves_posibles = ['mascara', 'mask', 'masks', 'arr_0']\n",
        "\n",
        "    for clave in claves_posibles:\n",
        "        if clave in datos:\n",
        "            mascara = datos[clave]\n",
        "            if mascara.ndim == 0:\n",
        "                mascara = mascara.item()\n",
        "            return mascara\n",
        "\n",
        "    primera_clave = list(datos.keys())[0]\n",
        "    return datos[primera_clave]\n",
        "\n",
        "\n",
        "def redimensionar_mascara(mascara: np.ndarray,\n",
        "                          tama√±o_objetivo: tuple) -> np.ndarray:\n",
        "    \"\"\"Redimensiona m√°scara al tama√±o de la imagen original.\"\"\"\n",
        "    mascara_pil = Image.fromarray(mascara.astype(np.uint8) * 255)\n",
        "    mascara_redim = mascara_pil.resize(tama√±o_objetivo, Image.NEAREST)\n",
        "    return np.array(mascara_redim) > 127\n",
        "\n",
        "\n",
        "def crear_overlay_mascara(imagen: Image.Image,\n",
        "                          mascara: np.ndarray,\n",
        "                          color: tuple = (0, 255, 0),\n",
        "                          alpha: float = 0.4) -> Image.Image:\n",
        "    \"\"\"Crea overlay semitransparente de la m√°scara sobre la imagen.\"\"\"\n",
        "    if imagen.mode != 'RGB':\n",
        "        imagen = imagen.convert('RGB')\n",
        "\n",
        "    overlay = Image.new('RGB', imagen.size, color)\n",
        "    mascara_alpha = Image.fromarray((mascara * int(255 * alpha)).astype(np.uint8))\n",
        "\n",
        "    resultado = imagen.copy()\n",
        "    resultado.paste(overlay, mask=mascara_alpha)\n",
        "\n",
        "    return resultado\n",
        "\n",
        "\n",
        "def crear_imagen_compuesta(imagen_original: Image.Image,\n",
        "                           mascara: np.ndarray,\n",
        "                           max_ancho: int = 1024) -> Image.Image:\n",
        "    \"\"\"Crea imagen compuesta: original | overlay de m√°scara.\"\"\"\n",
        "    ratio = min(1.0, max_ancho / imagen_original.width)\n",
        "    if ratio < 1.0:\n",
        "        nuevo_tama√±o = (int(imagen_original.width * ratio),\n",
        "                        int(imagen_original.height * ratio))\n",
        "        imagen_original = imagen_original.resize(nuevo_tama√±o, Image.LANCZOS)\n",
        "        mascara = redimensionar_mascara(mascara, nuevo_tama√±o)\n",
        "\n",
        "    imagen_overlay = crear_overlay_mascara(imagen_original, mascara)\n",
        "\n",
        "    ancho_total = imagen_original.width * 2\n",
        "    alto = imagen_original.height\n",
        "\n",
        "    compuesta = Image.new('RGB', (ancho_total, alto))\n",
        "    compuesta.paste(imagen_original, (0, 0))\n",
        "    compuesta.paste(imagen_overlay, (imagen_original.width, 0))\n",
        "\n",
        "    return compuesta"
      ],
      "metadata": {
        "id": "qvsFytwGF-qC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# AN√ÅLISIS CON VLM\n",
        "# ==============================================================================\n",
        "\n",
        "def configurar_gemini(api_key: str) -> genai.GenerativeModel:\n",
        "    \"\"\"Configura la conexi√≥n con Gemini.\"\"\"\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    modelo = genai.GenerativeModel(\n",
        "        model_name=ConfigVLM.GEMINI_MODEL,\n",
        "        generation_config=ConfigVLM.GENERATION_CONFIG\n",
        "    )\n",
        "\n",
        "    print(f\"[OK] Modelo {ConfigVLM.GEMINI_MODEL} configurado\")\n",
        "    return modelo\n",
        "\n",
        "\n",
        "def analizar_fotografia(modelo: genai.GenerativeModel,\n",
        "                        imagen_compuesta: Image.Image,\n",
        "                        prompt: str,\n",
        "                        reintentos: int = 2) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Env√≠a imagen al VLM y obtiene an√°lisis estructurado.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo Gemini configurado\n",
        "        imagen_compuesta: Imagen PIL (original + overlay)\n",
        "        prompt: Prompt con contexto t√©cnico\n",
        "        reintentos: N√∫mero de reintentos en caso de error\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con an√°lisis o error\n",
        "    \"\"\"\n",
        "    for intento in range(reintentos + 1):\n",
        "        try:\n",
        "            respuesta = modelo.generate_content([prompt, imagen_compuesta])\n",
        "\n",
        "            texto = respuesta.text.strip()\n",
        "\n",
        "            # Limpiar marcadores de c√≥digo\n",
        "            if texto.startswith(\"```json\"):\n",
        "                texto = texto[7:]\n",
        "            if texto.startswith(\"```\"):\n",
        "                texto = texto[3:]\n",
        "            if texto.endswith(\"```\"):\n",
        "                texto = texto[:-3]\n",
        "            texto = texto.strip()\n",
        "\n",
        "            analisis = json.loads(texto)\n",
        "            analisis[\"_metadata\"] = {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"modelo_vlm\": ConfigVLM.GEMINI_MODEL,\n",
        "                \"estado\": \"exito\",\n",
        "                \"intento\": intento + 1\n",
        "            }\n",
        "\n",
        "            return analisis\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            if intento < reintentos:\n",
        "                print(f\"       [Reintento {intento + 2}/{reintentos + 1}] Error JSON, reintentando...\")\n",
        "                continue\n",
        "            return {\n",
        "                \"_metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"estado\": \"error_json\",\n",
        "                    \"error\": str(e),\n",
        "                    \"respuesta_raw\": texto if 'texto' in locals() else \"N/A\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            if intento < reintentos:\n",
        "                print(f\"       [Reintento {intento + 2}/{reintentos + 1}] Error, reintentando...\")\n",
        "                continue\n",
        "            return {\n",
        "                \"_metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"estado\": \"error\",\n",
        "                    \"error\": str(e)\n",
        "                }\n",
        "            }\n"
      ],
      "metadata": {
        "id": "Xxqo_3uEGC5w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PIPELINE COMPLETO\n",
        "# ==============================================================================\n",
        "\n",
        "def cargar_indice_maestro(ruta: Path) -> Dict:\n",
        "    \"\"\"Carga el √≠ndice maestro con referencias a todos los recursos.\"\"\"\n",
        "    with open(ruta, 'r', encoding='utf-8') as f:\n",
        "        indice = json.load(f)\n",
        "\n",
        "    fotos = {k: v for k, v in indice.items() if isinstance(v, dict) and 'foto_id' in v}\n",
        "\n",
        "    print(f\"[OK] √çndice maestro cargado: {len(fotos)} fotograf√≠as\")\n",
        "    return fotos\n",
        "\n",
        "\n",
        "def analizar_foto_completa(modelo_vlm: genai.GenerativeModel,\n",
        "                           indice: Dict,\n",
        "                           df_metricas: pd.DataFrame,\n",
        "                           codigo_foto: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Pipeline completo de an√°lisis para una foto.\n",
        "\n",
        "    Args:\n",
        "        modelo_vlm: Modelo Gemini configurado\n",
        "        indice: √çndice maestro\n",
        "        df_metricas: DataFrame con m√©tricas\n",
        "        codigo_foto: C√≥digo de la foto a analizar\n",
        "\n",
        "    Returns:\n",
        "        Resultado del an√°lisis\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANALIZANDO: {codigo_foto}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Verificar que existe la foto\n",
        "    if codigo_foto not in indice:\n",
        "        print(f\"[ERROR] Foto {codigo_foto} no encontrada en √≠ndice\")\n",
        "        return None\n",
        "\n",
        "    datos_foto = indice[codigo_foto]\n",
        "\n",
        "    # [1] Cargar imagen\n",
        "    print(\"[1/5] Cargando imagen...\")\n",
        "    ruta_imagen = Path(datos_foto.get(\"rutas\", {}).get(\"imagen\", \"\"))\n",
        "    if not ruta_imagen.exists():\n",
        "        ruta_imagen = ConfigVLM.FOTOS_EDITADAS / f\"{codigo_foto}.jpg\"\n",
        "\n",
        "    if not ruta_imagen.exists():\n",
        "        print(f\"[ERROR] No se encuentra imagen: {ruta_imagen}\")\n",
        "        return None\n",
        "\n",
        "    imagen = Image.open(ruta_imagen)\n",
        "    print(f\"      Tama√±o: {imagen.size}\")\n",
        "\n",
        "    # [2] Buscar m√°scara v√°lida (evitar instance y t0.85)\n",
        "    print(\"[2/5] Cargando m√°scara...\")\n",
        "    modelos_disponibles = datos_foto.get(\"modelos_disponibles\", {})\n",
        "\n",
        "    mascara = None\n",
        "    config_usada = None\n",
        "\n",
        "    for config_nombre, config_datos in modelos_disponibles.items():\n",
        "        if \"oneformer\" in config_nombre.lower():\n",
        "            if \"instance\" in config_nombre.lower():\n",
        "                continue\n",
        "            if \"t0.85\" in config_nombre or \"t085\" in config_nombre:\n",
        "                continue\n",
        "\n",
        "            ruta_mascara = Path(config_datos.get(\"ruta_mascara\", \"\"))\n",
        "            if ruta_mascara.exists():\n",
        "                mascara = cargar_mascara(ruta_mascara)\n",
        "                config_usada = config_nombre\n",
        "                print(f\"      Config: {config_nombre}\")\n",
        "                break\n",
        "\n",
        "    if mascara is None:\n",
        "        print(\"[ERROR] No se encontr√≥ m√°scara v√°lida\")\n",
        "        return None\n",
        "\n",
        "    # [3] Crear imagen compuesta\n",
        "    print(\"[3/5] Generando imagen compuesta...\")\n",
        "    mascara_redim = redimensionar_mascara(mascara, imagen.size)\n",
        "    imagen_compuesta = crear_imagen_compuesta(imagen, mascara_redim)\n",
        "\n",
        "    ruta_compuesta = ConfigVLM.OUTPUT_IMAGENES / f\"{codigo_foto}_compuesta.jpg\"\n",
        "    imagen_compuesta.save(ruta_compuesta, quality=90)\n",
        "    print(f\"      Guardada: {ruta_compuesta.name}\")\n",
        "\n",
        "    # [4] Obtener contexto enriquecido\n",
        "    print(\"[4/5] Extrayendo contexto...\")\n",
        "    contexto = obtener_contexto_enriquecido(df_metricas, codigo_foto, config_usada)\n",
        "\n",
        "    if contexto is None:\n",
        "        print(\"[ERROR] No se pudo extraer contexto\")\n",
        "        return None\n",
        "\n",
        "    print(f\"      IoU: {contexto['iou']:.1%}, Nitidez: {contexto['nitidez']:.1f}\")\n",
        "\n",
        "    # [5] Enviar a VLM\n",
        "    print(\"[5/5] Consultando al mentor VLM...\")\n",
        "    prompt = generar_prompt_mentor(contexto)\n",
        "    resultado = analizar_fotografia(modelo_vlm, imagen_compuesta, prompt)\n",
        "\n",
        "    # A√±adir metadata\n",
        "    resultado[\"_foto\"] = {\n",
        "        \"codigo_foto\": codigo_foto,\n",
        "        \"config_mascara\": config_usada,\n",
        "        \"contexto\": contexto\n",
        "    }\n",
        "\n",
        "    # Guardar resultado\n",
        "    estado = resultado.get(\"_metadata\", {}).get(\"estado\", \"desconocido\")\n",
        "    print(f\"      Estado: {estado}\")\n",
        "\n",
        "    if estado == \"exito\":\n",
        "        ruta_resultado = ConfigVLM.OUTPUT_RESPUESTAS / f\"{codigo_foto}_mentor.json\"\n",
        "        with open(ruta_resultado, 'w', encoding='utf-8') as f:\n",
        "            json.dump(resultado, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"      Guardado: {ruta_resultado.name}\")\n",
        "\n",
        "    return resultado"
      ],
      "metadata": {
        "id": "1k0EvbOeGGta"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6Hl8vPtBC9qK"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# EJECUCI√ìN PRINCIPAL\n",
        "# ==============================================================================\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def mostrar_resumen_resultado(resultado: Dict[str, Any], codigo_foto: str) -> None:\n",
        "    \"\"\"Muestra resumen legible del resultado del an√°lisis.\"\"\"\n",
        "    if resultado and resultado.get(\"_metadata\", {}).get(\"estado\") == \"exito\":\n",
        "        print(f\"\\n--- FEEDBACK PARA {codigo_foto} ---\")\n",
        "\n",
        "        # Evaluaci√≥n de edici√≥n\n",
        "        if \"evaluacion_edicion\" in resultado:\n",
        "            print(f\"\\n  [EDICI√ìN] {resultado['evaluacion_edicion'].get('resumen', 'N/A')}\")\n",
        "\n",
        "        # Fortalezas\n",
        "        if \"fortalezas\" in resultado and resultado[\"fortalezas\"]:\n",
        "            print(f\"\\n  [+] FORTALEZAS:\")\n",
        "            for f in resultado[\"fortalezas\"][:2]:  # M√°ximo 2\n",
        "                print(f\"      - {f.get('aspecto', 'N/A')}: {f.get('dato_que_lo_demuestra', '')}\")\n",
        "\n",
        "        # Recomendaciones\n",
        "        if \"recomendaciones\" in resultado and resultado[\"recomendaciones\"]:\n",
        "            print(f\"\\n  [!] RECOMENDACIONES:\")\n",
        "            for r in resultado[\"recomendaciones\"]:\n",
        "                print(f\"      {r.get('prioridad', '?')}. {r.get('problema_detectado', 'N/A')}\")\n",
        "                print(f\"         ‚Üí {r.get('ajuste_recomendado', 'N/A')}\")\n",
        "                impacto = r.get('impacto_esperado', {})\n",
        "                if impacto.get('en_metricas'):\n",
        "                    print(f\"         üìà {impacto['en_metricas']}\")\n",
        "\n",
        "      # Sugerencia de fondo\n",
        "        if \"sugerencia_fondo\" in resultado:\n",
        "            sf = resultado['sugerencia_fondo']\n",
        "            print(f\"\\n  [üé®] FONDO ACTUAL: {sf.get('fondo_actual', {}).get('valoracion', 'N/A')}\")\n",
        "            if sf.get('fondos_recomendados'):\n",
        "                print(f\"      RECOMENDADOS:\")\n",
        "                for fondo in sf['fondos_recomendados'][:2]:\n",
        "                    print(f\"        - {fondo.get('color', 'N/A')} ({fondo.get('hex', '')}) - {fondo.get('teoria_color', '')}\")\n",
        "    else:\n",
        "        error = resultado.get(\"_metadata\", {}).get(\"error\", \"Error desconocido\")\n",
        "        print(f\"\\n--- ERROR EN {codigo_foto}: {error} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "API_KEY = \"AIzaSyB4lzypsEDDsCMJ2tgnECQHC6rDZ-KIYy0\"\n",
        "\n",
        "# Modo TEST (1 foto)\n",
        "resultado = main(API_KEY, modo=\"test\")\n",
        "\n",
        "# Modo TEST con foto espec√≠fica\n",
        "#resultado = main(API_KEY, modo=\"test\", foto_especifica=\"_DSC0023\")\n",
        "\n",
        "# Modo COMPLETO (20 fotos, 60s entre cada una)\n",
        "#resultados = main(API_KEY, modo=\"completo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RpzaAUvsGQWF",
        "outputId": "99984804-22ce-4908-87ab-7d5addcdf9f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "FASE VLM - MENTOR DE FOTOGRAF√çA\n",
            "======================================================================\n",
            "Timestamp: 2025-12-14T23:56:02.365496\n",
            "Modo: TEST\n",
            "======================================================================\n",
            "[OK] Modelo gemini-2.5-flash configurado\n",
            "\n",
            "Cargando datos...\n",
            "[OK] √çndice maestro cargado: 20 fotograf√≠as\n",
            "[OK] M√©tricas cargadas: 2360 registros\n",
            "\n",
            "[MODO TEST] Procesando 1 foto: _DSC0023\n",
            "\n",
            "======================================================================\n",
            "[1/1] Procesando: _DSC0023\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "ANALIZANDO: _DSC0023\n",
            "============================================================\n",
            "[1/5] Cargando imagen...\n",
            "      Tama√±o: (4000, 6000)\n",
            "[2/5] Cargando m√°scara...\n",
            "      Config: oneformer_ade20k_tiny_panoptic_t040\n",
            "[3/5] Generando imagen compuesta...\n",
            "      Guardada: _DSC0023_compuesta.jpg\n",
            "[4/5] Extrayendo contexto...\n",
            "      IoU: 95.1%, Nitidez: 29.7\n",
            "[5/5] Consultando al mentor VLM...\n",
            "      Estado: exito\n",
            "      Guardado: _DSC0023_mentor.json\n",
            "\n",
            "--- FEEDBACK PARA _DSC0023 ---\n",
            "\n",
            "  [EDICI√ìN] Las decisiones de edici√≥n han corregido eficazmente la subexposici√≥n inicial y mejorado la nitidez y el contraste para presentar un retrato vibrante y bien definido.\n",
            "\n",
            "  [+] FORTALEZAS:\n",
            "      - Enfoque del sujeto y composici√≥n: Concentraci√≥n atenci√≥n: 1.66 (>1 = centrada en sujeto)\n",
            "      - Separaci√≥n general del sujeto: IoU: 95.1% (>90% excelente), Precision: 99.8%\n",
            "\n",
            "  [!] RECOMENDACIONES:\n",
            "      1. Deficiencia cr√≠tica en la precisi√≥n de los bordes del sujeto, especialmente en detalles finos como el cabello.\n",
            "         ‚Üí Implementar t√©cnicas de enmascaramiento m√°s avanzadas o refinamiento de selecci√≥n de bordes en post-producci√≥n. Utilizar herramientas espec√≠ficas para cabello o m√°scaras de luminosidad/color para mejorar la transici√≥n del sujeto al fondo.\n",
            "         üìà Aumentar√≠a el Boundary IoU a un rango de 70-85%, mejorando la solidez de la silueta en al menos un 10-15%.\n",
            "      2. Subexposici√≥n inicial en la captura RAW, limitando el margen de edici√≥n para las luces y sombras.\n",
            "         ‚Üí Aumentar la exposici√≥n en la c√°mara durante la captura para acercar el brillo del RAW al rango √≥ptimo. Esto podr√≠a lograrse ajustando la velocidad de obturaci√≥n o la iluminaci√≥n.\n",
            "         üìà El Brillo RAW aumentar√≠a a 120-130/255, y el SNR podr√≠a mejorar ligeramente al reducir la necesidad de levantar sombras digitalmente.\n",
            "      3. Nitidez en el RAW ligeramente por debajo del umbral √≥ptimo, lo que requiere mayor esfuerzo de post-procesado.\n",
            "         ‚Üí Asegurar un enfoque m√°s preciso en la captura, posiblemente utilizando el enfoque puntual en los ojos. Si la profundidad de campo lo permite sin sacrificar el bokeh deseado, considerar una apertura ligeramente m√°s cerrada.\n",
            "         üìà La Nitidez Laplacian RAW aumentar√≠a a 32-35, proporcionando una base m√°s s√≥lida para la nitidez final.\n",
            "\n",
            "  [üé®] FONDO ACTUAL: mejorable\n",
            "      RECOMENDADOS:\n",
            "        - Beige C√°lido (#E0D8D0) - An√°logo (neutro c√°lido)\n",
            "        - Gris Azulado Suave (#A8B4C0) - An√°logo (fr√≠o, complementario al tono c√°lido de piel/cabello)\n",
            "\n",
            "======================================================================\n",
            "RESUMEN FINAL\n",
            "======================================================================\n",
            "Total procesadas: 1\n",
            "Exitosas: 1\n",
            "Errores: 0\n",
            "Resultados guardados en: /content/drive/MyDrive/TFM/3_Analisis/fase_vlm/respuestas_vlm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "7RzpYn4VGxdm"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}