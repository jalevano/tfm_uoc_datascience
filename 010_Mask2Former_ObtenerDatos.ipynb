{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsOClwVF6QX+BtKUwKqeER",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/010_Mask2Former_ObtenerDatos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BavQ4XeSfGoB",
        "outputId": "1c138166-a867-443f-fe3f-cf81492f38c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "### Importación de librerías estándar y especializadas para procesamiento de imágenes, manejo de datos y visualización ###\n",
        "\n",
        "### Librería principal para computación con tensores y modelos de deep learning (PyTorch)\n",
        "import torch\n",
        "### Operaciones matemáticas y manejo de arrays multidimensionales\n",
        "import numpy as np\n",
        "### OpenCV: procesamiento de imágenes y video\n",
        "import cv2\n",
        "### Manejo de datos en formato JSON\n",
        "import json\n",
        "### Funciones relacionadas con el tiempo (ej. timestamps, delays)\n",
        "import time\n",
        "### Interacción con el sistema operativo (rutas, archivos, etc.)\n",
        "import os\n",
        "### Manejo de fechas y horas con precisión\n",
        "from datetime import datetime\n",
        "### Manejo de rutas de archivos de forma más robusta que con strings\n",
        "from pathlib import Path\n",
        "### Generación de hashes (útil para verificar integridad o crear IDs únicos)\n",
        "import hashlib\n",
        "### Decorador para crear clases de datos de forma concisa\n",
        "from dataclasses import dataclass\n",
        "### Tipado estático para mejorar legibilidad y mantenimiento\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "### Importación de modelos y procesadores de imágenes desde Hugging Face Transformers ###\n",
        "from transformers import AutoImageProcessor, AutoModelForUniversalSegmentation\n",
        "### AutoImageProcessor: preprocesamiento automático de imágenes para modelos específicos\n",
        "### AutoModelForUniversalSegmentation: modelo para segmentación semántica de imágenes\n",
        "\n",
        "### Librerías adicionales para manejo de imágenes y visualización ###\n",
        "\n",
        "### PIL: manipulación de imágenes (abrir, convertir, analizar estadísticas)\n",
        "from PIL import Image, ImageStat\n",
        "### Visualización de datos e imágenes\n",
        "import matplotlib.pyplot as plt\n",
        "### Barra de progreso para loops largos (mejora experiencia visual en ejecución)\n",
        "from tqdm import tqdm\n",
        "\n",
        "### Montaje de Google Drive en entorno Colab para acceder a archivos almacenados en la nube ###\n",
        "from google.colab import drive\n",
        "### Solicita autorización para montar Google Drive en la ruta especificada\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Clase de configuración centralizada para definir rutas, parámetros y modelos utilizados en el proyecto ###\n",
        "@dataclass\n",
        "class Config:\n",
        "\n",
        "    ### Ruta al conjunto de datos de entrada (imágenes a procesar) ###\n",
        "    DATASET_PATH: Path = Path(\"/content/drive/MyDrive/TFM/mask2former/imagenes\")\n",
        "\n",
        "    ### Ruta donde se guardarán los resultados del procesamiento (segmentaciones, métricas, etc.) ###\n",
        "    OUTPUT_PATH: Path = Path(\"/content/drive/MyDrive/TFM/mask2former/resultados\")\n",
        "\n",
        "    ### Diccionario de umbrales predefinidos para filtrar o evaluar resultados de segmentación.\n",
        "    ### Cada clave representa un perfil de sensibilidad:\n",
        "    ### - 'ultra': extremadamente sensible (detecta incluso mínimos cambios)\n",
        "    ### - 'agresivo': sensibilidad alta\n",
        "    ### - 'normal': sensibilidad media\n",
        "    ### - 'conservador': sensibilidad baja (solo cambios significativos)\n",
        "    UMBRALES = {\n",
        "        'ultra': [0.0001, 0.001, 0.01, 0.1],\n",
        "        'agresivo': [0.001, 0.01, 0.05, 0.1, 0.3],\n",
        "        'normal': [0.01, 0.1, 0.3, 0.5],\n",
        "        'conservador': [0.3, 0.5, 0.7]\n",
        "    }\n",
        "\n",
        "    ### Lista de modelos de segmentación disponibles.\n",
        "    ### Cada modelo tiene diferentes arquitecturas y está entrenado en distintos datasets:\n",
        "    ### - COCO Instance: segmentación de objetos individuales\n",
        "    ### - ADE Semantic: segmentación semántica (clases por píxel)\n",
        "    MODELOS = [\n",
        "        \"facebook/mask2former-swin-large-coco-instance\",\n",
        "        \"facebook/mask2former-swin-base-ade-semantic\",\n",
        "        \"facebook/mask2former-swin-small-coco-instance\"\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "_6tzVhPjk8NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Se crea una instancia de la clase Config para acceder a rutas, modelos y parámetros definidos ###\n",
        "config = Config()\n",
        "\n",
        "### Se asegura que la carpeta de salida exista; si no existe, se crea automáticamente.\n",
        "### El parámetro `exist_ok=True` evita errores si la carpeta ya está creada ###\n",
        "os.makedirs(config.OUTPUT_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFOEdEGzk9nU",
        "outputId": "50fd2d4e-03b6-45cc-ffd8-bba81e6b1dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Clase que agrupa funciones utilitarias reutilizables para el procesamiento de imágenes y manejo de archivos ###\n",
        "class Utils:\n",
        "\n",
        "    ### Carga todas las imágenes desde una ruta dada, buscando recursivamente archivos con extensiones válidas.\n",
        "    ### Parámetros:\n",
        "    ### - ruta (str): ruta base donde buscar imágenes\n",
        "    ### Retorna:\n",
        "    ### - Lista de rutas (str) de imágenes encontradas\n",
        "    @staticmethod\n",
        "    def cargar_imagenes(ruta: str) -> List[str]:\n",
        "        path = Path(ruta)\n",
        "        extensiones = (\".jpg\", \".png\", \".jpeg\")\n",
        "        imagenes = [str(p) for p in path.glob(\"**/*\") if p.suffix.lower() in extensiones]\n",
        "        print(f\"Encontradas {len(imagenes)} imágenes\")\n",
        "        return imagenes\n",
        "\n",
        "    ### Abre una imagen desde disco, la convierte a RGB y la redimensiona si excede el tamaño máximo.\n",
        "    ### Parámetros:\n",
        "    ### - ruta (str): ruta del archivo de imagen\n",
        "    ### - max_size (int): tamaño máximo permitido para ancho/alto (por defecto 1024)\n",
        "    ### Retorna:\n",
        "    ### - Imagen PIL preparada para procesamiento\n",
        "    @staticmethod\n",
        "    def preparar_imagen(ruta: str, max_size: int = 1024) -> Image.Image:\n",
        "        img = Image.open(ruta).convert(\"RGB\")\n",
        "        if max(img.size) > max_size:\n",
        "            img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "        return img\n",
        "\n",
        "    ### Calcula un hash MD5 de los contenidos binarios del archivo.\n",
        "    ### Útil para identificar imágenes de forma única o evitar duplicados.\n",
        "    ### Parámetros:\n",
        "    ### - ruta (str): ruta del archivo\n",
        "    ### Retorna:\n",
        "    ### - Cadena con los primeros 8 caracteres del hash MD5\n",
        "    @staticmethod\n",
        "    def calcular_hash(ruta: str) -> str:\n",
        "        return hashlib.md5(open(ruta, 'rb').read()).hexdigest()[:8]\n",
        "\n",
        "    ### Guarda un objeto Python (lista, diccionario, etc.) en un archivo JSON.\n",
        "    ### Parámetros:\n",
        "    ### - datos (Any): objeto a guardar\n",
        "    ### - archivo (str): ruta del archivo destino\n",
        "    ### No retorna nada, pero imprime confirmación en consola.\n",
        "    @staticmethod\n",
        "    def guardar_json(datos: Any, archivo: str) -> None:\n",
        "        with open(archivo, 'w', encoding='utf-8') as f:\n",
        "            json.dump(datos, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Guardado: {archivo}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWVk3grDliyZ",
        "outputId": "7d4413d3-687f-432c-ab08-fbfe8c23e326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilidades definidas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Clase DetectorPersonas\n",
        "### Esta clase encapsula la lógica para detectar personas en imágenes utilizando modelos de segmentación universal\n",
        "### de Hugging Face. Es compatible con modelos de segmentación semántica (por píxel) e instancia (por objeto).\n",
        "###\n",
        "### Funcionalidades principales:\n",
        "### - Carga automática del modelo y procesador desde Hugging Face\n",
        "### - Identificación dinámica de la clase \"persona\"\n",
        "### - Inferencia sobre imágenes PIL\n",
        "### - Post-procesamiento adaptativo según el tipo de modelo\n",
        "### - Evaluación por múltiples umbrales de sensibilidad\n",
        "### - Liberación de memoria para entornos con GPU limitada\n",
        "class DetectorPersonas:\n",
        "    ### Constructor de la clase\n",
        "    ### Parámetros:\n",
        "    ### - modelo_name (str): nombre del modelo Hugging Face a utilizar. Debe ser compatible con segmentación universal.\n",
        "    def __init__(self, modelo_name: str):\n",
        "        ### Guarda el nombre del modelo para referencia interna\n",
        "        self.modelo_name = modelo_name\n",
        "\n",
        "        ### Selección automática del dispositivo: GPU si está disponible, de lo contrario CPU\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        print(f\"###### Cargando: {modelo_name}\")\n",
        "\n",
        "        ### Carga el procesador de imágenes asociado al modelo desde Hugging Face\n",
        "        ### Este objeto se encarga de convertir imágenes PIL en tensores listos para el modelo\n",
        "        self.processor = AutoImageProcessor.from_pretrained(modelo_name)\n",
        "\n",
        "        ### Carga el modelo de segmentación universal desde Hugging Face\n",
        "        ### Puede ser de tipo semántico (por píxel) o de instancia (por objeto)\n",
        "        self.model = AutoModelForUniversalSegmentation.from_pretrained(modelo_name)\n",
        "\n",
        "        ### Mueve el modelo al dispositivo seleccionado y lo pone en modo evaluación\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        ### Determina si el modelo es de segmentación semántica\n",
        "        ### Se basa en la presencia de palabras clave como 'ade' o 'semantic' en el nombre del modelo\n",
        "        self.es_semantico = ('ade' in modelo_name.lower() or 'semantic' in modelo_name.lower())\n",
        "\n",
        "        ### Si el modelo tiene un diccionario de clases (id2label), se intenta identificar la clase \"persona\"\n",
        "        if hasattr(self.model.config, 'id2label'):\n",
        "            self.id2label = self.model.config.id2label\n",
        "            print(f\"Clase 0: {self.id2label.get(0, 'DESCONOCIDA')}\")\n",
        "\n",
        "            ### Búsqueda de la clase \"persona\" en el diccionario de etiquetas\n",
        "            ### Se asigna el ID correspondiente a la clase que contenga 'person' o 'people'\n",
        "            self.clase_persona = 0\n",
        "            for clase_id, nombre in self.id2label.items():\n",
        "                if 'person' in nombre.lower() or 'people' in nombre.lower():\n",
        "                    self.clase_persona = clase_id\n",
        "                    print(f\"Clase persona encontrada: {clase_id} = {nombre}\")\n",
        "                    break\n",
        "        else:\n",
        "            ### Si no hay diccionario de clases, se asume por defecto que la clase \"persona\" es la 0\n",
        "            self.clase_persona = 0\n",
        "\n",
        "        print(f\"Modelo cargado en {self.device} (semántico: {self.es_semantico})\")\n",
        "\n",
        "    ### Método detectar_en_imagen\n",
        "    ### Realiza inferencia sobre una imagen PIL y aplica post-procesamiento según el tipo de modelo\n",
        "    ###\n",
        "    ### Parámetros:\n",
        "    ### - imagen (PIL.Image): imagen a procesar. Debe estar en formato RGB y con tamaño adecuado.\n",
        "    ### - umbrales (List[float]): lista de umbrales de sensibilidad. Cada umbral representa el mínimo requerido\n",
        "    ###   para considerar que hay presencia significativa de personas. En modelos semánticos se interpreta como\n",
        "    ###   porcentaje de píxeles, en modelos de instancia como score mínimo de confianza.\n",
        "    ###\n",
        "    ### Retorna:\n",
        "    ### - Dict con resultados por umbral. Cada entrada incluye:\n",
        "    ###   - personas: número de personas detectadas\n",
        "    ###   - total: número total de clases o detecciones\n",
        "    ###   - scores_personas: lista de scores de confianza para detecciones de personas\n",
        "    ###   - todas_clases: lista de IDs de clases detectadas\n",
        "    ###   - todos_scores: lista de scores de todas las detecciones\n",
        "    ###   - max_score: score más alto entre todas las detecciones\n",
        "    ###   - En modelos semánticos: porcentaje de píxeles de clase persona, total de clases únicas\n",
        "    def detectar_en_imagen(self, imagen: Image.Image, umbrales: List[float]) -> Dict:\n",
        "        ### Obtiene dimensiones de la imagen (orden correcto: width, height)\n",
        "        w, h = imagen.size\n",
        "\n",
        "        ### Preprocesa la imagen y la convierte en tensores para el modelo\n",
        "        inputs = self.processor(images=imagen, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        ### Ejecuta la inferencia sin cálculo de gradientes y mide el tiempo de ejecución\n",
        "        inicio = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        tiempo_ms = (time.time() - inicio) * 1000\n",
        "\n",
        "        ### Diccionario para almacenar resultados por umbral\n",
        "        resultados = {'tiempo_inferencia_ms': tiempo_ms}\n",
        "\n",
        "        ### Post-procesamiento para modelos semánticos\n",
        "        if self.es_semantico:\n",
        "            try:\n",
        "                ### Obtiene la máscara semántica con clases por píxel\n",
        "                ### target_sizes debe estar en formato (height, width)\n",
        "                resultado_semantico = self.processor.post_process_semantic_segmentation(\n",
        "                    outputs, target_sizes=[(h, w)]\n",
        "                )[0]\n",
        "\n",
        "                ### Extrae clases únicas presentes en la imagen\n",
        "                unique_classes = torch.unique(resultado_semantico)\n",
        "\n",
        "                ### Para cada umbral, se calcula si hay presencia significativa de la clase \"persona\"\n",
        "                for umbral in umbrales:\n",
        "                    ### Crea una máscara booleana donde los píxeles coinciden con la clase \"persona\"\n",
        "                    persona_mask = (resultado_semantico == self.clase_persona)\n",
        "\n",
        "                    ### Cuenta el número de píxeles de clase \"persona\"\n",
        "                    persona_pixels = persona_mask.sum().item()\n",
        "                    total_pixels = resultado_semantico.numel()\n",
        "\n",
        "                    ### Calcula el porcentaje actual de píxeles de clase \"persona\"\n",
        "                    porcentaje_actual = (persona_pixels / total_pixels) * 100\n",
        "                    porcentaje_minimo = umbral * 100\n",
        "\n",
        "                    ### Se considera que hay personas si se supera el umbral y hay más de 50 píxeles\n",
        "                    personas = 1 if porcentaje_actual >= porcentaje_minimo and persona_pixels > 50 else 0\n",
        "\n",
        "                    resultados[f'umbral_{umbral}'] = {\n",
        "                        'personas': personas,\n",
        "                        'total': len(unique_classes),\n",
        "                        'todas_clases': unique_classes.tolist(),\n",
        "                        'persona_pixels': persona_pixels,\n",
        "                        'porcentaje_persona': porcentaje_actual,\n",
        "                        'scores_personas': [1.0] if personas > 0 else []\n",
        "                    }\n",
        "\n",
        "            ### En caso de error, se registra en el resultado por cada umbral\n",
        "            except Exception as e:\n",
        "                for umbral in umbrales:\n",
        "                    resultados[f'umbral_{umbral}'] = {\n",
        "                        'error': str(e), 'personas': 0, 'total': 0\n",
        "                    }\n",
        "\n",
        "        ### Post-procesamiento para modelos de instancia\n",
        "        else:\n",
        "            for umbral in umbrales:\n",
        "                try:\n",
        "                    ### Aplica post-procesamiento con el umbral de score especificado\n",
        "                    resultado = self.processor.post_process_instance_segmentation(\n",
        "                        outputs, target_sizes=[(h, w)], threshold=umbral\n",
        "                    )[0]\n",
        "\n",
        "                    ### Extrae etiquetas y scores de las detecciones\n",
        "                    labels = resultado.get(\"labels\", torch.tensor([]))\n",
        "                    scores = resultado.get(\"scores\", torch.tensor([]))\n",
        "\n",
        "                    ### Filtra detecciones que corresponden a la clase \"persona\"\n",
        "                    personas = sum(1 for l in labels if int(l.item()) == self.clase_persona)\n",
        "\n",
        "                    ### Extrae scores de las detecciones de personas\n",
        "                    scores_personas = [\n",
        "                        float(s.item()) for l, s in zip(labels, scores)\n",
        "                        if int(l.item()) == self.clase_persona\n",
        "                    ]\n",
        "\n",
        "                    ### Extrae todas las clases y scores detectados\n",
        "                    todas_clases = [int(l.item()) for l in labels]\n",
        "                    todos_scores = [float(s.item()) for s in scores]\n",
        "\n",
        "                    resultados[f'umbral_{umbral}'] = {\n",
        "                        'personas': personas,\n",
        "                        'total': len(labels),\n",
        "                        'scores_personas': scores_personas,\n",
        "                        'todas_clases': todas_clases,\n",
        "                        'todos_scores': todos_scores,\n",
        "                        'max_score': max(todos_scores) if todos_scores else 0.0\n",
        "                    }\n",
        "\n",
        "                ### En caso de error, se registra en el resultado por cada umbral\n",
        "                except Exception as e:\n",
        "                    resultados[f'umbral_{umbral}'] = {\n",
        "                        'error': str(e), 'personas': 0, 'total': 0\n",
        "                    }\n",
        "\n",
        "        return\n",
        "\n",
        "    ### Método para liberar memoria del modelo y procesador ###\n",
        "    def limpiar(self):\n",
        "        del self.model, self.processor\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScUUIIdwlytw",
        "outputId": "77c0f34b-9ce2-4734-fa02-dc632e89411d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DetectorPersonas definido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Clase encargada de procesar los resultados de detección de personas en imágenes.\n",
        "### Se encarga de preparar la imagen, ejecutar la detección, extraer información relevante\n",
        "### y formatear los resultados en un diccionario estructurado.\n",
        "class ProcesadorResultados:\n",
        "    ### Constructor de la clase\n",
        "    ### Parámetros:\n",
        "    ### - output_path (str): ruta donde se guardarán los resultados procesados.\n",
        "    def __init__(self, output_path: str):\n",
        "        self.output_path = Path(output_path)\n",
        "\n",
        "    ### Método para procesar una imagen completa\n",
        "    ### Parámetros:\n",
        "    ### - ruta_imagen (str): ruta del archivo de imagen a procesar\n",
        "    ### - detector (DetectorPersonas): instancia del detector previamente configurado\n",
        "    ### - umbrales (List[float]): lista de umbrales de sensibilidad para la detección\n",
        "    ###\n",
        "    ### Retorna:\n",
        "    ### - Dict con información de la imagen, resultados de detección, tiempo de procesamiento,\n",
        "    ###   nombre del modelo y estado de éxito o error.\n",
        "    def procesar_imagen(self, ruta_imagen: str, detector: DetectorPersonas,\n",
        "                        umbrales: List[float]) -> Optional[Dict]:\n",
        "        inicio = time.time()\n",
        "\n",
        "        try:\n",
        "            ### Prepara la imagen para el modelo (redimensiona si es necesario, convierte a RGB)\n",
        "            imagen = Utils.preparar_imagen(ruta_imagen)\n",
        "\n",
        "            ### Calcula un hash único de la imagen para trazabilidad y evitar duplicados\n",
        "            hash_img = Utils.calcular_hash(ruta_imagen)\n",
        "\n",
        "            ### Convierte la imagen a array NumPy para análisis adicional\n",
        "            img_array = np.array(imagen)\n",
        "\n",
        "            ### Convierte la imagen a escala de grises para calcular brillo promedio\n",
        "            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            ### Extrae información básica de la imagen\n",
        "            info_imagen = {\n",
        "                'archivo': os.path.basename(ruta_imagen),  # nombre del archivo\n",
        "                'hash': hash_img,                          # identificador único\n",
        "                'tamaño': imagen.size,                     # (ancho, alto)\n",
        "                'brillo_promedio': float(np.mean(gray))   # valor medio de intensidad (0–255)\n",
        "            }\n",
        "\n",
        "            ### Ejecuta la detección de personas usando el detector y los umbrales definidos\n",
        "            resultados_deteccion = detector.detectar_en_imagen(imagen, umbrales)\n",
        "\n",
        "            ### Calcula el tiempo total de procesamiento en milisegundos\n",
        "            tiempo_total = (time.time() - inicio) * 1000\n",
        "\n",
        "            ### Solo continuaremos si detectamos personas en algún umbral\n",
        "            hay_personas = any(datos.get('personas',0) > 0\n",
        "                               for clave, datos in resultados_deteccion.items()\n",
        "                               if clave.startswith('umbral_'))\n",
        "\n",
        "            if not hay_personas:\n",
        "                return None\n",
        "\n",
        "            ### Devuelve el resultado estructurado\n",
        "            return {\n",
        "                'timestamp': datetime.now().isoformat(),     # fecha y hora del procesamiento\n",
        "                'imagen': info_imagen,                       # metadatos de la imagen\n",
        "                'deteccion': resultados_deteccion,           # resultados por umbral\n",
        "                'tiempo_total_ms': tiempo_total,             # duración total\n",
        "                'modelo': detector.modelo_name,              # nombre del modelo usado\n",
        "                'exitoso': True                              # indicador de éxito\n",
        "            }\n",
        "\n",
        "        ### En caso de error, se captura la excepción y se devuelve un resultado parcial\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'imagen': {'archivo': os.path.basename(ruta_imagen)},\n",
        "                'error': str(e),\n",
        "                'exitoso': False\n",
        "            }\n",
        "\n",
        "    ### Método para mostrar un resumen estadístico de los resultados procesados\n",
        "    ### Parámetros:\n",
        "    ### - resultados (List[Dict]): lista de diccionarios generados por procesar_imagen()\n",
        "    ###\n",
        "    ### Este método imprime en consola:\n",
        "    ### - Número total de imágenes procesadas con éxito\n",
        "    ### - Para cada umbral definido en el perfil 'agresivo':\n",
        "    ###     - Total de personas detectadas\n",
        "    ###     - Número de imágenes donde se detectó al menos una persona\n",
        "    def mostrar_resumen(self, resultados: List[Dict]) -> None:\n",
        "        exitosos = sum(1 for r in resultados if r.get('exitoso', False))\n",
        "        print(f\"\\nRESUMEN: {exitosos}/{len(resultados)} imágenes procesadas\")\n",
        "\n",
        "        if exitosos == 0:\n",
        "            return\n",
        "\n",
        "        ### Itera sobre los umbrales definidos en el perfil 'agresivo' de la configuración\n",
        "        for umbral in config.UMBRALES['agresivo']:\n",
        "            personas_detectadas = []\n",
        "\n",
        "            ### Recorre cada resultado exitoso y extrae el número de personas detectadas para ese umbral\n",
        "            for r in resultados:\n",
        "                if r.get('exitoso', False):\n",
        "                    deteccion = r.get('deteccion', {})\n",
        "                    umbral_data = deteccion.get(f'umbral_{umbral}', {})\n",
        "                    personas_detectadas.append(umbral_data.get('personas', 0))\n",
        "\n",
        "            ### Si hay datos, calcula totales y muestra resumen por umbral\n",
        "            if personas_detectadas:\n",
        "                total_personas = sum(personas_detectadas)\n",
        "                imagenes_con_personas = sum(1 for p in personas_detectadas if p > 0)\n",
        "                print(f\"  Umbral {umbral:5.3f}: {total_personas} personas en {imagenes_con_personas} imágenes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txkQ0UPPmUbc",
        "outputId": "2cd04455-eca9-498d-bf7a-66a781594743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ProcesadorResultados definido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Ejecuta una evaluación básica sobre todas las imágenes del dataset usando un modelo específico\n",
        "### y una configuración de umbrales determinada.\n",
        "###\n",
        "### Parámetros:\n",
        "### - modelo_idx (int): índice del modelo a usar dentro de config.MODELOS (0 = más potente)\n",
        "### - umbral_config (str): clave del diccionario config.UMBRALES ('conservador', 'normal', 'agresivo', 'ultra')\n",
        "###\n",
        "### Retorna:\n",
        "### - Ruta del archivo JSON con los resultados guardados\n",
        "def ejecutar_evaluacion_basica(modelo_idx: int = 0, umbral_config: str = 'agresivo'):\n",
        "    print(f\"EVALUACIÓN BÁSICA - Modelo {modelo_idx}, Umbrales: {umbral_config}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    ### Validación de índice de modelo\n",
        "    if modelo_idx >= len(config.MODELOS):\n",
        "        print(f\"Modelo index inválido. Máximo: {len(config.MODELOS)-1}\")\n",
        "        return\n",
        "\n",
        "    ### Validación de configuración de umbrales\n",
        "    if umbral_config not in config.UMBRALES:\n",
        "        print(f\"Configuración inválida. Disponibles: {list(config.UMBRALES.keys())}\")\n",
        "        return\n",
        "\n",
        "    ### Carga de imágenes desde el dataset\n",
        "    imagenes = Utils.cargar_imagenes(config.DATASET_PATH)\n",
        "    if not imagenes:\n",
        "        print(\"No hay imágenes\")\n",
        "        return\n",
        "\n",
        "    ### Selección de modelo y umbrales\n",
        "    modelo = config.MODELOS[modelo_idx]\n",
        "    umbrales = config.UMBRALES[umbral_config]\n",
        "\n",
        "    ### Inicialización de detector y procesador\n",
        "    detector = DetectorPersonas(modelo)\n",
        "    procesador = ProcesadorResultados(config.OUTPUT_PATH)\n",
        "\n",
        "    ### Procesamiento de cada imagen\n",
        "    resultados = []\n",
        "    for i, ruta in enumerate(tqdm(imagenes, desc=\"Procesando\")):\n",
        "        print(f\"[{i+1:3d}/{len(imagenes)}] {os.path.basename(ruta)}\")\n",
        "        resultado = procesador.procesar_imagen(ruta, detector, umbrales)\n",
        "        resultados.append(resultado)\n",
        "\n",
        "        ### Log básico por imagen\n",
        "        if resultado.get('exitoso', False):\n",
        "            deteccion = resultado['deteccion']\n",
        "            mejor_umbral = min(umbrales)  # Se usa el umbral más sensible\n",
        "            datos = deteccion.get(f'umbral_{mejor_umbral}', {})\n",
        "            personas = datos.get('personas', 0)\n",
        "            total = datos.get('total', 0)\n",
        "            print(f\"{personas} personas de {total} detecciones\")\n",
        "        else:\n",
        "            print(f\"Error: {resultado.get('error', 'unknown')}\")\n",
        "\n",
        "    ### Guardado de resultados en archivo JSON\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    archivo = config.OUTPUT_PATH / f\"evaluacion_{modelo_idx}_{umbral_config}_{timestamp}.json\"\n",
        "    Utils.guardar_json(resultados, str(archivo))\n",
        "\n",
        "    ### Mostrar resumen y liberar memoria\n",
        "    procesador.mostrar_resumen(resultados)\n",
        "    detector.limpiar()\n",
        "\n",
        "    return str(archivo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFGSs_jRmurU",
        "outputId": "39d44622-80cf-4f2a-e576-ec6435b8e2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funciones ejecutables definidas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Prueba una imagen específica con el modelo más potente y umbrales ultra sensibles.\n",
        "### Ideal para depuración o validación rápida.\n",
        "###\n",
        "### Parámetros:\n",
        "### - ruta_imagen (str): ruta completa al archivo de imagen\n",
        "### - mostrar_detalles (bool): si se desea imprimir clases detectadas por umbral\n",
        "###\n",
        "### Retorna:\n",
        "### - Diccionario con los resultados de detección\n",
        "def probar_imagen_individual(ruta_imagen: str, mostrar_detalles: bool = True):\n",
        "    print(f\"PRUEBA INDIVIDUAL: {os.path.basename(ruta_imagen)}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    ### Validación de existencia del archivo\n",
        "    if not os.path.exists(ruta_imagen):\n",
        "        print(f\"Imagen no encontrada: {ruta_imagen}\")\n",
        "        return\n",
        "\n",
        "    ### Inicialización con el modelo más potente y umbrales ultra sensibles\n",
        "    detector = DetectorPersonas(config.MODELOS[0])\n",
        "    procesador = ProcesadorResultados(config.OUTPUT_PATH)\n",
        "\n",
        "    resultado = procesador.procesar_imagen(ruta_imagen, detector, config.UMBRALES['ultra'])\n",
        "\n",
        "    ### Manejo de errores\n",
        "    if not resultado.get('exitoso', False):\n",
        "        print(f\"Error: {resultado.get('error')}\")\n",
        "        detector.limpiar()\n",
        "        return\n",
        "\n",
        "    ### Mostrar información básica de la imagen\n",
        "    deteccion = resultado['deteccion']\n",
        "    imagen_info = resultado['imagen']\n",
        "    print(f\"Imagen: {imagen_info['tamaño']} pixels, brillo: {imagen_info['brillo_promedio']:.1f}\")\n",
        "    print(f\"Tiempo: {deteccion['tiempo_inferencia_ms']:.1f}ms\\n\")\n",
        "\n",
        "    ### Mostrar resultados por umbral\n",
        "    print(\"DETECCIONES POR UMBRAL:\")\n",
        "    for umbral in config.UMBRALES['ultra']:\n",
        "        datos = deteccion.get(f'umbral_{umbral}', {})\n",
        "        if 'error' in datos:\n",
        "            print(f\"  {umbral:6.3f}: Error - {datos['error']}\")\n",
        "            continue\n",
        "\n",
        "        personas = datos.get('personas', 0)\n",
        "        total = datos.get('total', 0)\n",
        "        max_score = datos.get('max_score', 0)\n",
        "        print(f\"  {umbral:6.3f}: {personas:2d} personas de {total:2d} total (max score: {max_score:.4f})\")\n",
        "\n",
        "        ### Mostrar clases detectadas si se solicita\n",
        "        if mostrar_detalles and datos.get('todas_clases'):\n",
        "            clases_unicas = sorted(set(datos['todas_clases']))\n",
        "            print(f\"             Clases: {clases_unicas}\")\n",
        "\n",
        "    ### Recomendación basada en detección con umbral más sensible\n",
        "    mejor_umbral_data = deteccion.get('umbral_0.01', deteccion.get('umbral_0.001', {}))\n",
        "    personas_detectadas = mejor_umbral_data.get('personas', 0)\n",
        "\n",
        "    print(f\"\\nRESULTADO: {personas_detectadas} personas detectadas\")\n",
        "    if personas_detectadas == 0:\n",
        "        total_detecciones = mejor_umbral_data.get('total', 0)\n",
        "        if total_detecciones > 0:\n",
        "            print(\" Se detectaron otros objetos pero no personas\")\n",
        "            print(\"   Posibles soluciones:\")\n",
        "            print(\"   - Usar umbrales aún más bajos\")\n",
        "            print(\"   - Verificar que la imagen contenga personas visibles\")\n",
        "            print(\"   - Probar con otro modelo\")\n",
        "        else:\n",
        "            print(\" No se detectó ningún objeto\")\n",
        "\n",
        "    detector.limpiar()\n",
        "    return resultado\n"
      ],
      "metadata": {
        "id": "fCCKI0J4f6Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Compara rápidamente todos los modelos disponibles sobre un subconjunto de imágenes.\n",
        "### Para benchmarking o selección de modelo.\n",
        "###\n",
        "### Parámetros:\n",
        "### - max_imagenes (int): número máximo de imágenes a procesar\n",
        "###\n",
        "### Retorna:\n",
        "### - Diccionario con estadísticas por modelo (personas detectadas, tiempo promedio, etc.)\n",
        "def comparar_modelos_rapido(max_imagenes: int = 3):\n",
        "    print(f\"COMPARACIÓN RÁPIDA - {max_imagenes} imágenes máximo\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    ### Carga un subconjunto de imágenes\n",
        "    imagenes = Utils.cargar_imagenes(config.DATASET_PATH)[:max_imagenes]\n",
        "    if not imagenes:\n",
        "        print(\"No hay imágenes\")\n",
        "        return\n",
        "\n",
        "    umbrales = config.UMBRALES['agresivo']\n",
        "    resultados_comparacion = {}\n",
        "\n",
        "    ### Itera sobre cada modelo disponible\n",
        "    for i, modelo in enumerate(config.MODELOS):\n",
        "        print(f\"\\nModelo {i+1}/{len(config.MODELOS)}: {modelo.split('/')[-1]}\")\n",
        "\n",
        "        detector = DetectorPersonas(modelo)\n",
        "        procesador = ProcesadorResultados(config.OUTPUT_PATH)\n",
        "\n",
        "        resultados_modelo = []\n",
        "        for ruta in imagenes:\n",
        "            resultado = procesador.procesar_imagen(ruta, detector, umbrales)\n",
        "            resultados_modelo.append(resultado)\n",
        "\n",
        "        ### Estadísticas rápidas por modelo\n",
        "        exitosos = sum(1 for r in resultados_modelo if r.get('exitoso', False))\n",
        "        personas_totales = 0\n",
        "        tiempo_promedio = 0\n",
        "\n",
        "        for resultado in resultados_modelo:\n",
        "            if resultado.get('exitoso', False):\n",
        "                deteccion = resultado['deteccion']\n",
        "                umbral_data = deteccion.get('umbral_0.01', deteccion.get('umbral_0.05', {}))\n",
        "                personas_totales += umbral_data.get('personas', 0)\n",
        "                tiempo_promedio += resultado.get('tiempo_total_ms', 0)\n",
        "\n",
        "        if exitosos > 0:\n",
        "            tiempo_promedio /= exitosos\n",
        "\n",
        "        resultados_comparacion[modelo] = {\n",
        "            'exitosos': exitosos,\n",
        "            'personas_totales': personas_totales,\n",
        "            'tiempo_promedio_ms': tiempo_promedio\n",
        "        }\n",
        "\n",
        "        print(f\" {exitosos}/{len(imagenes)} exitosas\")\n",
        "        print(f\" {personas_totales} personas detectadas\")\n",
        "        print(f\" {tiempo_promedio:.1f}ms promedio\")"
      ],
      "metadata": {
        "id": "Xr3zD9PQf9qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Ejecutamos evaluación básica.\n",
        "for i in range(len(config.MODELOS)):\n",
        "    for umbral in config.UMBRALES.keys():\n",
        "        ejecutar_evaluacion_basica(i, umbral)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GaIHsaRnZvg",
        "outputId": "eb73bc38-a831-48d0-fe19-1ef09a6c33fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUACIÓN BÁSICA - Modelo 0, Umbrales: ultra\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-large-coco-instance\n",
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_0_ultra_20250902_184211.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 0, Umbrales: agresivo\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-large-coco-instance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_0_agresivo_20250902_184213.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 0, Umbrales: normal\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-large-coco-instance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_0_normal_20250902_184215.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 0, Umbrales: conservador\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-large-coco-instance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_0_conservador_20250902_184217.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 1, Umbrales: ultra\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-base-ade-semantic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: wall\n",
            "Clase persona encontrada: 12 = person\n",
            "Modelo cargado en cuda (semántico: True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 personas de 6 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_1_ultra_20250902_184218.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 1 personas en 1 imágenes\n",
            "  Umbral 0.010: 1 personas en 1 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 1 personas en 1 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 1, Umbrales: agresivo\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-base-ade-semantic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: wall\n",
            "Clase persona encontrada: 12 = person\n",
            "Modelo cargado en cuda (semántico: True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 personas de 6 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_1_agresivo_20250902_184220.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 1 personas en 1 imágenes\n",
            "  Umbral 0.010: 1 personas en 1 imágenes\n",
            "  Umbral 0.050: 1 personas en 1 imágenes\n",
            "  Umbral 0.100: 1 personas en 1 imágenes\n",
            "  Umbral 0.300: 1 personas en 1 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 1, Umbrales: normal\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-base-ade-semantic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: wall\n",
            "Clase persona encontrada: 12 = person\n",
            "Modelo cargado en cuda (semántico: True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 personas de 6 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_1_normal_20250902_184222.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 1 personas en 1 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 1 personas en 1 imágenes\n",
            "  Umbral 0.300: 1 personas en 1 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 1, Umbrales: conservador\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-base-ade-semantic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: wall\n",
            "Clase persona encontrada: 12 = person\n",
            "Modelo cargado en cuda (semántico: True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 personas de 6 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_1_conservador_20250902_184223.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 1 personas en 1 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 2, Umbrales: ultra\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-small-coco-instance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_2_ultra_20250902_184225.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 2, Umbrales: agresivo\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-small-coco-instance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_2_agresivo_20250902_184227.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 2, Umbrales: normal\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-small-coco-instance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_2_normal_20250902_184228.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n",
            "EVALUACIÓN BÁSICA - Modelo 2, Umbrales: conservador\n",
            "============================================================\n",
            "Encontradas 1 imágenes\n",
            "Cargando: facebook/mask2former-swin-small-coco-instance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 0: person\n",
            "Clase persona encontrada: 0 = person\n",
            "Modelo cargado en cuda (semántico: False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/1] _DSC0147.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 personas de 0 detecciones\n",
            "Guardado: /content/drive/MyDrive/TFM/mask2former/resultados/evaluacion_2_conservador_20250902_184230.json\n",
            "\n",
            "RESUMEN: 1/1 imágenes procesadas\n",
            "  Umbral 0.001: 0 personas en 0 imágenes\n",
            "  Umbral 0.010: 0 personas en 0 imágenes\n",
            "  Umbral 0.050: 0 personas en 0 imágenes\n",
            "  Umbral 0.100: 0 personas en 0 imágenes\n",
            "  Umbral 0.300: 0 personas en 0 imágenes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}