{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMg6MT8Dhn9GJ/Y7XAExPa8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/01_Mask2Former_Recopilacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluador Mask2Former para recolección de Métricas\n",
        "**Recopilación de todas las métricas posibles sin ground truth.**\n",
        "\n",
        "- Autor: Jesús L.\n",
        "- Proyecto: TFM. Evaluación comparativa de técnicas de segmentación."
      ],
      "metadata": {
        "id": "a8lOSAZmzYlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2pdz2A9tsSqk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForUniversalSegmentation, AutoModelForSemanticSegmentation\n",
        "from PIL import Image, ImageStat\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import measure\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. Montar Google Drive\n",
        "# ==========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta a tu dataset en Google Drive\n",
        "DATASET_PATH = \"/content/drive/MyDrive/TFM/mask2former/imagenes\"\n",
        "# Carpeta donde se guardarán los resultados\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/TFM/mask2former/resultados\"\n",
        "\n",
        "# Crear carpeta de resultados si no existe\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Lista de modelos a evaluar\n",
        "MODELOS = [\n",
        "    \"facebook/mask2former-swin-large-coco-instance\",\n",
        "    \"facebook/maskformer-swin-base-coco\",\n",
        "    \"facebook/mask2former-swin-base-ade-semantic\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqQBwP7t_vDN",
        "outputId": "61e70f1c-ca4a-4a1c-ef93-e2024443ca27"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RecolectorMetricasCompletas:\n",
        "    \"\"\"\n",
        "    Recolecta todas las métricas posibles de una segmentación sin ground truth.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # Detección automática de arquitectura\n",
        "        model_name = config['model_name'].lower()\n",
        "        if \"mask2former\" in model_name:\n",
        "            self.processor = AutoImageProcessor.from_pretrained(config['model_name'])\n",
        "            self.model = AutoModelForUniversalSegmentation.from_pretrained(config['model_name'])\n",
        "        elif \"maskformer\" in model_name:\n",
        "            self.processor = AutoImageProcessor.from_pretrained(config['model_name'])\n",
        "            self.model = AutoModelForSemanticSegmentation.from_pretrained(config['model_name'])\n",
        "        else:\n",
        "            raise ValueError(f\"Arquitectura no reconocida para {config['model_name']}\")\n",
        "\n",
        "        self.device = torch.device(config['device'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Crear directorios\n",
        "        self.output_dir = Path(config['output_dir'])\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        (self.output_dir / 'datos_completos').mkdir(exist_ok=True)\n",
        "\n",
        "        self.resultados = []\n",
        "        print(f\"Modelo cargado en {self.device}\")\n",
        "\n",
        "\n",
        "    def preprocesar_imagen(ruta, max_size=1024):\n",
        "      img = Image.open(ruta).convert(\"RGB\")\n",
        "      img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "      return img\n",
        "\n",
        "    def procesar_imagen(self, ruta_imagen):\n",
        "        \"\"\"Procesa una imagen y recolecta TODAS las métricas posibles.\"\"\"\n",
        "\n",
        "        inicio = time.time()\n",
        "\n",
        "        try:\n",
        "            # 1. CARGA Y ANÁLISIS BÁSICO DE IMAGEN\n",
        "            imagen_pil = self.preprocesar_imagen(ruta_imagen)\n",
        "            imagen_np = np.array(imagen_pil)\n",
        "            h, w = imagen_np.shape[:2]\n",
        "            h, w = imagen_np.shape[:2]\n",
        "\n",
        "            # Hash único para la imagen\n",
        "            ruta_imagen = Path(ruta_imagen)\n",
        "            hash_img = hashlib.md5(open(ruta_imagen, 'rb').read()).hexdigest()[:12]\n",
        "\n",
        "            # Estadísticas básicas de la imagen\n",
        "            stat = ImageStat.Stat(imagen_pil)\n",
        "            gray = cv2.cvtColor(imagen_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # 2. ANÁLISIS VISUAL DE LA IMAGEN (sin modelo)\n",
        "            metricas_imagen = {\n",
        "                # Básicas\n",
        "                'archivo': os.path.basename(ruta_imagen),\n",
        "                'hash': hash_img,\n",
        "                'resolucion_w': w,\n",
        "                'resolucion_h': h,\n",
        "                'aspect_ratio': w/h,\n",
        "                'area_total': w*h,\n",
        "\n",
        "                # Color y brillo\n",
        "                'brillo_promedio': float(np.mean(gray)),\n",
        "                'brillo_std': float(np.std(gray)),\n",
        "                'rgb_mean_r': float(stat.mean[0]),\n",
        "                'rgb_mean_g': float(stat.mean[1]),\n",
        "                'rgb_mean_b': float(stat.mean[2]),\n",
        "                'rgb_std_r': float(stat.stddev[0]),\n",
        "                'rgb_std_g': float(stat.stddev[1]),\n",
        "                'rgb_std_b': float(stat.stddev[2]),\n",
        "\n",
        "                # Textura y complejidad\n",
        "                'varianza_laplacian': float(cv2.Laplacian(gray, cv2.CV_64F).var()),\n",
        "                'entropia': self._calcular_entropia(gray),\n",
        "                'densidad_bordes': self._calcular_densidad_bordes(gray),\n",
        "\n",
        "                # HSV\n",
        "                'saturacion_media': float(np.mean(cv2.cvtColor(imagen_np, cv2.COLOR_RGB2HSV)[:,:,1])),\n",
        "                'saturacion_std': float(np.std(cv2.cvtColor(imagen_np, cv2.COLOR_RGB2HSV)[:,:,1])),\n",
        "\n",
        "                # Contraste local\n",
        "                'contraste_local_std': float(np.std([cv2.Laplacian(gray[i:i+32, j:j+32], cv2.CV_64F).var()\n",
        "                                                   for i in range(0, h-32, 32)\n",
        "                                                   for j in range(0, w-32, 32) if i+32<h and j+32<w]))\n",
        "            }\n",
        "\n",
        "            # 3. RECURSOS ANTES DE INFERENCIA\n",
        "            recursos_antes = self._obtener_recursos()\n",
        "\n",
        "            # 4. INFERENCIA DEL MODELO\n",
        "            inputs = self.processor(images=imagen_pil, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            tiempo_inferencia_inicio = time.time()\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            tiempo_inferencia = (time.time() - tiempo_inferencia_inicio) * 1000\n",
        "\n",
        "            # 5. POST-PROCESAMIENTO CON MÚLTIPLES UMBRALES\n",
        "            umbrales = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "            resultados_por_umbral = {}\n",
        "\n",
        "            for umbral in umbrales:\n",
        "                resultado = self.processor.post_process_instance_segmentation(\n",
        "                    outputs, target_sizes=[(h, w)], threshold=umbral\n",
        "                )[0]\n",
        "\n",
        "                # Extraer personas (clase 0)\n",
        "                mascaras_personas = []\n",
        "                scores_personas = []\n",
        "\n",
        "                if 'labels' in resultado and 'masks' in resultado:\n",
        "                    for i, (label, score) in enumerate(zip(resultado[\"labels\"], resultado[\"scores\"])):\n",
        "                        if label == 0:  # persona\n",
        "                            mask = resultado[\"masks\"][i].cpu().numpy().squeeze() > 0.5\n",
        "                            mascaras_personas.append(mask)\n",
        "                            scores_personas.append(float(score.item()))\n",
        "\n",
        "                # Máscara combinada de todas las personas\n",
        "                if mascaras_personas:\n",
        "                    mascara_combinada = np.logical_or.reduce(mascaras_personas)\n",
        "                else:\n",
        "                    mascara_combinada = np.zeros((h, w), dtype=bool)\n",
        "\n",
        "                # Métricas por umbral\n",
        "                resultados_por_umbral[f'umbral_{umbral}'] = {\n",
        "                    'num_detecciones': len(mascaras_personas),\n",
        "                    'scores': scores_personas,\n",
        "                    'area_segmentada': int(np.sum(mascara_combinada)),\n",
        "                    'porcentaje_imagen': float(np.sum(mascara_combinada) / (h*w) * 100),\n",
        "                    'coherencia': self._metricas_coherencia(mascara_combinada)\n",
        "                }\n",
        "\n",
        "            # 6. ANÁLISIS PANÓPTICO\n",
        "            metricas_panoptico = {}\n",
        "            try:\n",
        "                resultado_panoptico = self.processor.post_process_panoptic_segmentation(\n",
        "                    outputs, target_sizes=[(h, w)]\n",
        "                )[0]\n",
        "\n",
        "                if 'segments_info' in resultado_panoptico:\n",
        "                    segmentos = resultado_panoptico['segments_info']\n",
        "                    categorias = [seg.get('category_id', -1) for seg in segmentos]\n",
        "\n",
        "                    metricas_panoptico = {\n",
        "                        'total_segmentos': len(segmentos),\n",
        "                        'categorias_unicas': len(set(categorias)),\n",
        "                        'areas_segmentos': [seg.get('area', 0) for seg in segmentos],\n",
        "                        'distribucion_categorias': {str(k): categorias.count(k) for k in set(categorias)},\n",
        "                        'segmentos_persona': sum(1 for seg in segmentos if seg.get('category_id') == 0)\n",
        "                    }\n",
        "            except:\n",
        "                metricas_panoptico = {'error': 'Panóptico no disponible'}\n",
        "\n",
        "            # 7. RECURSOS DESPUÉS DE INFERENCIA\n",
        "            recursos_despues = self._obtener_recursos()\n",
        "            tiempo_total = (time.time() - inicio) * 1000\n",
        "\n",
        "            # 8. ANÁLISIS DE LA MEJOR DETECCIÓN (umbral 0.5 como referencia)\n",
        "            mejor_resultado = resultados_por_umbral.get('umbral_0.5', {})\n",
        "\n",
        "            # 9. CLASIFICACIÓN AUTOMÁTICA DE CONTEXTO\n",
        "            contexto = self._clasificar_contexto_automatico(\n",
        "                imagen_np,\n",
        "                mejor_resultado.get('num_detecciones', 0),\n",
        "                metricas_imagen\n",
        "            )\n",
        "\n",
        "            # 10. RESULTADO FINAL COMPLETO\n",
        "            resultado_completo = {\n",
        "                # Metadatos\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'id_procesamiento': f\"{hash_img}_{int(time.time())}\",\n",
        "\n",
        "                # Datos de la imagen\n",
        "                'imagen': metricas_imagen,\n",
        "\n",
        "                # Rendimiento\n",
        "                'rendimiento': {\n",
        "                    'tiempo_inferencia_ms': tiempo_inferencia,\n",
        "                    'tiempo_total_ms': tiempo_total,\n",
        "                    'memoria_antes_mb': recursos_antes.get('memoria_usada_mb', 0),\n",
        "                    'memoria_despues_mb': recursos_despues.get('memoria_usada_mb', 0),\n",
        "                    'memoria_gpu_mb': recursos_despues.get('gpu_memoria_mb', 0),\n",
        "                    'cpu_percent': recursos_despues.get('cpu_percent', 0)\n",
        "                },\n",
        "\n",
        "                # Resultados por umbral\n",
        "                'segmentacion': resultados_por_umbral,\n",
        "\n",
        "                # Análisis panóptico\n",
        "                'panoptico': metricas_panoptico,\n",
        "\n",
        "                # Contexto automático\n",
        "                'contexto': contexto,\n",
        "\n",
        "                # Modelo info\n",
        "                'modelo': {\n",
        "                    'nombre': self.config['model_name'],\n",
        "                    'device': str(self.device),\n",
        "                    'confianza_umbral_principal': 0.5\n",
        "                }\n",
        "            }\n",
        "\n",
        "            del inputs, outputs, imagen_pil, imagen_np\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            return resultado_completo\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'imagen': {'archivo': os.path.basename(ruta_imagen)},\n",
        "                'error': str(e),\n",
        "                'procesamiento_fallido': True\n",
        "            }\n",
        "\n",
        "    def _calcular_entropia(self, imagen_gray):\n",
        "        \"\"\"Calcula la entropía de Shannon de la imagen.\"\"\"\n",
        "        histogram, _ = np.histogram(imagen_gray, bins=256, range=(0, 256))\n",
        "        histogram = histogram + 1e-7  # Evitar log(0)\n",
        "        prob = histogram / np.sum(histogram)\n",
        "        return float(-np.sum(prob * np.log2(prob)))\n",
        "\n",
        "    def _calcular_densidad_bordes(self, imagen_gray):\n",
        "        \"\"\"Calcula densidad de bordes usando Canny.\"\"\"\n",
        "        edges = cv2.Canny(imagen_gray, 50, 150)\n",
        "        return float(np.sum(edges > 0) / edges.size)\n",
        "\n",
        "    def _metricas_coherencia(self, mascara):\n",
        "        \"\"\"Métricas de coherencia espacial de una máscara.\"\"\"\n",
        "        if np.sum(mascara) == 0:\n",
        "            return {\n",
        "                'area': 0,\n",
        "                'componentes': 0,\n",
        "                'compacidad': 0.0,\n",
        "                'solidez': 0.0\n",
        "            }\n",
        "\n",
        "        # Componentes conectados\n",
        "        labeled = measure.label(mascara)\n",
        "        num_componentes = int(labeled.max())\n",
        "\n",
        "        # Contornos para otras métricas\n",
        "        contours, _ = cv2.findContours(mascara.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if contours:\n",
        "            contorno_principal = max(contours, key=cv2.contourArea)\n",
        "            area = cv2.contourArea(contorno_principal)\n",
        "            perimetro = cv2.arcLength(contorno_principal, True)\n",
        "\n",
        "            # Convex hull\n",
        "            hull = cv2.convexHull(contorno_principal)\n",
        "            area_hull = cv2.contourArea(hull)\n",
        "\n",
        "            # Métricas\n",
        "            compacidad = (4 * np.pi * area) / (perimetro ** 2) if perimetro > 0 else 0\n",
        "            solidez = area / area_hull if area_hull > 0 else 0\n",
        "\n",
        "            return {\n",
        "                'area': int(area),\n",
        "                'componentes': num_componentes,\n",
        "                'compacidad': float(compacidad),\n",
        "                'solidez': float(solidez),\n",
        "                'perimetro': float(perimetro)\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'area': int(np.sum(mascara)),\n",
        "            'componentes': num_componentes,\n",
        "            'compacidad': 0.0,\n",
        "            'solidez': 0.0\n",
        "        }\n",
        "\n",
        "    def _obtener_recursos(self):\n",
        "        \"\"\"Obtiene información de recursos del sistema.\"\"\"\n",
        "        try:\n",
        "            mem = psutil.virtual_memory()\n",
        "            recursos = {\n",
        "                'memoria_usada_mb': mem.used / (1024*1024),\n",
        "                'cpu_percent': psutil.cpu_percent()\n",
        "            }\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                recursos['gpu_memoria_mb'] = torch.cuda.memory_allocated() / (1024*1024)\n",
        "\n",
        "            return recursos\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def _clasificar_contexto_automatico(self, imagen, num_personas, metricas_img):\n",
        "        \"\"\"Clasificación automática de contexto basada en métricas.\"\"\"\n",
        "\n",
        "        # Reglas simples de clasificación\n",
        "        if num_personas == 0:\n",
        "            categoria = 'sin_personas'\n",
        "        elif num_personas == 1:\n",
        "            if metricas_img['densidad_bordes'] < 0.1:\n",
        "                categoria = 'retrato_simple'\n",
        "            else:\n",
        "                categoria = 'retrato_complejo'\n",
        "        else:\n",
        "            categoria = 'multiples_personas'\n",
        "\n",
        "        # Análisis de complejidad\n",
        "        complejidad = (\n",
        "            metricas_img['densidad_bordes'] * 0.4 +\n",
        "            min(metricas_img['varianza_laplacian'] / 1000, 1.0) * 0.3 +\n",
        "            (metricas_img['contraste_local_std'] / 100) * 0.3\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'categoria': categoria,\n",
        "            'complejidad_score': float(min(complejidad, 1.0)),\n",
        "            'num_personas_detectadas': num_personas,\n",
        "            'iluminacion': 'baja' if metricas_img['brillo_promedio'] < 80 else\n",
        "                         'alta' if metricas_img['brillo_promedio'] > 180 else 'normal'\n",
        "        }\n",
        "\n",
        "    def evaluar_dataset(self, rutas_imagenes):\n",
        "        \"\"\"Evalúa un conjunto de imágenes completo.\"\"\"\n",
        "        print(f\"Iniciando evaluación de {len(rutas_imagenes)} imágenes\")\n",
        "\n",
        "        self.resultados = []\n",
        "        exitosas = 0\n",
        "\n",
        "        for i, ruta in enumerate(rutas_imagenes):\n",
        "            print(f\"[{i+1:4d}/{len(rutas_imagenes)}] {os.path.basename(ruta)}\")\n",
        "\n",
        "            resultado = self.procesar_imagen(ruta)\n",
        "            self.resultados.append(resultado)\n",
        "\n",
        "            if not resultado.get('procesamiento_fallido', False):\n",
        "                exitosas += 1\n",
        "                num_personas = resultado.get('segmentacion', {}).get('umbral_0.5', {}).get('num_detecciones', 0)\n",
        "                tiempo = resultado.get('rendimiento', {}).get('tiempo_total_ms', 0)\n",
        "                print(f\"  {num_personas} personas, {tiempo:.1f}ms\")\n",
        "            else:\n",
        "                print(f\"  Error: {resultado.get('error', 'unknown')}\")\n",
        "\n",
        "            # Limpieza periódica de memoria\n",
        "            if (i + 1) % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"\\nResumen: {exitosas}/{len(rutas_imagenes)} exitosas\")\n",
        "        return self.resultados\n",
        "\n",
        "    def guardar_resultados(self, nombre_archivo=None):\n",
        "        \"\"\"Guarda todos los resultados en formato JSON.\"\"\"\n",
        "        if not nombre_archivo:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            nombre_archivo = f\"evaluacion_mask2former_{timestamp}.json\"\n",
        "\n",
        "        ruta_archivo = self.output_dir / 'datos_completos' / nombre_archivo\n",
        "\n",
        "        # Preparar datos para JSON\n",
        "        datos_exportacion = {\n",
        "            'resumen': {\n",
        "                'total_imagenes': len(self.resultados),\n",
        "                'exitosas': sum(1 for r in self.resultados if not r.get('procesamiento_fallido', False)),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'modelo': self.config['model_name']\n",
        "            },\n",
        "            'configuracion': self.config,\n",
        "            'resultados': self.resultados\n",
        "        }\n",
        "\n",
        "        with open(ruta_archivo, 'w', encoding='utf-8') as f:\n",
        "            json.dump(datos_exportacion, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Resultados guardados en: {ruta_archivo}\")\n",
        "        print(f\"Tamaño del archivo: {os.path.getsize(ruta_archivo) / (1024*1024):.2f} MB\")\n",
        "\n",
        "        return str(ruta_archivo)"
      ],
      "metadata": {
        "id": "zwsRY9gBCXOn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. Función para cargar dataset desde carpeta\n",
        "# ==========================================\n",
        "def cargar_dataset(ruta_dataset, extensiones=(\".jpg\", \".png\", \".jpeg\")):\n",
        "    ruta = Path(ruta_dataset)\n",
        "    imagenes = [str(p) for p in ruta.glob(\"**/*\") if p.suffix.lower() in extensiones]\n",
        "    print(f\"Dataset cargado: {len(imagenes)} imágenes encontradas.\")\n",
        "    return imagenes"
      ],
      "metadata": {
        "id": "X8VYJDQVAhz4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Ejecutor por lotes con limpieza de GPU\n",
        "# ==========================================\n",
        "def procesar_por_lotes(evaluador, imagenes, tam_lote=5):\n",
        "    resultados_totales = []\n",
        "    for i in tqdm(range(0, len(imagenes), tam_lote), desc=\"Procesando lotes\"):\n",
        "        lote = imagenes[i:i+tam_lote]\n",
        "        resultados_lote = evaluador.evaluar_dataset(lote)\n",
        "        resultados_totales.extend(resultados_lote)\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    return resultados_totales"
      ],
      "metadata": {
        "id": "dI0ttYONAksY"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_resumen(resultados):\n",
        "    resumen = {}\n",
        "    for r in resultados:\n",
        "        for clave, valor in r.items():\n",
        "            if isinstance(valor, (int, float)):\n",
        "                resumen.setdefault(clave, []).append(valor)\n",
        "    return {k: sum(v)/len(v) for k, v in resumen.items() if v}"
      ],
      "metadata": {
        "id": "7hkAGgVCEiF1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. Ejecución sobre múltiples modelos\n",
        "# ==========================================\n",
        "def ejecutar_multi_modelo(modelos, dataset_path, drive_output_path, tam_lote=1):\n",
        "    imagenes = cargar_dataset(dataset_path)\n",
        "\n",
        "    resumen_global = {}\n",
        "\n",
        "    for modelo in modelos:\n",
        "        print(f\"\\n=========================\")\n",
        "        print(f\"Ejecutando modelo: {modelo}\")\n",
        "        print(f\"=========================\")\n",
        "\n",
        "        config = {\n",
        "            'model_name': modelo,\n",
        "            'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            'output_dir': drive_output_path\n",
        "        }\n",
        "\n",
        "        evaluador = RecolectorMetricasCompletas(config)\n",
        "        resultados = procesar_por_lotes(evaluador, imagenes, tam_lote)\n",
        "\n",
        "        # Guardar resultados por modelo\n",
        "        nombre_archivo = f\"resultados_{modelo.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        ruta_archivo = Path(drive_output_path) / nombre_archivo\n",
        "        with open(ruta_archivo, 'w', encoding='utf-8') as f:\n",
        "            json.dump(resultados, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Resultados guardados en: {ruta_archivo}\")\n",
        "\n",
        "        resumen_global[modelo] = generar_resumen(resultados)\n",
        "\n",
        "    # Guardar resumen global\n",
        "    archivo_resumen = Path(drive_output_path) / f\"resumen_global_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(archivo_resumen, 'w', encoding='utf-8') as f:\n",
        "        json.dump(resumen_global, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"\\nResumen global guardado en: {archivo_resumen}\")"
      ],
      "metadata": {
        "id": "CSANdR_yBOk_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. Configuración y ejecución principal\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    ejecutar_multi_modelo(MODELOS, DATASET_PATH, OUTPUT_PATH, tam_lote=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "eAJL4CxFBVn7",
        "outputId": "d0bccc84-5112-4e15-f2e1-f21284354a46"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cargado: 3 imágenes encontradas.\n",
            "\n",
            "=========================\n",
            "Ejecutando modelo: facebook/mask2former-swin-large-coco-instance\n",
            "=========================\n",
            "Modelo cargado en cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcesando lotes:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando evaluación de 3 imágenes\n",
            "[   1/3] _DSC0320.jpg\n",
            "  Error: 'RecolectorMetricasCompletas' object has no attribute 'read'\n",
            "[   2/3] _DSC0160.jpg\n",
            "  Error: 'RecolectorMetricasCompletas' object has no attribute 'read'\n",
            "[   3/3] _DSC1017.jpg\n",
            "  Error: 'RecolectorMetricasCompletas' object has no attribute 'read'\n",
            "\n",
            "Resumen: 0/3 exitosas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando lotes: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados guardados en: /content/drive/MyDrive/TFM/mask2former/resultados/resultados_facebook_mask2former-swin-large-coco-instance_20250826_205928.json\n",
            "\n",
            "=========================\n",
            "Ejecutando modelo: facebook/maskformer-swin-base-coco\n",
            "=========================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized configuration class <class 'transformers.models.maskformer.configuration_maskformer.MaskFormerConfig'> for this kind of AutoModel: AutoModelForSemanticSegmentation.\nModel type should be one of BeitConfig, Data2VecVisionConfig, DPTConfig, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, SegformerConfig, UperNetConfig.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3428512435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mejecutar_multi_modelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtam_lote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3600275104.py\u001b[0m in \u001b[0;36mejecutar_multi_modelo\u001b[0;34m(modelos, dataset_path, drive_output_path, tam_lote)\u001b[0m\n\u001b[1;32m     18\u001b[0m         }\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mevaluador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecolectorMetricasCompletas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mresultados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesar_por_lotes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluador\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtam_lote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3102772492.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"maskformer\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoImageProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSemanticSegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Arquitectura no reconocida para {config['model_name']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             )\n\u001b[0;32m--> 603\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.maskformer.configuration_maskformer.MaskFormerConfig'> for this kind of AutoModel: AutoModelForSemanticSegmentation.\nModel type should be one of BeitConfig, Data2VecVisionConfig, DPTConfig, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, SegformerConfig, UperNetConfig."
          ]
        }
      ]
    }
  ]
}