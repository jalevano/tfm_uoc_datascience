{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalevano/tfm_uoc_datascience/blob/main/03_Analisis_Fase_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "pB26CjaZmcVT",
        "outputId": "5b1bb5eb-f863-481c-8c0e-2aca690f9a73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n================================================================================\\nNOTEBOOK 3 - FASE 1: INTEGRACIÓN DE DATOS (VERSIÓN COMPLETA)\\n================================================================================\\nTrabajo Fin de Máster: Evaluación Comparativa de Técnicas de Segmentación\\nen Fotografía de Retrato\\n\\nAutor: Jesús L.\\nUniversidad: Universidad Oberta de Cataluña (UOC)\\nFecha: Noviembre 2025\\n\\nDESCRIPCIÓN:\\nFase 1 del análisis comparativo de modelos de segmentación. Integra datos de \\ncaracterísticas fotográficas (Notebook 00), máscaras de modelos de segmentación \\n(Notebooks 01-04), metadata de ejecución y ground truth manual anotado en CVAT \\nen una estructura unificada para procesamiento posterior.\\n\\nCOMPONENTES PRINCIPALES:\\n1. Lector de características fotográficas con manejo defensivo de EXIF\\n2. Lector de ground truth desde anotaciones CVAT\\n3. Escaneo dinámico y recursivo de configuraciones de modelos\\n4. Carga de máscaras NPZ con estructuras heterogéneas\\n5. Carga de metadata de ejecución (tiempos, scores, configuraciones)\\n6. Consolidación en estructura DatosFotografia por imagen\\n7. Validación de integridad de datos con control de errores críticos\\n\\nMEJORAS EN ESTA VERSIÓN:\\n- Logging sin duplicación (Google Colab compatible)\\n- Búsqueda recursiva de configuraciones de modelos\\n- Soporte para JSONs en subdirectorios\\n- Carga de metadata de ejecución de modelos\\n- Búsqueda flexible de máscaras NPZ en múltiples ubicaciones\\n- Unificación de metadata de diferentes modelos\\n\\nCRITERIOS DE ERROR:\\n- ERROR CRÍTICO (detiene ejecución): Falta Ground Truth, falta características\\n- WARNING (continúa): Falta máscara de un modelo específico, campos opcionales\\n\\nReferencias:\\n- Gonzalez, R.C. & Woods, R.E. (2018). Digital Image Processing (4th ed.)\\n- Lin, T.Y., et al. (2014). Microsoft COCO: Common Objects in Context\\n================================================================================\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "================================================================================\n",
        "NOTEBOOK 3 - FASE 1: INTEGRACIÓN DE DATOS (VERSIÓN COMPLETA)\n",
        "================================================================================\n",
        "Trabajo Fin de Máster: Evaluación Comparativa de Técnicas de Segmentación\n",
        "en Fotografía de Retrato\n",
        "\n",
        "Autor: Jesús L.\n",
        "Universidad: Universidad Oberta de Cataluña (UOC)\n",
        "Fecha: Noviembre 2025\n",
        "\n",
        "DESCRIPCIÓN:\n",
        "Fase 1 del análisis comparativo de modelos de segmentación. Integra datos de\n",
        "características fotográficas (Notebook 00), máscaras de modelos de segmentación\n",
        "(Notebooks 01-04), metadata de ejecución y ground truth manual anotado en CVAT\n",
        "en una estructura unificada para procesamiento posterior.\n",
        "\n",
        "COMPONENTES PRINCIPALES:\n",
        "1. Lector de características fotográficas con manejo defensivo de EXIF\n",
        "2. Lector de ground truth desde anotaciones CVAT\n",
        "3. Escaneo dinámico y recursivo de configuraciones de modelos\n",
        "4. Carga de máscaras NPZ con estructuras heterogéneas\n",
        "5. Carga de metadata de ejecución (tiempos, scores, configuraciones)\n",
        "6. Consolidación en estructura DatosFotografia por imagen\n",
        "7. Validación de integridad de datos con control de errores críticos\n",
        "\n",
        "MEJORAS EN ESTA VERSIÓN:\n",
        "- Logging sin duplicación (Google Colab compatible)\n",
        "- Búsqueda recursiva de configuraciones de modelos\n",
        "- Soporte para JSONs en subdirectorios\n",
        "- Carga de metadata de ejecución de modelos\n",
        "- Búsqueda flexible de máscaras NPZ en múltiples ubicaciones\n",
        "- Unificación de metadata de diferentes modelos\n",
        "\n",
        "CRITERIOS DE ERROR:\n",
        "- ERROR CRÍTICO (detiene ejecución): Falta Ground Truth, falta características\n",
        "- WARNING (continúa): Falta máscara de un modelo específico, campos opcionales\n",
        "\n",
        "Referencias:\n",
        "- Gonzalez, R.C. & Woods, R.E. (2018). Digital Image Processing (4th ed.)\n",
        "- Lin, T.Y., et al. (2014). Microsoft COCO: Common Objects in Context\n",
        "================================================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field, asdict\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "X9xwcErDoNmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar si estamos en Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"\\nEntorno: Google Colab detectado\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"\\nEntorno: Ejecución local (no Colab)\")\n",
        "\n",
        "# Montar Google Drive si estamos en Colab\n",
        "if IN_COLAB:\n",
        "    print(\"\\nMontando Google Drive...\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    from google.colab import drive\n",
        "\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"Google Drive montado correctamente en: /content/drive\")\n",
        "\n",
        "        # Verificar que el directorio TFM existe\n",
        "        ruta_tfm_check = Path(\"/content/drive/MyDrive/TFM\")\n",
        "        if ruta_tfm_check.exists():\n",
        "            print(f\"Directorio TFM encontrado: {ruta_tfm_check}\")\n",
        "        else:\n",
        "            print(f\"\\nADVERTENCIA: No se encontró directorio TFM en {ruta_tfm_check}\")\n",
        "            print(\"Verifica que la estructura del proyecto esté correctamente ubicada\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR montando Google Drive: {e}\")\n",
        "        print(\"Verifica permisos y conexión a Drive\")\n",
        "        raise\n",
        "else:\n",
        "    print(\"\\nEjecución local - saltando montaje de Drive\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEr4cZquuOb3",
        "outputId": "ab54f5a5-e2d3-4616-e068-41cd278adb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entorno: Google Colab detectado\n",
            "\n",
            "Montando Google Drive...\n",
            "--------------------------------------------------------------------------------\n",
            "Mounted at /content/drive\n",
            "Google Drive montado correctamente en: /content/drive\n",
            "Directorio TFM encontrado: /content/drive/MyDrive/TFM\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONFIGURACIÓN DE LOGGING\n",
        "# =============================================================================\n",
        "\n",
        "def configurar_logging() -> logging.Logger:\n",
        "    \"\"\"\n",
        "    Configura sistema de logging para la Fase 1.\n",
        "\n",
        "    Elimina handlers previos para evitar duplicación de mensajes\n",
        "    en Google Colab cuando se re-ejecutan las celdas.\n",
        "\n",
        "    Returns:\n",
        "        Logger configurado con formato académico\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger('Fase1_Integracion')\n",
        "\n",
        "    # CRÍTICO: Eliminar todos los handlers previos para evitar duplicación\n",
        "    # Esto es necesario en Google Colab donde las celdas se re-ejecutan\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    # Handler para consola\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "\n",
        "    # Formato académico sin emoticones\n",
        "    formatter = logging.Formatter(\n",
        "        '[%(asctime)s] %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    console_handler.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    # Evitar que los mensajes se propaguen al logger raíz\n",
        "    logger.propagate = False\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "logger = configurar_logging()"
      ],
      "metadata": {
        "id": "_eF1PGsRoU8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONFIGURACIÓN DE RUTAS\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ConfiguracionRutas:\n",
        "    \"\"\"\n",
        "    Configuración centralizada de rutas del proyecto.\n",
        "\n",
        "    Descubre automáticamente la estructura de directorios en TFM/ en lugar\n",
        "    de asumir nombres fijos.\n",
        "\n",
        "    Attributes:\n",
        "        ruta_base: Directorio raíz del proyecto TFM\n",
        "        ruta_imagenes: Directorio con imágenes originales\n",
        "        ruta_caracteristicas: Directorio con JSONs de características\n",
        "        ruta_gt: Directorio con máscaras ground truth de CVAT\n",
        "        ruta_modelos: Directorio base de resultados de modelos\n",
        "        ruta_analisis: Directorio de salida para análisis (Notebook 3)\n",
        "    \"\"\"\n",
        "    ruta_base: Path\n",
        "    ruta_imagenes: Path\n",
        "    ruta_caracteristicas: Path\n",
        "    ruta_gt: Path\n",
        "    ruta_modelos: Path\n",
        "    ruta_analisis: Path\n",
        "\n",
        "    @classmethod\n",
        "    def descubrir_desde_base(cls, ruta_base: str) -> 'ConfiguracionRutas':\n",
        "        \"\"\"\n",
        "        Descubre automáticamente la estructura de directorios en TFM.\n",
        "\n",
        "        Busca directorios que cumplan ciertos patrones en lugar de asumir\n",
        "        nombres exactos. Esto hace el código más robusto ante variaciones\n",
        "        en la estructura del proyecto.\n",
        "\n",
        "        Args:\n",
        "            ruta_base: Ruta al directorio TFM (ej: '/content/drive/MyDrive/TFM')\n",
        "\n",
        "        Returns:\n",
        "            ConfiguracionRutas con rutas descubiertas\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: Si no se encuentra algún directorio crítico\n",
        "        \"\"\"\n",
        "        base = Path(ruta_base)\n",
        "\n",
        "        if not base.exists():\n",
        "            raise FileNotFoundError(\n",
        "                f\"Directorio base no existe: {base}\\n\"\n",
        "                f\"Verifica que la ruta al proyecto TFM sea correcta.\"\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Escaneando estructura en: {base}\")\n",
        "\n",
        "        # Descubrir directorio de imágenes\n",
        "        ruta_imagenes = cls._buscar_directorio(\n",
        "            base,\n",
        "            ['0_Imagenes', 'Imagenes', '0_imagenes', 'imagenes'],\n",
        "            'Imágenes originales'\n",
        "        )\n",
        "\n",
        "        # Descubrir directorio de características\n",
        "        ruta_caracteristicas = cls._buscar_directorio(\n",
        "            base,\n",
        "            ['1_Caracteristicas', 'Caracteristicas', '1_caracteristicas', 'caracteristicas'],\n",
        "            'Características fotográficas'\n",
        "        )\n",
        "\n",
        "        # Descubrir directorio de ground truth\n",
        "        ruta_gt = None\n",
        "        posibles_gt = [\n",
        "            base / '0_Imagenes_CVAT' / 'ground_truth_masks',\n",
        "            base / 'Imagenes_CVAT' / 'ground_truth_masks',\n",
        "            base / '0_Imagenes_CVAT' / 'ground_truth',\n",
        "            base / 'ground_truth_masks',\n",
        "            base / 'ground_truth',\n",
        "            base / 'GT'\n",
        "        ]\n",
        "\n",
        "        for ruta in posibles_gt:\n",
        "            if ruta.exists() and ruta.is_dir():\n",
        "                ruta_gt = ruta\n",
        "                logger.info(f\"  Ground Truth encontrado en: {ruta.relative_to(base)}\")\n",
        "                break\n",
        "\n",
        "        if ruta_gt is None:\n",
        "            raise FileNotFoundError(\n",
        "                f\"No se encontró directorio de Ground Truth.\\n\"\n",
        "                f\"Buscado en: {[str(p.relative_to(base)) for p in posibles_gt]}\\n\"\n",
        "                f\"Verifica que las máscaras GT estén en alguna de estas ubicaciones.\"\n",
        "            )\n",
        "\n",
        "        # Descubrir directorio de modelos\n",
        "        ruta_modelos = cls._buscar_directorio(\n",
        "            base,\n",
        "            ['2_Modelos', 'Modelos', '2_modelos', 'modelos'],\n",
        "            'Resultados de modelos'\n",
        "        )\n",
        "\n",
        "        # Crear directorio de análisis si no existe\n",
        "        ruta_analisis = base / '3_Analisis'\n",
        "        ruta_analisis.mkdir(parents=True, exist_ok=True)\n",
        "        logger.info(f\"  Directorio de análisis: {ruta_analisis.relative_to(base)}\")\n",
        "\n",
        "        return cls(\n",
        "            ruta_base=base,\n",
        "            ruta_imagenes=ruta_imagenes,\n",
        "            ruta_caracteristicas=ruta_caracteristicas,\n",
        "            ruta_gt=ruta_gt,\n",
        "            ruta_modelos=ruta_modelos,\n",
        "            ruta_analisis=ruta_analisis\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _buscar_directorio(base: Path, nombres_posibles: List[str], descripcion: str) -> Path:\n",
        "        \"\"\"\n",
        "        Busca un directorio con varios nombres posibles.\n",
        "\n",
        "        Args:\n",
        "            base: Directorio base donde buscar\n",
        "            nombres_posibles: Lista de nombres a probar\n",
        "            descripcion: Descripción para mensajes de error\n",
        "\n",
        "        Returns:\n",
        "            Path al directorio encontrado\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: Si no se encuentra ninguno\n",
        "        \"\"\"\n",
        "        for nombre in nombres_posibles:\n",
        "            ruta = base / nombre\n",
        "            if ruta.exists() and ruta.is_dir():\n",
        "                logger.info(f\"  {descripcion} encontrado en: {ruta.relative_to(base)}\")\n",
        "                return ruta\n",
        "\n",
        "        # Si llegamos aquí, no se encontró\n",
        "        raise FileNotFoundError(\n",
        "            f\"No se encontró directorio de {descripcion}.\\n\"\n",
        "            f\"Buscado en: {nombres_posibles}\\n\"\n",
        "            f\"Verifica la estructura del proyecto en: {base}\"\n",
        "        )\n",
        "\n",
        "    def validar(self) -> Tuple[bool, List[str]]:\n",
        "        \"\"\"\n",
        "        Valida existencia de directorios críticos.\n",
        "\n",
        "        Returns:\n",
        "            Tupla (todas_existen, lista_errores)\n",
        "        \"\"\"\n",
        "        errores = []\n",
        "\n",
        "        rutas_criticas = [\n",
        "            ('Base', self.ruta_base),\n",
        "            ('Imágenes', self.ruta_imagenes),\n",
        "            ('Características', self.ruta_caracteristicas),\n",
        "            ('Ground Truth', self.ruta_gt),\n",
        "            ('Modelos', self.ruta_modelos)\n",
        "        ]\n",
        "\n",
        "        for nombre, ruta in rutas_criticas:\n",
        "            if not ruta.exists():\n",
        "                errores.append(f\"Ruta {nombre} no existe: {ruta}\")\n",
        "\n",
        "        return len(errores) == 0, errores\n",
        "\n",
        "    def mostrar_estructura(self):\n",
        "        \"\"\"Muestra la estructura de rutas descubiertas.\"\"\"\n",
        "        print(\"\\nEstructura de rutas descubiertas:\")\n",
        "        print(f\"  Base:             {self.ruta_base}\")\n",
        "        print(f\"  Imágenes:         {self.ruta_imagenes}\")\n",
        "        print(f\"  Características:  {self.ruta_caracteristicas}\")\n",
        "        print(f\"  Ground Truth:     {self.ruta_gt}\")\n",
        "        print(f\"  Modelos:          {self.ruta_modelos}\")\n",
        "        print(f\"  Análisis:         {self.ruta_analisis}\")"
      ],
      "metadata": {
        "id": "p97dllLdooKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ESTRUCTURA DE DATOS CONSOLIDADA\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class DatosFotografia:\n",
        "    \"\"\"\n",
        "    Estructura consolidada de todos los datos de una fotografía.\n",
        "\n",
        "    Integra características fotográficas, ground truth, máscaras de todos\n",
        "    los modelos evaluados y metadata de ejecución.\n",
        "\n",
        "    Attributes:\n",
        "        foto_id: Identificador único (nombre sin extensión, ej: '_DSC0002')\n",
        "        ruta_imagen: Path a imagen original\n",
        "        ruta_json_caracteristicas: Path a JSON de características\n",
        "        ruta_gt: Path a máscara ground truth NPZ\n",
        "\n",
        "        metadatos: Información de archivo (dimensiones, tamaño)\n",
        "        exif_disponible: Flag indicando si EXIF está presente\n",
        "        exif: Diccionario con metadatos EXIF (vacío si no disponible)\n",
        "        color: Estadísticas de color (RGB, HSV, LAB)\n",
        "        saliencia: Análisis de saliencia visual\n",
        "        calidad: Métricas de calidad de imagen\n",
        "\n",
        "        gt_disponible: Flag indicando si ground truth existe\n",
        "        gt_mascara: Máscara ground truth binaria (H, W)\n",
        "        gt_shape: Dimensiones de la máscara GT\n",
        "\n",
        "        mascaras_modelos: Dict {codigo_config: datos_mascara}\n",
        "        metadata_modelos: Dict {codigo_config: metadata_ejecucion}\n",
        "\n",
        "        timestamp_integracion: Timestamp de procesamiento\n",
        "        num_configs_disponibles: Número de configuraciones de modelos cargadas\n",
        "    \"\"\"\n",
        "    foto_id: str\n",
        "\n",
        "    # Rutas de archivos\n",
        "    ruta_imagen: Path\n",
        "    ruta_json_caracteristicas: Path\n",
        "    ruta_gt: Optional[Path]\n",
        "\n",
        "    # Características intrínsecas\n",
        "    metadatos: Dict[str, Any]\n",
        "    exif_disponible: bool\n",
        "    exif: Dict[str, Any]\n",
        "    color: Dict[str, Any]\n",
        "    saliencia: Dict[str, Any]\n",
        "    calidad: Dict[str, Any]\n",
        "\n",
        "    # Ground Truth\n",
        "    gt_disponible: bool\n",
        "    gt_mascara: Optional[np.ndarray]\n",
        "    gt_shape: Optional[Tuple[int, int]]\n",
        "\n",
        "    # Máscaras y metadata de modelos\n",
        "    mascaras_modelos: Dict[str, Dict[str, Any]]\n",
        "    metadata_modelos: Dict[str, Dict[str, Any]]\n",
        "\n",
        "    # Metadata de procesamiento\n",
        "    timestamp_integracion: str\n",
        "    num_configs_disponibles: int\n",
        "\n",
        "    def a_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Convierte a diccionario serializable (sin arrays numpy).\n",
        "\n",
        "        Returns:\n",
        "            Dict con estructura completa excepto máscaras numpy\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'foto_id': self.foto_id,\n",
        "            'rutas': {\n",
        "                'imagen': str(self.ruta_imagen),\n",
        "                'caracteristicas': str(self.ruta_json_caracteristicas),\n",
        "                'ground_truth': str(self.ruta_gt) if self.ruta_gt else None\n",
        "            },\n",
        "            'metadatos': self.metadatos,\n",
        "            'exif_disponible': self.exif_disponible,\n",
        "            'exif': self.exif,\n",
        "            'color': self.color,\n",
        "            'saliencia': self.saliencia,\n",
        "            'calidad': self.calidad,\n",
        "            'ground_truth': {\n",
        "                'disponible': self.gt_disponible,\n",
        "                'shape': self.gt_shape\n",
        "            },\n",
        "            'modelos': {\n",
        "                'num_configuraciones': self.num_configs_disponibles,\n",
        "                'configuraciones': list(self.mascaras_modelos.keys()),\n",
        "                'metadata_disponible': list(self.metadata_modelos.keys())\n",
        "            },\n",
        "            'timestamp': self.timestamp_integracion\n",
        "        }\n"
      ],
      "metadata": {
        "id": "0SEcjikhoqvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# LECTOR DE CARACTERÍSTICAS FOTOGRÁFICAS\n",
        "# =============================================================================\n",
        "\n",
        "class LectorCaracteristicas:\n",
        "    \"\"\"\n",
        "    Carga y procesa características fotográficas del Notebook 00.\n",
        "\n",
        "    Implementa manejo defensivo de EXIF que puede estar vacío en las\n",
        "    fotografías reales del dataset. Busca JSONs en subdirectorio json/\n",
        "    si existe.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ruta_caracteristicas: Path):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ruta_caracteristicas: Path al directorio de JSONs de características\n",
        "        \"\"\"\n",
        "        self.ruta_caracteristicas = ruta_caracteristicas\n",
        "\n",
        "        # Verificar si los JSONs están en subdirectorio 'json/'\n",
        "        self.ruta_json_subdir = ruta_caracteristicas / 'json'\n",
        "        self.usar_subdirectorio = self.ruta_json_subdir.exists()\n",
        "\n",
        "        if self.usar_subdirectorio:\n",
        "            logger.info(f\"  JSONs encontrados en subdirectorio: {self.ruta_json_subdir.relative_to(ruta_caracteristicas.parent)}\")\n",
        "\n",
        "    def cargar(self, foto_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Carga características de una fotografía desde JSON.\n",
        "\n",
        "        Busca primero en subdirectorio json/, luego en el directorio raíz.\n",
        "\n",
        "        Args:\n",
        "            foto_id: Identificador de la foto (ej: '_DSC0002')\n",
        "\n",
        "        Returns:\n",
        "            Dict con características extraídas o None si error crítico\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: Si el JSON no existe (error crítico)\n",
        "        \"\"\"\n",
        "        # Intentar primero en subdirectorio json/\n",
        "        if self.usar_subdirectorio:\n",
        "            ruta_json = self.ruta_json_subdir / f\"{foto_id}_caracteristicas.json\"\n",
        "        else:\n",
        "            ruta_json = self.ruta_caracteristicas / f\"{foto_id}_caracteristicas.json\"\n",
        "\n",
        "        # Si no existe, intentar en el directorio raíz\n",
        "        if not ruta_json.exists():\n",
        "            ruta_alternativa = self.ruta_caracteristicas / f\"{foto_id}_caracteristicas.json\"\n",
        "            if ruta_alternativa.exists():\n",
        "                ruta_json = ruta_alternativa\n",
        "            else:\n",
        "                # Último intento: buscar en subdirectorio json/\n",
        "                ruta_json_alt = self.ruta_caracteristicas / 'json' / f\"{foto_id}_caracteristicas.json\"\n",
        "                if ruta_json_alt.exists():\n",
        "                    ruta_json = ruta_json_alt\n",
        "                else:\n",
        "                    raise FileNotFoundError(\n",
        "                        f\"JSON de características no encontrado: {ruta_json}\\n\"\n",
        "                        f\"También se buscó en: {ruta_alternativa}\\n\"\n",
        "                        f\"Y en: {ruta_json_alt}\\n\"\n",
        "                        f\"Este es un error crítico. La foto {foto_id} debe tener \"\n",
        "                        f\"características generadas por Notebook 00.\"\n",
        "                    )\n",
        "\n",
        "        try:\n",
        "            with open(ruta_json, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Extraer campos con valores por defecto\n",
        "            caracteristicas = {\n",
        "                'ruta_json': ruta_json,\n",
        "                'metadatos': self._extraer_metadatos(data),\n",
        "                'exif_disponible': self._verificar_exif(data),\n",
        "                'exif': self._extraer_exif(data),\n",
        "                'color': self._extraer_color(data),\n",
        "                'saliencia': self._extraer_saliencia(data),\n",
        "                'calidad': self._extraer_calidad(data)\n",
        "            }\n",
        "\n",
        "            return caracteristicas\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(\n",
        "                f\"JSON corrupto para {foto_id}: {e}\\n\"\n",
        "                f\"Este es un error crítico. El JSON debe ser válido.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error inesperado cargando {foto_id}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _extraer_metadatos(self, data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Extrae metadatos de archivo con manejo de campos faltantes.\"\"\"\n",
        "        metadatos_raw = data.get('metadatos_archivo', {})\n",
        "\n",
        "        return {\n",
        "            'nombre': metadatos_raw.get('nombre_archivo', 'desconocido'),\n",
        "            'ancho': metadatos_raw.get('ancho_original', 0),\n",
        "            'alto': metadatos_raw.get('alto_original', 0),\n",
        "            'tamaño_mb': metadatos_raw.get('tamaño_mb', 0.0),\n",
        "            'fecha_modificacion': metadatos_raw.get('fecha_modificacion', None),\n",
        "            'modo_color': metadatos_raw.get('modo_color', 'RGB')\n",
        "        }\n",
        "\n",
        "    def _verificar_exif(self, data: Dict) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica si hay datos EXIF disponibles.\n",
        "\n",
        "        En el dataset real, metadatos_exif es un diccionario vacío.\n",
        "        \"\"\"\n",
        "        exif = data.get('metadatos_exif', {})\n",
        "        return len(exif) > 0\n",
        "\n",
        "    def _extraer_exif(self, data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extrae metadatos EXIF con manejo defensivo.\n",
        "\n",
        "        Nota: En el dataset real, este diccionario está vacío.\n",
        "        Se mantiene la estructura para compatibilidad futura.\n",
        "        \"\"\"\n",
        "        exif = data.get('metadatos_exif', {})\n",
        "\n",
        "        # Si está vacío, retornar estructura con valores None\n",
        "        if len(exif) == 0:\n",
        "            return {\n",
        "                'aperture': None,\n",
        "                'iso': None,\n",
        "                'shutter_speed': None,\n",
        "                'focal_length': None,\n",
        "                'camera': None,\n",
        "                'lens': None\n",
        "            }\n",
        "\n",
        "        # Si hay datos, extraer con valores por defecto\n",
        "        return {\n",
        "            'aperture': exif.get('aperture', None),\n",
        "            'iso': exif.get('iso', None),\n",
        "            'shutter_speed': exif.get('shutter_speed', None),\n",
        "            'focal_length': exif.get('focal_length', None),\n",
        "            'camera': exif.get('camera', None),\n",
        "            'lens': exif.get('lens', None)\n",
        "        }\n",
        "\n",
        "    def _extraer_color(self, data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Extrae estadísticas de color.\"\"\"\n",
        "        color_data = data.get('estadisticas_color', {})\n",
        "\n",
        "        rgb = color_data.get('rgb', {})\n",
        "        hsv = color_data.get('hsv', {})\n",
        "        lab = color_data.get('lab', {})\n",
        "        global_stats = color_data.get('global', {})\n",
        "\n",
        "        return {\n",
        "            'rgb_mean': rgb.get('mean', [0, 0, 0]),\n",
        "            'rgb_stddev': rgb.get('stddev', [0, 0, 0]),\n",
        "            'hsv_saturation_mean': hsv.get('saturation_mean', 0.0),\n",
        "            'hsv_value_mean': hsv.get('value_mean', 0.0),\n",
        "            'lab_l_mean': lab.get('l_mean', 0.0),\n",
        "            'brillo_promedio': global_stats.get('brillo_promedio', 0.0),\n",
        "            'contraste': global_stats.get('contraste', 0.0),\n",
        "            'entropia': global_stats.get('entropia', 0.0)\n",
        "        }\n",
        "\n",
        "    def _extraer_saliencia(self, data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Extrae análisis de saliencia visual.\"\"\"\n",
        "        saliencia_data = data.get('saliencia_visual', {})\n",
        "        centroide = saliencia_data.get('centroide', {})\n",
        "\n",
        "        return {\n",
        "            'centroide_x': centroide.get('centroide_x_normalizado', 0.5),\n",
        "            'centroide_y': centroide.get('centroide_y_normalizado', 0.5),\n",
        "            'dist_centro': centroide.get('distancia_desde_centro', 0.0),\n",
        "            'area_saliente_pct': centroide.get('area_saliente_porcentaje', 0.0)\n",
        "        }\n",
        "\n",
        "    def _extraer_calidad(self, data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Extrae métricas de calidad de imagen.\"\"\"\n",
        "        calidad_data = data.get('calidad', {})\n",
        "\n",
        "        nitidez = calidad_data.get('nitidez', {})\n",
        "        ruido = calidad_data.get('ruido', {})\n",
        "        exposicion = calidad_data.get('exposicion', {})\n",
        "\n",
        "        return {\n",
        "            'nitidez_laplacian': nitidez.get('laplacian', 0.0),\n",
        "            'nitidez_tenengrad': nitidez.get('tenengrad', 0.0),\n",
        "            'snr_db': ruido.get('snr_db', 0.0),\n",
        "            'brillo': exposicion.get('brillo_medio', 0.0),\n",
        "            'sobre_expuesto_pct': exposicion.get('sobre_expuesto_pct', 0.0),\n",
        "            'sub_expuesto_pct': exposicion.get('sub_expuesto_pct', 0.0)\n",
        "        }"
      ],
      "metadata": {
        "id": "Kk9ETmwFo9dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# LECTOR DE GROUND TRUTH\n",
        "# =============================================================================\n",
        "\n",
        "class LectorGroundTruth:\n",
        "    \"\"\"\n",
        "    Carga máscaras ground truth anotadas manualmente en CVAT.\n",
        "\n",
        "    Las máscaras GT son críticas para el análisis comparativo. Su ausencia\n",
        "    genera error crítico que detiene la ejecución.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ruta_gt: Path):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ruta_gt: Path al directorio de máscaras ground truth NPZ\n",
        "        \"\"\"\n",
        "        self.ruta_gt = ruta_gt\n",
        "\n",
        "    def cargar(self, foto_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Carga máscara ground truth para una fotografía.\n",
        "\n",
        "        Args:\n",
        "            foto_id: Identificador de la foto\n",
        "\n",
        "        Returns:\n",
        "            Dict con máscara y metadata\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: Si la máscara GT no existe (error crítico)\n",
        "        \"\"\"\n",
        "        ruta_npz = self.ruta_gt / f\"{foto_id}_gt.npz\"\n",
        "\n",
        "        if not ruta_npz.exists():\n",
        "            raise FileNotFoundError(\n",
        "                f\"Ground Truth no encontrado: {ruta_npz}\\n\"\n",
        "                f\"Este es un error crítico. Todas las fotografías deben tener \"\n",
        "                f\"anotación manual en CVAT para evaluación cuantitativa.\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            data = np.load(ruta_npz)\n",
        "\n",
        "            # Extraer máscara (campo 'masks' según generación CVAT)\n",
        "            if 'masks' not in data:\n",
        "                raise ValueError(\n",
        "                    f\"NPZ de GT no contiene campo 'masks': {ruta_npz}\\n\"\n",
        "                    f\"Campos disponibles: {list(data.keys())}\"\n",
        "                )\n",
        "\n",
        "            mascara = data['masks']\n",
        "\n",
        "            # Convertir a binaria si es necesario (puede estar en [0, 1] float)\n",
        "            if mascara.dtype == np.float32 or mascara.dtype == np.float64:\n",
        "                mascara_binaria = (mascara > 0.5).astype(np.uint8)\n",
        "            else:\n",
        "                mascara_binaria = mascara.astype(np.uint8)\n",
        "\n",
        "            # Validar que sea binaria\n",
        "            valores_unicos = np.unique(mascara_binaria)\n",
        "            if not np.array_equal(valores_unicos, [0, 1]) and not np.array_equal(valores_unicos, [0]) and not np.array_equal(valores_unicos, [1]):\n",
        "                logger.warning(\n",
        "                    f\"Máscara GT de {foto_id} no es estrictamente binaria. \"\n",
        "                    f\"Valores únicos: {valores_unicos}\"\n",
        "                )\n",
        "\n",
        "            return {\n",
        "                'existe': True,\n",
        "                'mascara': mascara_binaria,\n",
        "                'shape': mascara_binaria.shape,\n",
        "                'ruta': ruta_npz,\n",
        "                'area_pixels': int(np.sum(mascara_binaria)),\n",
        "                'cobertura_pct': float(np.mean(mascara_binaria) * 100)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                f\"Error crítico cargando GT de {foto_id}: {e}\"\n",
        "            )\n"
      ],
      "metadata": {
        "id": "P9PoptKw1Azl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ESCANEO DE CONFIGURACIONES DE MODELOS\n",
        "# =============================================================================\n",
        "\n",
        "class EscanerConfiguraciones:\n",
        "    \"\"\"\n",
        "    Escanea dinámicamente configuraciones de modelos disponibles.\n",
        "\n",
        "    Busca recursivamente carpetas 'mascaras' en la estructura de cada modelo,\n",
        "    ya que diferentes modelos tienen estructuras de directorios distintas.\n",
        "    \"\"\"\n",
        "\n",
        "    MODELOS = ['mask2former', 'oneformer', 'sam2', 'yolov8', 'bodypix']\n",
        "\n",
        "    # Mapeo de códigos cortos para nomenclatura GitHub-safe\n",
        "    CODIGOS_MODELO = {\n",
        "        'mask2former': 'm2f',\n",
        "        'oneformer': 'o1f',\n",
        "        'sam2': 'sam',\n",
        "        'yolov8': 'yolo',\n",
        "        'bodypix': 'bpx'\n",
        "    }\n",
        "\n",
        "    def __init__(self, ruta_modelos: Path):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ruta_modelos: Path al directorio base de modelos (/TFM/2_Modelos)\n",
        "        \"\"\"\n",
        "        self.ruta_modelos = ruta_modelos\n",
        "\n",
        "    def escanear(self) -> Dict[str, List[Tuple[str, Path]]]:\n",
        "        \"\"\"\n",
        "        Escanea directorios de modelos y retorna configuraciones disponibles.\n",
        "\n",
        "        Returns:\n",
        "            Dict {modelo: [(nombre_config, ruta_mascaras), ...]}\n",
        "        \"\"\"\n",
        "        configs_por_modelo = {}\n",
        "\n",
        "        for modelo in self.MODELOS:\n",
        "            ruta_modelo = self.ruta_modelos / modelo\n",
        "\n",
        "            if not ruta_modelo.exists():\n",
        "                logger.warning(\n",
        "                    f\"Directorio de modelo no existe: {modelo}. \"\n",
        "                    f\"No se cargarán máscaras de este modelo.\"\n",
        "                )\n",
        "                configs_por_modelo[modelo] = []\n",
        "                continue\n",
        "\n",
        "            # Buscar recursivamente carpetas 'mascaras'\n",
        "            configs = self._buscar_configuraciones_recursivo(ruta_modelo, modelo)\n",
        "\n",
        "            configs_por_modelo[modelo] = configs\n",
        "\n",
        "            logger.info(\n",
        "                f\"Modelo {modelo}: {len(configs)} configuraciones encontradas\"\n",
        "            )\n",
        "\n",
        "            # Mostrar cada configuración encontrada\n",
        "            if len(configs) > 0 and len(configs) <= 20:  # Solo si no son demasiadas\n",
        "                logger.info(f\"  Configuraciones de {modelo}:\")\n",
        "                for nombre_config, ruta_mascaras in configs:\n",
        "                    # Mostrar ruta relativa desde el modelo\n",
        "                    ruta_relativa = ruta_mascaras.relative_to(ruta_modelo)\n",
        "                    logger.info(f\"    - {nombre_config}: {ruta_relativa}\")\n",
        "\n",
        "        return configs_por_modelo\n",
        "\n",
        "    def _buscar_configuraciones_recursivo(self, ruta_base: Path, modelo: str, max_depth: int = 4) -> List[Tuple[str, Path]]:\n",
        "        \"\"\"\n",
        "        Busca recursivamente carpetas 'mascaras' hasta una profundidad máxima.\n",
        "\n",
        "        Args:\n",
        "            ruta_base: Directorio base del modelo\n",
        "            modelo: Nombre del modelo\n",
        "            max_depth: Profundidad máxima de búsqueda\n",
        "\n",
        "        Returns:\n",
        "            Lista de tuplas (nombre_config, ruta_mascaras)\n",
        "        \"\"\"\n",
        "        configs = []\n",
        "\n",
        "        def buscar_recursivo(ruta_actual: Path, profundidad: int, ruta_relativa: str = \"\"):\n",
        "            if profundidad > max_depth:\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                for item in ruta_actual.iterdir():\n",
        "                    if not item.is_dir():\n",
        "                        continue\n",
        "\n",
        "                    # Si encontramos carpeta 'mascaras'\n",
        "                    if item.name == 'mascaras':\n",
        "                        # Generar nombre de configuración desde la ruta relativa\n",
        "                        if ruta_relativa:\n",
        "                            nombre_config = ruta_relativa.replace('/', '_')\n",
        "                        else:\n",
        "                            # Si está directamente en el modelo\n",
        "                            nombre_config = 'default'\n",
        "\n",
        "                        configs.append((nombre_config, item))\n",
        "                        continue\n",
        "\n",
        "                    # Continuar buscando en subdirectorios\n",
        "                    nueva_ruta_relativa = f\"{ruta_relativa}/{item.name}\" if ruta_relativa else item.name\n",
        "                    buscar_recursivo(item, profundidad + 1, nueva_ruta_relativa)\n",
        "            except PermissionError:\n",
        "                # Ignorar directorios sin permisos\n",
        "                pass\n",
        "\n",
        "        buscar_recursivo(ruta_base, 0)\n",
        "        return configs\n",
        "\n",
        "    def generar_codigo_config(self, modelo: str, config: str) -> str:\n",
        "        \"\"\"\n",
        "        Genera código corto GitHub-safe para una configuración.\n",
        "\n",
        "        Args:\n",
        "            modelo: Nombre del modelo\n",
        "            config: Nombre de la configuración\n",
        "\n",
        "        Returns:\n",
        "            Código único, ej: \"m2f_large_ade_baja\"\n",
        "        \"\"\"\n",
        "        codigo_modelo = self.CODIGOS_MODELO.get(modelo, modelo[:3])\n",
        "\n",
        "        # Limpiar nombre de configuración\n",
        "        config_limpio = config.replace('_resultados', '').replace('resultados_', '')\n",
        "\n",
        "        # Combinar y truncar si es necesario\n",
        "        codigo_completo = f\"{codigo_modelo}_{config_limpio}\"\n",
        "\n",
        "        if len(codigo_completo) > 50:\n",
        "            max_config = 50 - len(codigo_modelo) - 1\n",
        "            config_truncado = config_limpio[:max_config]\n",
        "            codigo_completo = f\"{codigo_modelo}_{config_truncado}\"\n",
        "\n",
        "        return codigo_completo"
      ],
      "metadata": {
        "id": "SwkVoFYQpUvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# LECTOR DE MÁSCARAS DE MODELOS\n",
        "# =============================================================================\n",
        "\n",
        "class LectorMascarasModelos:\n",
        "    \"\"\"\n",
        "    Carga máscaras NPZ de modelos con manejo robusto de estructuras heterogéneas.\n",
        "\n",
        "    Cada modelo guarda diferentes campos en sus archivos NPZ. Esta clase\n",
        "    implementa detección automática de campos disponibles y extracción\n",
        "    defensiva con try/except.\n",
        "\n",
        "    Criterio de error: Falta de máscara de un modelo específico es WARNING,\n",
        "    no ERROR CRÍTICO (puede que un modelo no haya procesado esa foto).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ruta_modelos: Path, escaner: EscanerConfiguraciones):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ruta_modelos: Path al directorio base de modelos\n",
        "            escaner: Instancia de EscanerConfiguraciones para códigos\n",
        "        \"\"\"\n",
        "        self.ruta_modelos = ruta_modelos\n",
        "        self.escaner = escaner\n",
        "        # Guardar mapeo de configs a rutas de máscaras\n",
        "        self.configs_rutas = {}\n",
        "\n",
        "    def registrar_configuracion(self, modelo: str, nombre_config: str, ruta_mascaras: Path):\n",
        "        \"\"\"\n",
        "        Registra la ruta de máscaras para una configuración.\n",
        "\n",
        "        Args:\n",
        "            modelo: Nombre del modelo\n",
        "            nombre_config: Nombre de la configuración\n",
        "            ruta_mascaras: Path al directorio de máscaras\n",
        "        \"\"\"\n",
        "        codigo = self.escaner.generar_codigo_config(modelo, nombre_config)\n",
        "        self.configs_rutas[codigo] = {\n",
        "            'modelo': modelo,\n",
        "            'config': nombre_config,\n",
        "            'ruta_mascaras': ruta_mascaras\n",
        "        }\n",
        "\n",
        "    def cargar(self, codigo_config: str, foto_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Carga máscara NPZ usando el código de configuración.\n",
        "\n",
        "        Args:\n",
        "            codigo_config: Código de configuración generado\n",
        "            foto_id: Identificador de la foto\n",
        "\n",
        "        Returns:\n",
        "            Dict con máscara y metadata disponible, o None si no existe\n",
        "        \"\"\"\n",
        "        if codigo_config not in self.configs_rutas:\n",
        "            logger.warning(f\"Configuración no registrada: {codigo_config}\")\n",
        "            return None\n",
        "\n",
        "        config_info = self.configs_rutas[codigo_config]\n",
        "        ruta_mascaras = config_info['ruta_mascaras']\n",
        "        modelo = config_info['modelo']\n",
        "\n",
        "        # Buscar el archivo NPZ para esta foto\n",
        "        ruta_npz = self._buscar_npz(ruta_mascaras, foto_id)\n",
        "\n",
        "        if ruta_npz is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            data = np.load(ruta_npz, allow_pickle=True)\n",
        "\n",
        "            # Intentar extraer máscara con diferentes nombres posibles\n",
        "            mascara = self._extraer_mascara(data, modelo)\n",
        "\n",
        "            if mascara is None:\n",
        "                logger.warning(\n",
        "                    f\"No se pudo extraer máscara de {codigo_config}/{foto_id}. \"\n",
        "                    f\"Campos disponibles: {list(data.keys())}\"\n",
        "                )\n",
        "                return None\n",
        "\n",
        "            # Convertir a binaria uint8 si no lo es\n",
        "            if mascara.dtype != np.uint8:\n",
        "                mascara = (mascara > 0.5).astype(np.uint8)\n",
        "\n",
        "            # Construir resultado con campos opcionales\n",
        "            resultado = {\n",
        "                'mascara': mascara,\n",
        "                'shape': mascara.shape,\n",
        "                'ruta': ruta_npz,\n",
        "                'campos_disponibles': list(data.keys())\n",
        "            }\n",
        "\n",
        "            # Extraer campos opcionales (no críticos)\n",
        "            resultado['bbox'] = self._extraer_bbox(data)\n",
        "            resultado['score'] = self._extraer_score(data)\n",
        "            resultado['area'] = self._extraer_area(data, mascara)\n",
        "\n",
        "            return resultado\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(\n",
        "                f\"Error cargando máscara {codigo_config}/{foto_id}: {e}\"\n",
        "            )\n",
        "            return None\n",
        "\n",
        "    def _buscar_npz(self, ruta_mascaras: Path, foto_id: str) -> Optional[Path]:\n",
        "        \"\"\"\n",
        "        Busca el archivo NPZ de una foto en múltiples ubicaciones posibles.\n",
        "\n",
        "        Args:\n",
        "            ruta_mascaras: Path al directorio de máscaras\n",
        "            foto_id: Identificador de la foto\n",
        "\n",
        "        Returns:\n",
        "            Path al NPZ encontrado o None\n",
        "        \"\"\"\n",
        "        # Intento 1: Directamente en mascaras/{foto_id}.npz\n",
        "        ruta_directa = ruta_mascaras / f\"{foto_id}.npz\"\n",
        "        if ruta_directa.exists():\n",
        "            return ruta_directa\n",
        "\n",
        "        # Intento 2: En subdirectorio con nombre de foto\n",
        "        subdir_foto = ruta_mascaras / foto_id\n",
        "        if subdir_foto.exists() and subdir_foto.is_dir():\n",
        "            # Buscar NPZ directamente en el subdirectorio\n",
        "            npz_directo = subdir_foto / f\"{foto_id}.npz\"\n",
        "            if npz_directo.exists():\n",
        "                return npz_directo\n",
        "\n",
        "            npz_simple = subdir_foto / \"mascara.npz\"\n",
        "            if npz_simple.exists():\n",
        "                return npz_simple\n",
        "\n",
        "            # Buscar en subdirectorios de umbrales (Mask2Former)\n",
        "            for item in subdir_foto.iterdir():\n",
        "                if item.is_dir():\n",
        "                    # Buscar primer NPZ en este directorio\n",
        "                    npz_files = list(item.glob(\"*.npz\"))\n",
        "                    if npz_files:\n",
        "                        # Tomar el primer archivo (o el que tenga mayor número)\n",
        "                        return sorted(npz_files)[0]\n",
        "\n",
        "        # Intento 3: Buscar recursivamente (último recurso)\n",
        "        npz_files = list(ruta_mascaras.rglob(f\"*{foto_id}*.npz\"))\n",
        "        if npz_files:\n",
        "            return npz_files[0]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extraer_mascara(self, data: np.lib.npyio.NpzFile, modelo: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Extrae máscara binaria con lógica específica por modelo.\n",
        "\n",
        "        Estrategia:\n",
        "        - mask2former: 'mascara' o 'mask'\n",
        "        - sam2: 'mascaras_binarias' (stack, tomar primera)\n",
        "        - yolov8: 'masks' (stack, tomar primera)\n",
        "        - oneformer: similar a mask2former\n",
        "        - bodypix: 'mascara_probabilidad' con threshold o mascaras_binarias_por_umbral\n",
        "        \"\"\"\n",
        "        # Intentar nombres comunes primero\n",
        "        for nombre in ['mascara', 'mask', 'segmentation']:\n",
        "            if nombre in data:\n",
        "                mascara = data[nombre]\n",
        "                # Si es un array válido, retornarlo\n",
        "                if isinstance(mascara, np.ndarray) and mascara.ndim >= 2:\n",
        "                    return mascara\n",
        "\n",
        "        # Modelos con stacks (SAM2, YOLO)\n",
        "        if 'mascaras_binarias' in data:  # SAM2\n",
        "            stack = data['mascaras_binarias']\n",
        "            if isinstance(stack, np.ndarray):\n",
        "                if stack.ndim == 3 and stack.shape[0] > 0:\n",
        "                    return stack[0]\n",
        "                elif stack.ndim == 2:\n",
        "                    return stack\n",
        "\n",
        "        if 'masks' in data:  # YOLO o general\n",
        "            stack = data['masks']\n",
        "            if isinstance(stack, np.ndarray):\n",
        "                if stack.ndim == 3 and stack.shape[0] > 0:\n",
        "                    return stack[0]\n",
        "                elif stack.ndim == 2:\n",
        "                    return stack\n",
        "\n",
        "        # BodyPix con umbrales\n",
        "        if 'mascaras_binarias_por_umbral' in data:\n",
        "            # Tomar umbral medio (suele ser el más balanceado)\n",
        "            try:\n",
        "                umbrales_dict = data['mascaras_binarias_por_umbral'].item()\n",
        "                if isinstance(umbrales_dict, dict) and len(umbrales_dict) > 0:\n",
        "                    umbral_medio = sorted(umbrales_dict.keys())[len(umbrales_dict)//2]\n",
        "                    return umbrales_dict[umbral_medio]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if 'mascara_probabilidad' in data:\n",
        "            # Threshold en 0.5\n",
        "            prob_mask = data['mascara_probabilidad']\n",
        "            if isinstance(prob_mask, np.ndarray):\n",
        "                return (prob_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extraer_bbox(self, data: np.lib.npyio.NpzFile) -> Optional[np.ndarray]:\n",
        "        \"\"\"Extrae bounding box si está disponible.\"\"\"\n",
        "        for nombre in ['bbox', 'bboxes', 'boxes']:\n",
        "            if nombre in data:\n",
        "                try:\n",
        "                    bbox_data = data[nombre]\n",
        "                    if isinstance(bbox_data, np.ndarray):\n",
        "                        if bbox_data.ndim == 2 and bbox_data.shape[0] > 0:\n",
        "                            return bbox_data[0]\n",
        "                        elif bbox_data.ndim == 1:\n",
        "                            return bbox_data\n",
        "                except:\n",
        "                    continue\n",
        "        return None\n",
        "\n",
        "    def _extraer_score(self, data: np.lib.npyio.NpzFile) -> Optional[float]:\n",
        "        \"\"\"Extrae score de confianza si está disponible.\"\"\"\n",
        "        for nombre in ['score', 'scores', 'confidence', 'person_confidence']:\n",
        "            if nombre in data:\n",
        "                try:\n",
        "                    score_data = data[nombre]\n",
        "                    if isinstance(score_data, np.ndarray):\n",
        "                        if score_data.size > 0:\n",
        "                            return float(score_data.flat[0])\n",
        "                    else:\n",
        "                        return float(score_data)\n",
        "                except:\n",
        "                    continue\n",
        "        return None\n",
        "\n",
        "    def _extraer_area(self, data: np.lib.npyio.NpzFile, mascara: np.ndarray) -> int:\n",
        "        \"\"\"Extrae área o la calcula desde la máscara.\"\"\"\n",
        "        for nombre in ['area', 'areas']:\n",
        "            if nombre in data:\n",
        "                try:\n",
        "                    area_data = data[nombre]\n",
        "                    if isinstance(area_data, np.ndarray):\n",
        "                        if area_data.size > 0:\n",
        "                            return int(area_data.flat[0])\n",
        "                    else:\n",
        "                        return int(area_data)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Calcular desde máscara si no está disponible\n",
        "        return int(np.sum(mascara))"
      ],
      "metadata": {
        "id": "iQSDdCSKpxWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# LECTOR DE METADATA DE MODELOS\n",
        "# =============================================================================\n",
        "\n",
        "class LectorMetadataModelos:\n",
        "    \"\"\"\n",
        "    Carga JSONs de metadata de ejecución de modelos.\n",
        "\n",
        "    Estos JSONs contienen información valiosa sobre:\n",
        "    - Tiempos de inferencia y procesamiento\n",
        "    - Scores y confianzas por umbral\n",
        "    - Configuraciones específicas del modelo\n",
        "    - Dimensiones procesadas y factor de escala\n",
        "    - Estrategias de prompts (SAM2)\n",
        "    - Partes del cuerpo (BodyPix)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ruta_modelos: Path):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ruta_modelos: Path al directorio base de modelos\n",
        "        \"\"\"\n",
        "        self.ruta_modelos = ruta_modelos\n",
        "        # Mapeo de códigos de configuración a rutas de JSONs\n",
        "        self.rutas_json = {}\n",
        "\n",
        "    def registrar_ruta_json(self, codigo_config: str, ruta_mascaras: Path):\n",
        "        \"\"\"\n",
        "        Registra dónde buscar JSONs para una configuración.\n",
        "\n",
        "        Args:\n",
        "            codigo_config: Código de la configuración\n",
        "            ruta_mascaras: Path al directorio de máscaras\n",
        "        \"\"\"\n",
        "        # Buscar directorio json/ al mismo nivel o en el padre\n",
        "        ruta_json = None\n",
        "\n",
        "        # Intento 1: Al mismo nivel (resultados/json y resultados/mascaras)\n",
        "        dir_padre = ruta_mascaras.parent\n",
        "        json_dir = dir_padre / 'json'\n",
        "        if json_dir.exists():\n",
        "            ruta_json = json_dir\n",
        "        else:\n",
        "            # Intento 2: En subdirectorio del padre\n",
        "            for item in dir_padre.rglob('json'):\n",
        "                if item.is_dir():\n",
        "                    ruta_json = item\n",
        "                    break\n",
        "\n",
        "        if ruta_json:\n",
        "            self.rutas_json[codigo_config] = ruta_json\n",
        "\n",
        "    def cargar(self, codigo_config: str, foto_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Carga metadata JSON de ejecución de un modelo para una foto.\n",
        "\n",
        "        Args:\n",
        "            codigo_config: Código de configuración\n",
        "            foto_id: Identificador de la foto\n",
        "\n",
        "        Returns:\n",
        "            Dict con metadata o None si no existe\n",
        "        \"\"\"\n",
        "        if codigo_config not in self.rutas_json:\n",
        "            return None\n",
        "\n",
        "        ruta_json_dir = self.rutas_json[codigo_config]\n",
        "\n",
        "        # Buscar JSON con el foto_id (puede tener timestamp)\n",
        "        json_files = list(ruta_json_dir.glob(f\"{foto_id}*.json\"))\n",
        "\n",
        "        if len(json_files) == 0:\n",
        "            return None\n",
        "\n",
        "        # Si hay múltiples, tomar el más reciente\n",
        "        if len(json_files) > 1:\n",
        "            json_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "\n",
        "        ruta_json = json_files[0]\n",
        "\n",
        "        try:\n",
        "            with open(ruta_json, 'r', encoding='utf-8') as f:\n",
        "                metadata = json.load(f)\n",
        "\n",
        "            # Extraer campos relevantes de forma unificada\n",
        "            metadata_unificada = self._unificar_metadata(metadata, codigo_config)\n",
        "            metadata_unificada['ruta_json'] = str(ruta_json)\n",
        "\n",
        "            return metadata_unificada\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error cargando metadata {codigo_config}/{foto_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _unificar_metadata(self, metadata: Dict, codigo_config: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Unifica metadata de diferentes modelos en estructura común.\n",
        "\n",
        "        Args:\n",
        "            metadata: Metadata raw del JSON\n",
        "            codigo_config: Código de configuración para detectar modelo\n",
        "\n",
        "        Returns:\n",
        "            Dict con campos unificados\n",
        "        \"\"\"\n",
        "        unificado = {\n",
        "            'metadata_raw': metadata,  # Guardar original completo\n",
        "            'modelo': None,\n",
        "            'timestamp': None,\n",
        "            'tiempos': {},\n",
        "            'dimensiones': {},\n",
        "            'scores': {},\n",
        "            'detecciones': {},\n",
        "            'configuracion': {}\n",
        "        }\n",
        "\n",
        "        # Detectar tipo de modelo por código\n",
        "        if 'm2f' in codigo_config or 'mask2former' in str(metadata.get('metadata', {}).get('modelo', '')).lower():\n",
        "            unificado = self._extraer_mask2former(metadata, unificado)\n",
        "        elif 'o1f' in codigo_config or 'oneformer' in str(metadata.get('metadata', {}).get('modelo', '')).lower():\n",
        "            unificado = self._extraer_oneformer(metadata, unificado)\n",
        "        elif 'sam' in codigo_config or 'sam2' in str(metadata.get('modelo', '')).lower():\n",
        "            unificado = self._extraer_sam2(metadata, unificado)\n",
        "        elif 'yolo' in codigo_config:\n",
        "            unificado = self._extraer_yolo(metadata, unificado)\n",
        "        elif 'bpx' in codigo_config or 'bodypix' in str(metadata.get('metadata', {}).get('modelo', {})).lower():\n",
        "            unificado = self._extraer_bodypix(metadata, unificado)\n",
        "\n",
        "        return unificado\n",
        "\n",
        "    def _extraer_mask2former(self, metadata: Dict, unificado: Dict) -> Dict:\n",
        "        \"\"\"Extrae campos específicos de Mask2Former.\"\"\"\n",
        "        meta = metadata.get('metadata', {})\n",
        "\n",
        "        unificado['modelo'] = meta.get('modelo', 'mask2former')\n",
        "        unificado['timestamp'] = meta.get('timestamp')\n",
        "\n",
        "        # Tiempos\n",
        "        rendimiento = metadata.get('rendimiento', {})\n",
        "        unificado['tiempos'] = {\n",
        "            'inferencia_ms': rendimiento.get('tiempo_inferencia_ms'),\n",
        "            'total_ms': rendimiento.get('tiempo_total_ms'),\n",
        "            'gpu_pico_mb': rendimiento.get('gpu_pico_mb')\n",
        "        }\n",
        "\n",
        "        # Dimensiones\n",
        "        dims = meta.get('dimensiones', {})\n",
        "        unificado['dimensiones'] = {\n",
        "            'original': dims.get('original', {}),\n",
        "            'procesada': dims.get('procesada', {}),\n",
        "            'redimensionada': dims.get('procesada', {}).get('redimensionada', False)\n",
        "        }\n",
        "\n",
        "        # Scores y detecciones por umbral\n",
        "        detecciones = metadata.get('detecciones_por_umbral', {})\n",
        "        unificado['detecciones'] = {\n",
        "            'por_umbral': {},\n",
        "            'num_umbrales': len(detecciones)\n",
        "        }\n",
        "\n",
        "        for umbral_key, umbral_data in detecciones.items():\n",
        "            unificado['detecciones']['por_umbral'][umbral_key] = {\n",
        "                'num_personas': umbral_data.get('num_personas_detectadas', 0),\n",
        "                'scores': umbral_data.get('scores', []),\n",
        "                'score_promedio': umbral_data.get('score_promedio', 0.0),\n",
        "                'score_maximo': umbral_data.get('score_maximo', 0.0)\n",
        "            }\n",
        "\n",
        "        # Configuración\n",
        "        config = meta.get('config_umbrales', {})\n",
        "        unificado['configuracion'] = {\n",
        "            'nombre': config.get('nombre'),\n",
        "            'umbrales': config.get('valores', []),\n",
        "            'descripcion': config.get('descripcion')\n",
        "        }\n",
        "\n",
        "        return unificado\n",
        "\n",
        "    def _extraer_oneformer(self, metadata: Dict, unificado: Dict) -> Dict:\n",
        "        \"\"\"Extrae campos específicos de OneFormer (similar a Mask2Former).\"\"\"\n",
        "        return self._extraer_mask2former(metadata, unificado)\n",
        "\n",
        "    def _extraer_sam2(self, metadata: Dict, unificado: Dict) -> Dict:\n",
        "        \"\"\"Extrae campos específicos de SAM2.\"\"\"\n",
        "        unificado['modelo'] = metadata.get('modelo', 'sam2')\n",
        "        unificado['timestamp'] = metadata.get('timestamp')\n",
        "\n",
        "        # Tiempos\n",
        "        proc = metadata.get('procesamiento', {})\n",
        "        unificado['tiempos'] = {\n",
        "            'carga_ms': proc.get('tiempo_carga_ms'),\n",
        "            'set_imagen_ms': proc.get('tiempo_set_imagen_ms'),\n",
        "            'prediccion_ms': proc.get('tiempo_prediccion_ms'),\n",
        "            'total_ms': proc.get('tiempo_total_ms')\n",
        "        }\n",
        "\n",
        "        # Dimensiones\n",
        "        dims = metadata.get('dimensiones', {})\n",
        "        unificado['dimensiones'] = {\n",
        "            'original': dims.get('original', {}),\n",
        "            'procesada': dims.get('procesada', {}),\n",
        "            'redimensionada': dims.get('procesada', {}).get('redimensionada', False)\n",
        "        }\n",
        "\n",
        "        # Prompts y estrategia\n",
        "        prompts = metadata.get('prompts', {})\n",
        "        unificado['configuracion'] = {\n",
        "            'estrategia': prompts.get('estrategia'),\n",
        "            'tipo_prompts': prompts.get('tipo'),\n",
        "            'num_puntos': prompts.get('num_point_coords', 0),\n",
        "            'num_boxes': prompts.get('num_boxes', 0)\n",
        "        }\n",
        "\n",
        "        # Scores y detecciones\n",
        "        resultados = metadata.get('resultados', {})\n",
        "        metricas = metadata.get('metricas', {})\n",
        "        unificado['detecciones'] = {\n",
        "            'mascaras_generadas': resultados.get('mascaras_generadas', 0),\n",
        "            'personas_detectadas': resultados.get('personas_detectadas', 0)\n",
        "        }\n",
        "        unificado['scores'] = {\n",
        "            'scores_sam': metricas.get('scores_sam', []),\n",
        "            'scores_personas': metricas.get('scores_personas', [])\n",
        "        }\n",
        "\n",
        "        return unificado\n",
        "\n",
        "    def _extraer_yolo(self, metadata: Dict, unificado: Dict) -> Dict:\n",
        "        \"\"\"Extrae campos específicos de YOLO.\"\"\"\n",
        "        # Implementar según estructura de YOLO cuando esté disponible\n",
        "        unificado['modelo'] = 'yolov8-seg'\n",
        "        return unificado\n",
        "\n",
        "    def _extraer_bodypix(self, metadata: Dict, unificado: Dict) -> Dict:\n",
        "        \"\"\"Extrae campos específicos de BodyPix.\"\"\"\n",
        "        meta = metadata.get('metadata', {})\n",
        "        modelo_info = meta.get('modelo', {})\n",
        "\n",
        "        unificado['modelo'] = modelo_info.get('nombre', 'bodypix')\n",
        "        unificado['timestamp'] = meta.get('timestamp')\n",
        "\n",
        "        # Tiempos\n",
        "        unificado['tiempos'] = {\n",
        "            'inferencia_ms': meta.get('tiempo_inferencia_ms'),\n",
        "            'total_ms': meta.get('tiempo_inferencia_ms')\n",
        "        }\n",
        "\n",
        "        # Dimensiones\n",
        "        imagen_info = meta.get('imagen', {})\n",
        "        unificado['dimensiones'] = {\n",
        "            'original': {\n",
        "                'width': imagen_info.get('ancho_original'),\n",
        "                'height': imagen_info.get('alto_original')\n",
        "            },\n",
        "            'procesada': {\n",
        "                'width': imagen_info.get('ancho_procesado'),\n",
        "                'height': imagen_info.get('alto_procesado')\n",
        "            },\n",
        "            'redimensionada': imagen_info.get('redimensionado', False),\n",
        "            'factor_escala': imagen_info.get('factor_escala')\n",
        "        }\n",
        "\n",
        "        # Detecciones por umbral\n",
        "        detecciones = metadata.get('detecciones_por_umbral', {})\n",
        "        unificado['detecciones'] = {\n",
        "            'por_umbral': {},\n",
        "            'num_umbrales': len(detecciones)\n",
        "        }\n",
        "\n",
        "        for umbral_key, umbral_data in detecciones.items():\n",
        "            unificado['detecciones']['por_umbral'][umbral_key] = {\n",
        "                'persona_detectada': umbral_data.get('persona_detectada', False),\n",
        "                'area_pixels': umbral_data.get('area_pixels', 0),\n",
        "                'porcentaje_imagen': umbral_data.get('porcentaje_imagen', 0.0),\n",
        "                'probabilidad_media': umbral_data.get('probabilidad_media_region', 0.0)\n",
        "            }\n",
        "\n",
        "        # Partes del cuerpo (único de BodyPix)\n",
        "        partes = metadata.get('partes_cuerpo', {})\n",
        "        unificado['configuracion'] = {\n",
        "            'partes_detectadas': partes.get('num_partes_detectadas', 0),\n",
        "            'grupos_detectados': partes.get('num_grupos_detectados', 0),\n",
        "            'grupos_disponibles': partes.get('grupos_disponibles', [])\n",
        "        }\n",
        "\n",
        "        return unificado"
      ],
      "metadata": {
        "id": "oLi4zEuLuft1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# INTEGRADOR PRINCIPAL\n",
        "# =============================================================================\n",
        "\n",
        "class IntegradorDatos:\n",
        "    \"\"\"\n",
        "    Orquesta la integración de todos los datos de una fotografía.\n",
        "\n",
        "    Combina características fotográficas, ground truth, máscaras de todos\n",
        "    los modelos y metadata de ejecución en estructura DatosFotografia unificada.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_rutas: ConfiguracionRutas):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config_rutas: Configuración de rutas del proyecto\n",
        "        \"\"\"\n",
        "        self.config = config_rutas\n",
        "\n",
        "        # Inicializar lectores\n",
        "        self.lector_caracteristicas = LectorCaracteristicas(\n",
        "            config_rutas.ruta_caracteristicas\n",
        "        )\n",
        "        self.lector_gt = LectorGroundTruth(config_rutas.ruta_gt)\n",
        "        self.escaner = EscanerConfiguraciones(config_rutas.ruta_modelos)\n",
        "        self.lector_mascaras = LectorMascarasModelos(\n",
        "            config_rutas.ruta_modelos,\n",
        "            self.escaner\n",
        "        )\n",
        "        self.lector_metadata = LectorMetadataModelos(config_rutas.ruta_modelos)\n",
        "\n",
        "        # Escanear configuraciones disponibles\n",
        "        logger.info(\"Escaneando configuraciones de modelos disponibles...\")\n",
        "        self.configs_disponibles = self.escaner.escanear()\n",
        "\n",
        "        # Registrar configuraciones en lectores\n",
        "        for modelo, configs in self.configs_disponibles.items():\n",
        "            for nombre_config, ruta_mascaras in configs:\n",
        "                self.lector_mascaras.registrar_configuracion(modelo, nombre_config, ruta_mascaras)\n",
        "                self.lector_metadata.registrar_ruta_json(\n",
        "                    self.escaner.generar_codigo_config(modelo, nombre_config),\n",
        "                    ruta_mascaras\n",
        "                )\n",
        "\n",
        "        total_configs = sum(len(c) for c in self.configs_disponibles.values())\n",
        "        logger.info(f\"Total configuraciones encontradas: {total_configs}\")\n",
        "\n",
        "    def integrar_fotografia(self, foto_id: str) -> Optional[DatosFotografia]:\n",
        "        \"\"\"\n",
        "        Integra todos los datos disponibles para una fotografía.\n",
        "\n",
        "        Args:\n",
        "            foto_id: Identificador de la foto (ej: '_DSC0002')\n",
        "\n",
        "        Returns:\n",
        "            DatosFotografia consolidado o None si error crítico\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: Si falta característica o GT (crítico)\n",
        "        \"\"\"\n",
        "        logger.info(f\"Integrando datos de: {foto_id}\")\n",
        "\n",
        "        try:\n",
        "            # 1. Cargar características (CRÍTICO)\n",
        "            caracteristicas = self.lector_caracteristicas.cargar(foto_id)\n",
        "\n",
        "            # 2. Cargar Ground Truth (CRÍTICO)\n",
        "            gt = self.lector_gt.cargar(foto_id)\n",
        "\n",
        "            # 3. Cargar máscaras de modelos (WARNING si falta)\n",
        "            mascaras_modelos = {}\n",
        "\n",
        "            for modelo, configs in self.configs_disponibles.items():\n",
        "                for config, _ in configs:\n",
        "                    codigo_config = self.escaner.generar_codigo_config(modelo, config)\n",
        "\n",
        "                    mascara_data = self.lector_mascaras.cargar(codigo_config, foto_id)\n",
        "\n",
        "                    if mascara_data is not None:\n",
        "                        # Añadir metadata de modelo/config\n",
        "                        mascara_data['modelo'] = modelo\n",
        "                        mascara_data['config'] = config\n",
        "                        mascara_data['codigo'] = codigo_config\n",
        "\n",
        "                        mascaras_modelos[codigo_config] = mascara_data\n",
        "\n",
        "            # 4. Cargar metadata de ejecución (WARNING si falta)\n",
        "            metadata_modelos = {}\n",
        "\n",
        "            for codigo_config in mascaras_modelos.keys():\n",
        "                metadata = self.lector_metadata.cargar(codigo_config, foto_id)\n",
        "                if metadata is not None:\n",
        "                    metadata_modelos[codigo_config] = metadata\n",
        "\n",
        "            # 5. Consolidar\n",
        "            datos = DatosFotografia(\n",
        "                foto_id=foto_id,\n",
        "                ruta_imagen=self.config.ruta_imagenes / f\"{foto_id}.jpg\",\n",
        "                ruta_json_caracteristicas=caracteristicas['ruta_json'],\n",
        "                ruta_gt=gt['ruta'],\n",
        "                metadatos=caracteristicas['metadatos'],\n",
        "                exif_disponible=caracteristicas['exif_disponible'],\n",
        "                exif=caracteristicas['exif'],\n",
        "                color=caracteristicas['color'],\n",
        "                saliencia=caracteristicas['saliencia'],\n",
        "                calidad=caracteristicas['calidad'],\n",
        "                gt_disponible=gt['existe'],\n",
        "                gt_mascara=gt['mascara'],\n",
        "                gt_shape=gt['shape'],\n",
        "                mascaras_modelos=mascaras_modelos,\n",
        "                metadata_modelos=metadata_modelos,\n",
        "                timestamp_integracion=datetime.now().isoformat(),\n",
        "                num_configs_disponibles=len(mascaras_modelos)\n",
        "            )\n",
        "\n",
        "            logger.info(f\"  - Características: OK\")\n",
        "            logger.info(f\"  - Ground Truth: OK (cobertura {gt['cobertura_pct']:.1f}%)\")\n",
        "            logger.info(f\"  - Máscaras modelos: {len(mascaras_modelos)} configuraciones\")\n",
        "            logger.info(f\"  - Metadata modelos: {len(metadata_modelos)} configuraciones\")\n",
        "            logger.info(f\"  - EXIF disponible: {'Sí' if datos.exif_disponible else 'No'}\")\n",
        "\n",
        "            return datos\n",
        "\n",
        "        except (FileNotFoundError, RuntimeError, ValueError) as e:\n",
        "            # Errores críticos se propagan\n",
        "            logger.error(f\"Error crítico en {foto_id}: {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            # Otros errores se logean pero no detienen\n",
        "            logger.error(f\"Error inesperado en {foto_id}: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "RzyePOhhqI0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# VALIDADOR DE INTEGRIDAD\n",
        "# =============================================================================\n",
        "\n",
        "class ValidadorIntegridad:\n",
        "    \"\"\"\n",
        "    Valida integridad de datos integrados y genera estadísticas.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validar(datos_fotos: List[DatosFotografia]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Valida integridad de la integración de datos.\n",
        "\n",
        "        Args:\n",
        "            datos_fotos: Lista de DatosFotografia integrados\n",
        "\n",
        "        Returns:\n",
        "            Dict con estadísticas de validación y warnings/errors\n",
        "        \"\"\"\n",
        "        if len(datos_fotos) == 0:\n",
        "            return {\n",
        "                'valido': False,\n",
        "                'error_critico': 'No se integraron datos de ninguna fotografía'\n",
        "            }\n",
        "\n",
        "        # Estadísticas básicas\n",
        "        total_fotos = len(datos_fotos)\n",
        "        fotos_con_gt = sum(1 for d in datos_fotos if d.gt_disponible)\n",
        "        fotos_con_exif = sum(1 for d in datos_fotos if d.exif_disponible)\n",
        "\n",
        "        # Estadísticas de máscaras por modelo\n",
        "        configs_por_foto = [d.num_configs_disponibles for d in datos_fotos]\n",
        "\n",
        "        # Estadísticas de metadata\n",
        "        metadata_por_foto = [len(d.metadata_modelos) for d in datos_fotos]\n",
        "\n",
        "        # Contar configuraciones por modelo\n",
        "        todos_codigos = set()\n",
        "        for d in datos_fotos:\n",
        "            todos_codigos.update(d.mascaras_modelos.keys())\n",
        "\n",
        "        configs_por_modelo = {}\n",
        "        for codigo in todos_codigos:\n",
        "            modelo = codigo.split('_')[0]\n",
        "            if modelo not in configs_por_modelo:\n",
        "                configs_por_modelo[modelo] = []\n",
        "            configs_por_modelo[modelo].append(codigo)\n",
        "\n",
        "        # Warnings\n",
        "        warnings = []\n",
        "        if fotos_con_exif < total_fotos:\n",
        "            warnings.append(\n",
        "                f\"{total_fotos - fotos_con_exif} fotos sin EXIF \"\n",
        "                f\"({100*(total_fotos-fotos_con_exif)/total_fotos:.1f}%). \"\n",
        "                f\"Análisis de correlaciones fotográficas limitado.\"\n",
        "            )\n",
        "\n",
        "        if min(configs_por_foto) < max(configs_por_foto):\n",
        "            warnings.append(\n",
        "                f\"Número inconsistente de máscaras por foto: \"\n",
        "                f\"min={min(configs_por_foto)}, max={max(configs_por_foto)}. \"\n",
        "                f\"Algunas configuraciones pueden faltar en ciertas fotos.\"\n",
        "            )\n",
        "\n",
        "        if sum(metadata_por_foto) < sum(configs_por_foto):\n",
        "            warnings.append(\n",
        "                f\"Metadata de ejecución no disponible para todas las configuraciones. \"\n",
        "                f\"Total máscaras: {sum(configs_por_foto)}, Total metadata: {sum(metadata_por_foto)}\"\n",
        "            )\n",
        "\n",
        "        # Resultado\n",
        "        resultado = {\n",
        "            'valido': True,\n",
        "            'estadisticas': {\n",
        "                'total_fotografias': total_fotos,\n",
        "                'fotos_con_ground_truth': fotos_con_gt,\n",
        "                'fotos_con_exif': fotos_con_exif,\n",
        "                'porcentaje_sin_exif': 100 * (total_fotos - fotos_con_exif) / total_fotos,\n",
        "                'configuraciones_por_foto': {\n",
        "                    'min': min(configs_por_foto),\n",
        "                    'max': max(configs_por_foto),\n",
        "                    'promedio': np.mean(configs_por_foto),\n",
        "                    'std': np.std(configs_por_foto)\n",
        "                },\n",
        "                'metadata_por_foto': {\n",
        "                    'min': min(metadata_por_foto),\n",
        "                    'max': max(metadata_por_foto),\n",
        "                    'promedio': np.mean(metadata_por_foto),\n",
        "                    'total': sum(metadata_por_foto)\n",
        "                },\n",
        "                'configuraciones_por_modelo': {\n",
        "                    modelo: len(configs)\n",
        "                    for modelo, configs in configs_por_modelo.items()\n",
        "                },\n",
        "                'total_configuraciones_unicas': len(todos_codigos)\n",
        "            },\n",
        "            'warnings': warnings\n",
        "        }\n",
        "\n",
        "        return resultado"
      ],
      "metadata": {
        "id": "Jecl1T0NqUsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FUNCIÓN PRINCIPAL\n",
        "# =============================================================================\n",
        "\n",
        "def ejecutar_fase1_integracion(ruta_base_tfm: str) -> Tuple[List[DatosFotografia], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Ejecuta la Fase 1 completa de integración de datos.\n",
        "\n",
        "    Args:\n",
        "        ruta_base_tfm: Ruta al directorio base del proyecto TFM\n",
        "\n",
        "    Returns:\n",
        "        Tupla (lista_datos_fotografias, dict_validacion)\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: Si rutas críticas no existen\n",
        "        RuntimeError: Si errores críticos en datos\n",
        "    \"\"\"\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(\"FASE 1: INTEGRACIÓN DE DATOS\")\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "    # 1. Descubrir estructura de directorios\n",
        "    logger.info(\"\\n[1/5] Descubriendo estructura del proyecto...\")\n",
        "    config_rutas = ConfiguracionRutas.descubrir_desde_base(ruta_base_tfm)\n",
        "\n",
        "    # Mostrar estructura descubierta\n",
        "    config_rutas.mostrar_estructura()\n",
        "\n",
        "    rutas_validas, errores_rutas = config_rutas.validar()\n",
        "    if not rutas_validas:\n",
        "        logger.error(\"\\nErrores en la estructura de directorios:\")\n",
        "        for error in errores_rutas:\n",
        "            logger.error(f\"  - {error}\")\n",
        "        raise FileNotFoundError(\n",
        "            \"No se pudo validar la estructura del proyecto. \"\n",
        "            \"Verifica los directorios críticos arriba.\"\n",
        "        )\n",
        "\n",
        "    logger.info(\"\\n  Estructura validada correctamente\")\n",
        "\n",
        "    # 2. Obtener lista de fotografías\n",
        "    logger.info(\"\\n[2/5] Escaneando fotografías en directorio...\")\n",
        "    imagenes = sorted(config_rutas.ruta_imagenes.glob(\"*.jpg\"))\n",
        "    fotos_ids = [img.stem for img in imagenes]\n",
        "\n",
        "    logger.info(f\"  Fotografías encontradas: {len(fotos_ids)}\")\n",
        "\n",
        "    if len(fotos_ids) == 0:\n",
        "        raise FileNotFoundError(\n",
        "            f\"No se encontraron imágenes JPG en {config_rutas.ruta_imagenes}\"\n",
        "        )\n",
        "\n",
        "    # Mostrar primeras fotos para verificación\n",
        "    if len(fotos_ids) > 0:\n",
        "        logger.info(f\"  Primeras fotos: {', '.join(fotos_ids[:3])}\")\n",
        "        if len(fotos_ids) > 3:\n",
        "            logger.info(f\"  ... y {len(fotos_ids) - 3} más\")\n",
        "\n",
        "    # 3. Inicializar integrador\n",
        "    logger.info(\"\\n[3/5] Inicializando integrador de datos...\")\n",
        "    integrador = IntegradorDatos(config_rutas)\n",
        "\n",
        "    # 4. Integrar cada fotografía\n",
        "    logger.info(\"\\n[4/5] Integrando datos por fotografía...\")\n",
        "    logger.info(\"-\"*80)\n",
        "\n",
        "    datos_integrados = []\n",
        "    errores_criticos = []\n",
        "\n",
        "    for i, foto_id in enumerate(fotos_ids, 1):\n",
        "        logger.info(f\"\\n[{i}/{len(fotos_ids)}] Procesando: {foto_id}\")\n",
        "\n",
        "        try:\n",
        "            datos = integrador.integrar_fotografia(foto_id)\n",
        "\n",
        "            if datos is not None:\n",
        "                datos_integrados.append(datos)\n",
        "            else:\n",
        "                logger.warning(f\"  No se pudo integrar {foto_id} (error no crítico)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            errores_criticos.append({\n",
        "                'foto_id': foto_id,\n",
        "                'error': str(e)\n",
        "            })\n",
        "            logger.error(f\"  ERROR CRÍTICO: {e}\")\n",
        "\n",
        "    # Si hay errores críticos, detener\n",
        "    if len(errores_criticos) > 0:\n",
        "        logger.error(\"\\n\" + \"=\"*80)\n",
        "        logger.error(\"ERRORES CRÍTICOS ENCONTRADOS\")\n",
        "        logger.error(\"=\"*80)\n",
        "        for error in errores_criticos:\n",
        "            logger.error(f\"  - {error['foto_id']}: {error['error']}\")\n",
        "\n",
        "        raise RuntimeError(\n",
        "            f\"Se encontraron {len(errores_criticos)} errores críticos. \"\n",
        "            f\"Revisar logs arriba para detalles.\"\n",
        "        )\n",
        "\n",
        "    # 5. Validar integridad\n",
        "    logger.info(\"\\n[5/5] Validando integridad de datos integrados...\")\n",
        "    logger.info(\"-\"*80)\n",
        "\n",
        "    validacion = ValidadorIntegridad.validar(datos_integrados)\n",
        "\n",
        "    if not validacion['valido']:\n",
        "        raise RuntimeError(\n",
        "            f\"Validación de integridad falló: {validacion.get('error_critico')}\"\n",
        "        )\n",
        "\n",
        "    # Mostrar estadísticas\n",
        "    stats = validacion['estadisticas']\n",
        "    logger.info(\"\\nEstadísticas de integración:\")\n",
        "    logger.info(f\"  - Total fotografías: {stats['total_fotografias']}\")\n",
        "    logger.info(f\"  - Con Ground Truth: {stats['fotos_con_ground_truth']}\")\n",
        "    logger.info(f\"  - Con EXIF: {stats['fotos_con_exif']} ({100-stats['porcentaje_sin_exif']:.1f}%)\")\n",
        "    logger.info(f\"  - Configuraciones únicas: {stats['total_configuraciones_unicas']}\")\n",
        "    logger.info(f\"  - Configs por foto (promedio): {stats['configuraciones_por_foto']['promedio']:.1f}\")\n",
        "    logger.info(f\"  - Metadata cargada: {stats['metadata_por_foto']['total']} entradas\")\n",
        "\n",
        "    logger.info(\"\\nConfigs por modelo:\")\n",
        "    for modelo, count in stats['configuraciones_por_modelo'].items():\n",
        "        logger.info(f\"  - {modelo}: {count} configuraciones\")\n",
        "\n",
        "    # Mostrar warnings\n",
        "    if len(validacion['warnings']) > 0:\n",
        "        logger.warning(\"\\nAdvertencias:\")\n",
        "        for warning in validacion['warnings']:\n",
        "            logger.warning(f\"  - {warning}\")\n",
        "\n",
        "    # Resultado final\n",
        "    logger.info(\"\\n\" + \"=\"*80)\n",
        "    logger.info(\"FASE 1 COMPLETADA EXITOSAMENTE\")\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(f\"Fotografías integradas: {len(datos_integrados)}\")\n",
        "    logger.info(f\"Máscaras totales cargadas: {sum(d.num_configs_disponibles for d in datos_integrados)}\")\n",
        "    logger.info(f\"Metadata totales cargadas: {sum(len(d.metadata_modelos) for d in datos_integrados)}\")\n",
        "\n",
        "    return datos_integrados, validacion"
      ],
      "metadata": {
        "id": "fR3WcaSctKbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PUNTO DE ENTRADA PARA TESTING\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    RUTA_BASE_TFM = \"/content/drive/MyDrive/TFM\"\n",
        "\n",
        "    try:\n",
        "        datos_fotos, validacion = ejecutar_fase1_integracion(RUTA_BASE_TFM)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RESUMEN DE INTEGRACIÓN\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Fotografías procesadas: {len(datos_fotos)}\")\n",
        "        print(f\"Validación: {'EXITOSA' if validacion['valido'] else 'FALLIDA'}\")\n",
        "\n",
        "        if len(datos_fotos) > 0:\n",
        "            print(f\"\\nEjemplo de datos integrados (primera foto):\")\n",
        "            ejemplo = datos_fotos[0]\n",
        "            print(f\"  - ID: {ejemplo.foto_id}\")\n",
        "            print(f\"  - GT disponible: {ejemplo.gt_disponible}\")\n",
        "            print(f\"  - EXIF disponible: {ejemplo.exif_disponible}\")\n",
        "            print(f\"  - Máscaras cargadas: {ejemplo.num_configs_disponibles}\")\n",
        "            print(f\"  - Metadata cargada: {len(ejemplo.metadata_modelos)}\")\n",
        "            print(f\"  - Modelos: {list(set(m['modelo'] for m in ejemplo.mascaras_modelos.values()))}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR DURANTE EJECUCIÓN: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "1toidnRotd0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42a833f-95f5-4dd2-b79a-54cd3b662a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2025-11-11 21:12:45] INFO - ================================================================================\n",
            "[2025-11-11 21:12:45] INFO - FASE 1: INTEGRACIÓN DE DATOS\n",
            "[2025-11-11 21:12:45] INFO - ================================================================================\n",
            "[2025-11-11 21:12:45] INFO - \n",
            "[1/5] Descubriendo estructura del proyecto...\n",
            "[2025-11-11 21:12:45] INFO - Escaneando estructura en: /content/drive/MyDrive/TFM\n",
            "[2025-11-11 21:12:45] INFO -   Imágenes originales encontrado en: 0_Imagenes\n",
            "[2025-11-11 21:12:45] INFO -   Características fotográficas encontrado en: 1_Caracteristicas\n",
            "[2025-11-11 21:12:45] INFO -   Ground Truth encontrado en: 0_Imagenes_CVAT/ground_truth_masks\n",
            "[2025-11-11 21:12:45] INFO -   Resultados de modelos encontrado en: 2_Modelos\n",
            "[2025-11-11 21:12:45] INFO -   Directorio de análisis: 3_Analisis\n",
            "[2025-11-11 21:12:45] INFO - \n",
            "  Estructura validada correctamente\n",
            "[2025-11-11 21:12:45] INFO - \n",
            "[2/5] Escaneando fotografías en directorio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estructura de rutas descubiertas:\n",
            "  Base:             /content/drive/MyDrive/TFM\n",
            "  Imágenes:         /content/drive/MyDrive/TFM/0_Imagenes\n",
            "  Características:  /content/drive/MyDrive/TFM/1_Caracteristicas\n",
            "  Ground Truth:     /content/drive/MyDrive/TFM/0_Imagenes_CVAT/ground_truth_masks\n",
            "  Modelos:          /content/drive/MyDrive/TFM/2_Modelos\n",
            "  Análisis:         /content/drive/MyDrive/TFM/3_Analisis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2025-11-11 21:12:45] INFO -   Fotografías encontradas: 20\n",
            "[2025-11-11 21:12:45] INFO -   Primeras fotos: _DSC0023, _DSC0036, _DSC0071\n",
            "[2025-11-11 21:12:45] INFO -   ... y 17 más\n",
            "[2025-11-11 21:12:45] INFO - \n",
            "[3/5] Inicializando integrador de datos...\n",
            "[2025-11-11 21:12:45] INFO -   JSONs encontrados en subdirectorio: 1_Caracteristicas/json\n",
            "[2025-11-11 21:12:45] INFO - Escaneando configuraciones de modelos disponibles...\n",
            "[2025-11-11 21:13:01] INFO - Modelo mask2former: 19 configuraciones encontradas\n",
            "[2025-11-11 21:13:01] INFO -   Configuraciones de mask2former:\n",
            "[2025-11-11 21:13:01] INFO -     - large_ade_baja_sensibilidad_resultados: large_ade/baja_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_ade_media_sensibilidad_resultados: large_ade/media_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_ade_alta_sensibilidad_resultados: large_ade/alta_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_ade_maxima_sensibilidad_resultados: large_ade/maxima_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_coco_experimental_coco_resultados: large_coco/experimental_coco/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_coco_baja_sensibilidad_resultados: large_coco/baja_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_coco_media_sensibilidad_resultados: large_coco/media_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_coco_alta_sensibilidad_resultados: large_coco/alta_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - large_coco_maxima_sensibilidad_resultados: large_coco/maxima_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - base_coco_experimental_coco_resultados: base_coco/experimental_coco/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - base_coco_baja_sensibilidad_resultados: base_coco/baja_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - base_coco_media_sensibilidad_resultados: base_coco/media_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - base_coco_alta_sensibilidad_resultados: base_coco/alta_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - base_coco_maxima_sensibilidad_resultados: base_coco/maxima_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - tiny_coco_experimental_coco_resultados: tiny_coco/experimental_coco/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - tiny_coco_baja_sensibilidad_resultados: tiny_coco/baja_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - tiny_coco_media_sensibilidad_resultados: tiny_coco/media_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - tiny_coco_alta_sensibilidad_resultados: tiny_coco/alta_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:01] INFO -     - tiny_coco_maxima_sensibilidad_resultados: tiny_coco/maxima_sensibilidad/resultados/mascaras\n",
            "[2025-11-11 21:13:08] INFO - Modelo oneformer: 12 configuraciones encontradas\n",
            "[2025-11-11 21:13:08] INFO -   Configuraciones de oneformer:\n",
            "[2025-11-11 21:13:08] INFO -     - coco_swin_instance_t0.40: coco_swin_instance_t0.40/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - coco_swin_instance_t0.60: coco_swin_instance_t0.60/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - coco_swin_instance_t0.75: coco_swin_instance_t0.75/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - coco_swin_instance_t0.85: coco_swin_instance_t0.85/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_swin_instance_t0.40: ade20k_swin_instance_t0.40/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_swin_instance_t0.60: ade20k_swin_instance_t0.60/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_swin_instance_t0.75: ade20k_swin_instance_t0.75/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_swin_instance_t0.85: ade20k_swin_instance_t0.85/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_tiny_instance_t0.40: ade20k_tiny_instance_t0.40/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_tiny_instance_t0.60: ade20k_tiny_instance_t0.60/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_tiny_instance_t0.75: ade20k_tiny_instance_t0.75/mascaras\n",
            "[2025-11-11 21:13:08] INFO -     - ade20k_tiny_instance_t0.85: ade20k_tiny_instance_t0.85/mascaras\n",
            "[2025-11-11 21:13:16] INFO - Modelo sam2: 12 configuraciones encontradas\n",
            "[2025-11-11 21:13:16] INFO -   Configuraciones de sam2:\n",
            "[2025-11-11 21:13:16] INFO -     - tiny_low_cost: tiny_low_cost/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - tiny_balanced: tiny_balanced/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - tiny_quality: tiny_quality/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - small_low_cost: small_low_cost/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - small_balanced: small_balanced/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - small_quality: small_quality/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - base_plus_low_cost: base_plus_low_cost/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - base_plus_balanced: base_plus_balanced/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - base_plus_quality: base_plus_quality/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - large_low_cost: large_low_cost/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - large_balanced: large_balanced/mascaras\n",
            "[2025-11-11 21:13:16] INFO -     - large_quality: large_quality/mascaras\n",
            "[2025-11-11 21:13:16] WARNING - Directorio de modelo no existe: yolov8. No se cargarán máscaras de este modelo.\n",
            "[2025-11-11 21:13:20] INFO - Modelo bodypix: 8 configuraciones encontradas\n",
            "[2025-11-11 21:13:20] INFO -   Configuraciones de bodypix:\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_050_ultra_sensible: mobilenet_v1_050/ultra_sensible/mascaras\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_050_sensibilidad_alta: mobilenet_v1_050/sensibilidad_alta/mascaras\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_050_sensibilidad_media: mobilenet_v1_050/sensibilidad_media/mascaras\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_050_baja_sensibilidad: mobilenet_v1_050/baja_sensibilidad/mascaras\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_075_ultra_sensible: mobilenet_v1_075/ultra_sensible/mascaras\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_075_sensibilidad_alta: mobilenet_v1_075/sensibilidad_alta/mascaras\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_075_sensibilidad_media: mobilenet_v1_075/sensibilidad_media/mascaras\n",
            "[2025-11-11 21:13:20] INFO -     - mobilenet_v1_075_baja_sensibilidad: mobilenet_v1_075/baja_sensibilidad/mascaras\n",
            "[2025-11-11 21:13:20] INFO - Total configuraciones encontradas: 51\n",
            "[2025-11-11 21:13:20] INFO - \n",
            "[4/5] Integrando datos por fotografía...\n",
            "[2025-11-11 21:13:20] INFO - --------------------------------------------------------------------------------\n",
            "[2025-11-11 21:13:20] INFO - \n",
            "[1/20] Procesando: _DSC0023\n",
            "[2025-11-11 21:13:20] INFO - Integrando datos de: _DSC0023\n",
            "[2025-11-11 21:14:12] INFO -   - Características: OK\n",
            "[2025-11-11 21:14:12] INFO -   - Ground Truth: OK (cobertura 68.0%)\n",
            "[2025-11-11 21:14:12] INFO -   - Máscaras modelos: 51 configuraciones\n",
            "[2025-11-11 21:14:12] INFO -   - Metadata modelos: 51 configuraciones\n",
            "[2025-11-11 21:14:12] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:14:12] INFO - \n",
            "[2/20] Procesando: _DSC0036\n",
            "[2025-11-11 21:14:12] INFO - Integrando datos de: _DSC0036\n",
            "[2025-11-11 21:15:03] INFO -   - Características: OK\n",
            "[2025-11-11 21:15:03] INFO -   - Ground Truth: OK (cobertura 48.9%)\n",
            "[2025-11-11 21:15:03] INFO -   - Máscaras modelos: 51 configuraciones\n",
            "[2025-11-11 21:15:03] INFO -   - Metadata modelos: 51 configuraciones\n",
            "[2025-11-11 21:15:03] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:15:03] INFO - \n",
            "[3/20] Procesando: _DSC0071\n",
            "[2025-11-11 21:15:03] INFO - Integrando datos de: _DSC0071\n",
            "[2025-11-11 21:15:59] INFO -   - Características: OK\n",
            "[2025-11-11 21:15:59] INFO -   - Ground Truth: OK (cobertura 42.4%)\n",
            "[2025-11-11 21:15:59] INFO -   - Máscaras modelos: 46 configuraciones\n",
            "[2025-11-11 21:15:59] INFO -   - Metadata modelos: 46 configuraciones\n",
            "[2025-11-11 21:15:59] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:15:59] INFO - \n",
            "[4/20] Procesando: _DSC0084\n",
            "[2025-11-11 21:15:59] INFO - Integrando datos de: _DSC0084\n",
            "[2025-11-11 21:16:53] INFO -   - Características: OK\n",
            "[2025-11-11 21:16:53] INFO -   - Ground Truth: OK (cobertura 84.3%)\n",
            "[2025-11-11 21:16:53] INFO -   - Máscaras modelos: 51 configuraciones\n",
            "[2025-11-11 21:16:53] INFO -   - Metadata modelos: 51 configuraciones\n",
            "[2025-11-11 21:16:53] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:16:53] INFO - \n",
            "[5/20] Procesando: _DSC0119\n",
            "[2025-11-11 21:16:53] INFO - Integrando datos de: _DSC0119\n",
            "[2025-11-11 21:17:44] INFO -   - Características: OK\n",
            "[2025-11-11 21:17:44] INFO -   - Ground Truth: OK (cobertura 16.6%)\n",
            "[2025-11-11 21:17:44] INFO -   - Máscaras modelos: 46 configuraciones\n",
            "[2025-11-11 21:17:44] INFO -   - Metadata modelos: 46 configuraciones\n",
            "[2025-11-11 21:17:44] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:17:44] INFO - \n",
            "[6/20] Procesando: _DSC0139\n",
            "[2025-11-11 21:17:44] INFO - Integrando datos de: _DSC0139\n",
            "[2025-11-11 21:18:30] INFO -   - Características: OK\n",
            "[2025-11-11 21:18:30] INFO -   - Ground Truth: OK (cobertura 22.7%)\n",
            "[2025-11-11 21:18:30] INFO -   - Máscaras modelos: 51 configuraciones\n",
            "[2025-11-11 21:18:30] INFO -   - Metadata modelos: 51 configuraciones\n",
            "[2025-11-11 21:18:30] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:18:30] INFO - \n",
            "[7/20] Procesando: _DSC0143\n",
            "[2025-11-11 21:18:30] INFO - Integrando datos de: _DSC0143\n",
            "[2025-11-11 21:19:16] INFO -   - Características: OK\n",
            "[2025-11-11 21:19:16] INFO -   - Ground Truth: OK (cobertura 18.1%)\n",
            "[2025-11-11 21:19:16] INFO -   - Máscaras modelos: 46 configuraciones\n",
            "[2025-11-11 21:19:16] INFO -   - Metadata modelos: 46 configuraciones\n",
            "[2025-11-11 21:19:16] INFO -   - EXIF disponible: Sí\n",
            "[2025-11-11 21:19:16] INFO - \n",
            "[8/20] Procesando: _DSC0147\n",
            "[2025-11-11 21:19:16] INFO - Integrando datos de: _DSC0147\n",
            "[2025-11-11 21:20:05] INFO -   - Características: OK\n",
            "[2025-11-11 21:20:05] INFO -   - Ground Truth: OK (cobertura 46.1%)\n",
            "[2025-11-11 21:20:05] INFO -   - Máscaras modelos: 46 configuraciones\n",
            "[2025-11-11 21:20:05] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:20:05] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:20:05] INFO - \n",
            "[9/20] Procesando: _DSC0281\n",
            "[2025-11-11 21:20:05] INFO - Integrando datos de: _DSC0281\n",
            "[2025-11-11 21:20:44] INFO -   - Características: OK\n",
            "[2025-11-11 21:20:44] INFO -   - Ground Truth: OK (cobertura 56.4%)\n",
            "[2025-11-11 21:20:44] INFO -   - Máscaras modelos: 39 configuraciones\n",
            "[2025-11-11 21:20:44] INFO -   - Metadata modelos: 39 configuraciones\n",
            "[2025-11-11 21:20:44] INFO -   - EXIF disponible: Sí\n",
            "[2025-11-11 21:20:44] INFO - \n",
            "[10/20] Procesando: _DSC0283\n",
            "[2025-11-11 21:20:44] INFO - Integrando datos de: _DSC0283\n",
            "[2025-11-11 21:21:27] INFO -   - Características: OK\n",
            "[2025-11-11 21:21:27] INFO -   - Ground Truth: OK (cobertura 18.7%)\n",
            "[2025-11-11 21:21:27] INFO -   - Máscaras modelos: 39 configuraciones\n",
            "[2025-11-11 21:21:27] INFO -   - Metadata modelos: 39 configuraciones\n",
            "[2025-11-11 21:21:27] INFO -   - EXIF disponible: Sí\n",
            "[2025-11-11 21:21:27] INFO - \n",
            "[11/20] Procesando: _DSC0411\n",
            "[2025-11-11 21:21:27] INFO - Integrando datos de: _DSC0411\n",
            "[2025-11-11 21:22:00] INFO -   - Características: OK\n",
            "[2025-11-11 21:22:00] INFO -   - Ground Truth: OK (cobertura 17.8%)\n",
            "[2025-11-11 21:22:00] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:22:00] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:22:00] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:22:00] INFO - \n",
            "[12/20] Procesando: _DSC0441\n",
            "[2025-11-11 21:22:00] INFO - Integrando datos de: _DSC0441\n",
            "[2025-11-11 21:22:36] INFO -   - Características: OK\n",
            "[2025-11-11 21:22:36] INFO -   - Ground Truth: OK (cobertura 64.5%)\n",
            "[2025-11-11 21:22:36] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:22:36] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:22:36] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:22:36] INFO - \n",
            "[13/20] Procesando: _DSC0449\n",
            "[2025-11-11 21:22:36] INFO - Integrando datos de: _DSC0449\n",
            "[2025-11-11 21:23:08] INFO -   - Características: OK\n",
            "[2025-11-11 21:23:08] INFO -   - Ground Truth: OK (cobertura 39.8%)\n",
            "[2025-11-11 21:23:08] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:23:08] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:23:08] INFO -   - EXIF disponible: Sí\n",
            "[2025-11-11 21:23:08] INFO - \n",
            "[14/20] Procesando: _DSC0472\n",
            "[2025-11-11 21:23:08] INFO - Integrando datos de: _DSC0472\n",
            "[2025-11-11 21:23:50] INFO -   - Características: OK\n",
            "[2025-11-11 21:23:50] INFO -   - Ground Truth: OK (cobertura 49.9%)\n",
            "[2025-11-11 21:23:50] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:23:50] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:23:50] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:23:50] INFO - \n",
            "[15/20] Procesando: _DSC0545\n",
            "[2025-11-11 21:23:50] INFO - Integrando datos de: _DSC0545\n",
            "[2025-11-11 21:24:29] INFO -   - Características: OK\n",
            "[2025-11-11 21:24:29] INFO -   - Ground Truth: OK (cobertura 37.0%)\n",
            "[2025-11-11 21:24:29] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:24:29] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:24:29] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:24:29] INFO - \n",
            "[16/20] Procesando: _DSC0584\n",
            "[2025-11-11 21:24:29] INFO - Integrando datos de: _DSC0584\n",
            "[2025-11-11 21:25:09] INFO -   - Características: OK\n",
            "[2025-11-11 21:25:09] INFO -   - Ground Truth: OK (cobertura 30.3%)\n",
            "[2025-11-11 21:25:09] INFO -   - Máscaras modelos: 39 configuraciones\n",
            "[2025-11-11 21:25:09] INFO -   - Metadata modelos: 39 configuraciones\n",
            "[2025-11-11 21:25:09] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:25:09] INFO - \n",
            "[17/20] Procesando: _DSC0592\n",
            "[2025-11-11 21:25:09] INFO - Integrando datos de: _DSC0592\n",
            "[2025-11-11 21:25:47] INFO -   - Características: OK\n",
            "[2025-11-11 21:25:47] INFO -   - Ground Truth: OK (cobertura 17.9%)\n",
            "[2025-11-11 21:25:47] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:25:47] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:25:47] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:25:47] INFO - \n",
            "[18/20] Procesando: _DSC0859\n",
            "[2025-11-11 21:25:47] INFO - Integrando datos de: _DSC0859\n",
            "[2025-11-11 21:26:25] INFO -   - Características: OK\n",
            "[2025-11-11 21:26:25] INFO -   - Ground Truth: OK (cobertura 56.9%)\n",
            "[2025-11-11 21:26:25] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:26:25] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:26:25] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:26:25] INFO - \n",
            "[19/20] Procesando: _DSC0962\n",
            "[2025-11-11 21:26:25] INFO - Integrando datos de: _DSC0962\n",
            "[2025-11-11 21:27:00] INFO -   - Características: OK\n",
            "[2025-11-11 21:27:00] INFO -   - Ground Truth: OK (cobertura 35.8%)\n",
            "[2025-11-11 21:27:00] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:27:00] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:27:00] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:27:00] INFO - \n",
            "[20/20] Procesando: _DSC0987\n",
            "[2025-11-11 21:27:00] INFO - Integrando datos de: _DSC0987\n",
            "[2025-11-11 21:27:38] INFO -   - Características: OK\n",
            "[2025-11-11 21:27:38] INFO -   - Ground Truth: OK (cobertura 70.3%)\n",
            "[2025-11-11 21:27:38] INFO -   - Máscaras modelos: 34 configuraciones\n",
            "[2025-11-11 21:27:38] INFO -   - Metadata modelos: 34 configuraciones\n",
            "[2025-11-11 21:27:38] INFO -   - EXIF disponible: No\n",
            "[2025-11-11 21:27:38] INFO - \n",
            "[5/5] Validando integridad de datos integrados...\n",
            "[2025-11-11 21:27:38] INFO - --------------------------------------------------------------------------------\n",
            "[2025-11-11 21:27:38] INFO - \n",
            "Estadísticas de integración:\n",
            "[2025-11-11 21:27:38] INFO -   - Total fotografías: 20\n",
            "[2025-11-11 21:27:38] INFO -   - Con Ground Truth: 20\n",
            "[2025-11-11 21:27:38] INFO -   - Con EXIF: 4 (20.0%)\n",
            "[2025-11-11 21:27:38] INFO -   - Configuraciones únicas: 51\n",
            "[2025-11-11 21:27:38] INFO -   - Configs por foto (promedio): 40.5\n",
            "[2025-11-11 21:27:38] INFO -   - Metadata cargada: 799 entradas\n",
            "[2025-11-11 21:27:38] INFO - \n",
            "Configs por modelo:\n",
            "[2025-11-11 21:27:38] INFO -   - m2f: 19 configuraciones\n",
            "[2025-11-11 21:27:38] INFO -   - o1f: 12 configuraciones\n",
            "[2025-11-11 21:27:38] INFO -   - sam: 12 configuraciones\n",
            "[2025-11-11 21:27:38] INFO -   - bpx: 8 configuraciones\n",
            "[2025-11-11 21:27:38] WARNING - \n",
            "Advertencias:\n",
            "[2025-11-11 21:27:38] WARNING -   - 16 fotos sin EXIF (80.0%). Análisis de correlaciones fotográficas limitado.\n",
            "[2025-11-11 21:27:38] WARNING -   - Número inconsistente de máscaras por foto: min=34, max=51. Algunas configuraciones pueden faltar en ciertas fotos.\n",
            "[2025-11-11 21:27:38] WARNING -   - Metadata de ejecución no disponible para todas las configuraciones. Total máscaras: 811, Total metadata: 799\n",
            "[2025-11-11 21:27:38] INFO - \n",
            "================================================================================\n",
            "[2025-11-11 21:27:38] INFO - FASE 1 COMPLETADA EXITOSAMENTE\n",
            "[2025-11-11 21:27:38] INFO - ================================================================================\n",
            "[2025-11-11 21:27:38] INFO - Fotografías integradas: 20\n",
            "[2025-11-11 21:27:38] INFO - Máscaras totales cargadas: 811\n",
            "[2025-11-11 21:27:38] INFO - Metadata totales cargadas: 799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RESUMEN DE INTEGRACIÓN\n",
            "================================================================================\n",
            "Fotografías procesadas: 20\n",
            "Validación: EXITOSA\n",
            "\n",
            "Ejemplo de datos integrados (primera foto):\n",
            "  - ID: _DSC0023\n",
            "  - GT disponible: True\n",
            "  - EXIF disponible: False\n",
            "  - Máscaras cargadas: 51\n",
            "  - Metadata cargada: 51\n",
            "  - Modelos: ['sam2', 'oneformer', 'mask2former', 'bodypix']\n"
          ]
        }
      ]
    }
  ]
}